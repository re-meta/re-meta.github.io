{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"configuration/","title":"remeta.configuration","text":""},{"location":"configuration/#remeta.configuration.Configuration","title":"Configuration  <code>dataclass</code>","text":"<p>Configuration for the ReMeta toolbox</p>"},{"location":"configuration/#remeta.configuration.Configuration--parameters","title":"Parameters","text":"<p>*** Basic definition of the model *** type2_fitting_type : str (default: 'criteria')     Whether confidence is fitted with discrete criteria or as a continuous variable.     Possible values: 'criteria', 'continuous' type2_noise_type : str (default: 'noisy-report)     Whether the model considers noise at readout or report.     Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature' type2_noise_dist : str         (default: noisy-report + criteria -&gt; 'truncated_norm_mode'                   noisy-report + continuous -&gt; 'truncated_norm_mode'                   noisy-readout + criteria -&gt; 'truncated_norm_mode'                   noisy-readout + continuous -&gt; 'truncated_norm_mode'                   noisy-temperature + criteria -&gt; 'lognorm_mode'                   noisy-temperature + continuous -&gt; 'truncated_norm_mode'         )     Metacognitive noise distribution.     Possible values:         noisy_report: 'beta_mean_std', 'beta_mode_std', 'beta_mode',                       'truncated_norm_mode_std', 'truncated_norm_mode' (default),                       'truncated_gumbel_mode_std', 'truncated_gumbel_mode',                       'truncated_lognorm_mode_std', 'truncated_lognorm', 'truncated_lognorm_mode',                       'truncated_lognorm_mean'         noisy_readout: 'lognorm_median_std', 'lognorm_mean', 'lognorm_mode', 'lognorm_mode_std', 'lognorm_mean_std',                        'gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv',                        'betaprime_mean_std',                        'truncated_norm_mode_std', 'truncated_norm_mode',                        'truncated_gumbel_mode_std', 'truncated_gumbel_mode'         noisy_temperature: same as noisy_readout</p> <p>*** Enable or disable specific parameters *** * Each setting can take the values 0, 1 or 2: *    0: Disable parameter. *    1: Enable parameter. *    2: Enable parameter and fit separate values for the negative and positive stimulus category         (works only for type 1 parameters!) enable_type1_param_noise : int (default: 1)     Fit separate type 1 noise parameters for both stimulus categories. enable_type1_param_noise_heteroscedastic : int (default: 0)     Fit an additional type 1 noise parameter for signal-dependent type 1 noise (the type of dependency is     defined via <code>type1_noise_signal_dependency</code>). enable_type1_param_nonlinear_encoding_gain : int (default: 0) enable_type1_param_nonlinear_encoding_transition : int (default: 0) enable_type1_param_thresh : int (default: 0)     Fit a type 1 threshold. enable_type1_param_bias : int (default: 1)     Fit a type 1 bias towards one of the stimulus categories. enable_type2_param_noise : int (default: 1)     Fit a metacognitive noise parameter enable_type2_param_evidence_bias_mult : int (default: 0)     Fit a multiplicative metacognitive bias loading on evidence. enable_type2_param_criteria : int (default: 0)     Fit confidence criteria.</p> <p>*** Additional options to specify the nature of type 2 fitting *** n_discrete_confidence_levels : int (default: 5)     Number of confidence criteria. Only applies in case of type2_fitting_type='criteria'.</p> <p>*** Define fitting characteristics of the parameters *** * The fitting of each parameter is characzerized as follows: *     1) An initial guess. *     2) Lower and upper bound. *     3) Grid range, i.e. list of values that are tested during the initial gridsearch search. * Sensible default values are provided for all parameters. To tweak those, one can either define an entire * ParameterSet, which is a container for a set of parameters, or each parameter individually. Note that the * parameters must be either defined as a Parameter instance or as List[Parameter] in case when separate values are * fitted for the positive and negative stimulus category/decision value). paramset_type1 : ParameterSet     Parameter set for the type 1 stage. paramset_type2 : ParameterSet     Parameter set for the type 2 stage. paramset : ParameterSet     Parameter set for both stages.</p> Union[Parameter, List[Parameter]]  (default: 1) <p>Parameter for type 1 noise.</p> <p>_type1_param_noise_heteroscedastic : Union[Parameter, List[Parameter]]  (default: 0)     Parameter for signal-dependent type 1 noise. _type1_param_nonlinear_encoding_gain : Union[Parameter, List[Parameter]]  (default: 0)     Gain parameter for nonlinear encoding (higher values -&gt; stronger nonlinearity). _type1_param_nonlinear_encoding_transition : Union[Parameter, List[Parameter]]  (default: 0)     Transition Parameter for nonlinear encoding (). type1_noise_signal_dependency: str (default: 'none')     Can be one of 'none', 'multiplicative', 'power', 'exponential' or 'logarithm'. _type1_param_thresh : Union[Parameter, List[Parameter]] (default: 0)     Parameter for the type 1 threshold. _type1_param_bias : Union[Parameter, List[Parameter]]  (default: 1)     Parameter for the type 1 bias. _type2_param_noise : Union[Parameter, List[Parameter]]  (default: 1)     Parameter for metacognitive noise. _type2_param_evidence_bias_mult : Union[Parameter, List[Parameter]]  (default: 0)     Parameter for a multiplicative metacognitive bias loading on evidence. type2_param_confidence_criteria : List[Parameter]  (default: 1)     List of parameter specifying the confidence criteria.</p> <p>*** Skip type 2 fitting *** skip_type2 : bool (default: False)     If True, ignore type 2 settings in the setup of the model configuration &amp; don't fit type 2 stage.</p> <p>*** Methodoligcal aspects of parameter fitting *** optim_type1_gridsearch : bool (default: False)     If True, perform initial (usually coarse) gridsearch search for type 1 fitting, based on the gridsearch defined     for a Parameter. optim_type1_fine_gridsearch : bool (default: False)     If True, perform an iteratively finer gridsearch search for each parameter (type 1). optim_type1_minimize_along_grid : bool (default: False)     If True, do sqlqp minimization for at each grid point (type 1). optim_type1_global_minimization : str (default: None)     Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with     a global minimizer (type 1). optim_type1_scipy_solvers : str or Tuple/List (default: 'trust-constr')     Set scipy.optimize.minimize gradient method (type 1)     If provided as Tuple/List, test different gradient methods and take the best optim_type2_gridsearch : bool (default: True)     If True, perform initial (usually coarse) gridsearch search for type 2 fitting, based on the gridsearch defined     for a Parameter. optim_type2_fine_gridsearch : bool (default: False)     If True, perform an iteratively finer gridsearch search for each parameter (type 2). optim_type2_minimize_along_grid : bool (default: False)     If True, do sqlqp minimization for at each grid point (type 2). optim_type2_global_minimization : str (default: None)     Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with     a global minimizer (type 2). optim_type2_scipy_solvers : str or Tuple/List (default: ('slsqp', 'Nelder-Mead'))     Set scipy.optimize.minimize gradient method (type 2)     If provided as Tuple/List, test different gradient methods and take the best optim_type2_slsqp_epsilon : float or Tuple/List (default: None)     Set parameter epsilon parameter for the SLSQP optimization method (type 2).     If provided as Tuple/List, test different eps parameters and take the best optim_multiproc : bool (default: False)     If True, use all optim_multiproc_cores cores for parameter estimation. If False, use a single core. optim_multiproc_cores : int (default: -1)     If multicore processing es enabled via optim_multiproc, use optim_multiproc_cores cores for parameter estimation     (-1 for all cores minus 1).</p> <p>*** Preprocessing *** normalize_stimuli_by_max : bool (default: True)     If True, normalize provided stimuli by their maximum value.</p> <p>*** Parameters for the type 2 likelihood computation *** min_type1_likelihood : float     Minimum probability used during the type 1 likelihood computation min_type2_likelihood : float     Minimum probability used during the type 2 likelihood computation type2_binsize : float     Integration bin size for the computation of the likelihood around empirical confidence values y_decval_range_nsds : int     Number of standard deviations around the mean considered for type 1 uncertainty. y_decval_range_nbins : int     Number of discrete decision values bins that are considered to represent type 1 uncertainty. resolution_noisy_temperature : float     Quintile resolution for the marginalization of type 1 noise in case of type2_noise_type 'noisy_temperature'. experimental_min_uniform_type2_likelihood : bool     Instead of using a minimum probability during the likelihood computation, use a maximum cumulative     likelihood based on a 'guessing' model experimental_wrap_type2_integration_window : bool (default: False)     Ensure constant window size for likelihood integration at the bounds.     Only applies in case of type2_fitting_type='continuous' and experimental_disable_type2_binsize=False experimental_include_incongruent_y_decval : bool (default: False)     Include incongruent decision values (i.e., sign(actual choice) != sign(decision value)) for the likelihood     computation experimental_disable_type2_binsize : bool (default: None)     Do not use an integegration window for likelihood computation.     Only applies in case of type2_fitting_type='continuous'</p> <p>*** Other *** true_params : Dict     Pass true (known) parameter values. This can be useful for testing to compare the likelihood of true and     fitted parameters. The likelihood of true parameters is returned (and printed). initilialize_fitting_at_true_params : bool (default: False)     Option to initialize the parameter fitting procedure at the true parameters; this can be helpful for testing. silence_configuration_warnings : bool (default: False)     If True, ignore warnings about user-specified settings. print_configuration : bool (default: True)     If True, print the configuration at instatiation of the ReMeta class.</p> Source code in <code>remeta/configuration.py</code> <pre><code>@reset_dataclass_on_init\n@dataclass\nclass Configuration(ReprMixin):\n    \"\"\"\n    Configuration for the ReMeta toolbox\n\n    Parameters\n    ----------\n    *** Basic definition of the model ***\n    type2_fitting_type : str (default: 'criteria')\n        Whether confidence is fitted with discrete *criteria* or as a continuous variable.\n        Possible values: 'criteria', 'continuous'\n    type2_noise_type : str (default: 'noisy-report)\n        Whether the model considers noise at readout or report.\n        Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'\n    type2_noise_dist : str\n            (default: noisy-report + criteria -&gt; 'truncated_norm_mode'\n                      noisy-report + continuous -&gt; 'truncated_norm_mode'\n                      noisy-readout + criteria -&gt; 'truncated_norm_mode'\n                      noisy-readout + continuous -&gt; 'truncated_norm_mode'\n                      noisy-temperature + criteria -&gt; 'lognorm_mode'\n                      noisy-temperature + continuous -&gt; 'truncated_norm_mode'\n            )\n        Metacognitive noise distribution.\n        Possible values:\n            noisy_report: 'beta_mean_std', 'beta_mode_std', 'beta_mode',\n                          'truncated_norm_mode_std', 'truncated_norm_mode' (default),\n                          'truncated_gumbel_mode_std', 'truncated_gumbel_mode',\n                          'truncated_lognorm_mode_std', 'truncated_lognorm', 'truncated_lognorm_mode',\n                          'truncated_lognorm_mean'\n            noisy_readout: 'lognorm_median_std', 'lognorm_mean', 'lognorm_mode', 'lognorm_mode_std', 'lognorm_mean_std',\n                           'gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv',\n                           'betaprime_mean_std',\n                           'truncated_norm_mode_std', 'truncated_norm_mode',\n                           'truncated_gumbel_mode_std', 'truncated_gumbel_mode'\n            noisy_temperature: same as noisy_readout\n\n\n    *** Enable or disable specific parameters ***\n    * Each setting can take the values 0, 1 or 2:\n    *    0: Disable parameter.\n    *    1: Enable parameter.\n    *    2: Enable parameter and fit separate values for the negative and positive stimulus category\n            (works only for type 1 parameters!)\n    enable_type1_param_noise : int (default: 1)\n        Fit separate type 1 noise parameters for both stimulus categories.\n    enable_type1_param_noise_heteroscedastic : int (default: 0)\n        Fit an additional type 1 noise parameter for signal-dependent type 1 noise (the type of dependency is\n        defined via `type1_noise_signal_dependency`).\n    enable_type1_param_nonlinear_encoding_gain : int (default: 0)\n    enable_type1_param_nonlinear_encoding_transition : int (default: 0)\n    enable_type1_param_thresh : int (default: 0)\n        Fit a type 1 threshold.\n    enable_type1_param_bias : int (default: 1)\n        Fit a type 1 bias towards one of the stimulus categories.\n    enable_type2_param_noise : int (default: 1)\n        Fit a metacognitive noise parameter\n    enable_type2_param_evidence_bias_mult : int (default: 0)\n        Fit a multiplicative metacognitive bias loading on evidence.\n    enable_type2_param_criteria : int (default: 0)\n        Fit confidence criteria.\n\n    *** Additional options to specify the nature of type 2 fitting ***\n    n_discrete_confidence_levels : int (default: 5)\n        Number of confidence criteria. Only applies in case of type2_fitting_type='criteria'.\n\n    *** Define fitting characteristics of the parameters ***\n    * The fitting of each parameter is characzerized as follows:\n    *     1) An initial guess.\n    *     2) Lower and upper bound.\n    *     3) Grid range, i.e. list of values that are tested during the initial gridsearch search.\n    * Sensible default values are provided for all parameters. To tweak those, one can either define an entire\n    * ParameterSet, which is a container for a set of parameters, or each parameter individually. Note that the\n    * parameters must be either defined as a Parameter instance or as List[Parameter] in case when separate values are\n    * fitted for the positive and negative stimulus category/decision value).\n    paramset_type1 : ParameterSet\n        Parameter set for the type 1 stage.\n    paramset_type2 : ParameterSet\n        Parameter set for the type 2 stage.\n    paramset : ParameterSet\n        Parameter set for both stages.\n\n    _type1_param_noise : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for type 1 noise.\n    _type1_param_noise_heteroscedastic : Union[Parameter, List[Parameter]]  (default: 0)\n        Parameter for signal-dependent type 1 noise.\n    _type1_param_nonlinear_encoding_gain : Union[Parameter, List[Parameter]]  (default: 0)\n        Gain parameter for nonlinear encoding (higher values -&gt; stronger nonlinearity).\n    _type1_param_nonlinear_encoding_transition : Union[Parameter, List[Parameter]]  (default: 0)\n        Transition Parameter for nonlinear encoding ().\n    type1_noise_signal_dependency: str (default: 'none')\n        Can be one of 'none', 'multiplicative', 'power', 'exponential' or 'logarithm'.\n    _type1_param_thresh : Union[Parameter, List[Parameter]] (default: 0)\n        Parameter for the type 1 threshold.\n    _type1_param_bias : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for the type 1 bias.\n    _type2_param_noise : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for metacognitive noise.\n    _type2_param_evidence_bias_mult : Union[Parameter, List[Parameter]]  (default: 0)\n        Parameter for a multiplicative metacognitive bias loading on evidence.\n    type2_param_confidence_criteria : List[Parameter]  (default: 1)\n        List of parameter specifying the confidence criteria.\n\n    *** Skip type 2 fitting ***\n    skip_type2 : bool (default: False)\n        If True, ignore type 2 settings in the setup of the model configuration &amp; don't fit type 2 stage.\n\n    *** Methodoligcal aspects of parameter fitting ***\n    optim_type1_gridsearch : bool (default: False)\n        If True, perform initial (usually coarse) gridsearch search for type 1 fitting, based on the gridsearch defined\n        for a Parameter.\n    optim_type1_fine_gridsearch : bool (default: False)\n        If True, perform an iteratively finer gridsearch search for each parameter (type 1).\n    optim_type1_minimize_along_grid : bool (default: False)\n        If True, do sqlqp minimization for at each grid point (type 1).\n    optim_type1_global_minimization : str (default: None)\n        Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with\n        a global minimizer (type 1).\n    optim_type1_scipy_solvers : str or Tuple/List (default: 'trust-constr')\n        Set scipy.optimize.minimize gradient method (type 1)\n        If provided as Tuple/List, test different gradient methods and take the best\n    optim_type2_gridsearch : bool (default: True)\n        If True, perform initial (usually coarse) gridsearch search for type 2 fitting, based on the gridsearch defined\n        for a Parameter.\n    optim_type2_fine_gridsearch : bool (default: False)\n        If True, perform an iteratively finer gridsearch search for each parameter (type 2).\n    optim_type2_minimize_along_grid : bool (default: False)\n        If True, do sqlqp minimization for at each grid point (type 2).\n    optim_type2_global_minimization : str (default: None)\n        Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with\n        a global minimizer (type 2).\n    optim_type2_scipy_solvers : str or Tuple/List (default: ('slsqp', 'Nelder-Mead'))\n        Set scipy.optimize.minimize gradient method (type 2)\n        If provided as Tuple/List, test different gradient methods and take the best\n    optim_type2_slsqp_epsilon : float or Tuple/List (default: None)\n        Set parameter epsilon parameter for the SLSQP optimization method (type 2).\n        If provided as Tuple/List, test different eps parameters and take the best\n    optim_multiproc : bool (default: False)\n        If True, use all optim_multiproc_cores cores for parameter estimation. If False, use a single core.\n    optim_multiproc_cores : int (default: -1)\n        If multicore processing es enabled via optim_multiproc, use optim_multiproc_cores cores for parameter estimation\n        (-1 for all cores minus 1).\n\n    *** Preprocessing ***\n    normalize_stimuli_by_max : bool (default: True)\n        If True, normalize provided stimuli by their maximum value.\n\n    *** Parameters for the type 2 likelihood computation ***\n    min_type1_likelihood : float\n        Minimum probability used during the type 1 likelihood computation\n    min_type2_likelihood : float\n        Minimum probability used during the type 2 likelihood computation\n    type2_binsize : float\n        Integration bin size for the computation of the likelihood around empirical confidence values\n    y_decval_range_nsds : int\n        Number of standard deviations around the mean considered for type 1 uncertainty.\n    y_decval_range_nbins : int\n        Number of discrete decision values bins that are considered to represent type 1 uncertainty.\n    resolution_noisy_temperature : float\n        Quintile resolution for the marginalization of type 1 noise in case of type2_noise_type 'noisy_temperature'.\n    experimental_min_uniform_type2_likelihood : bool\n        Instead of using a minimum probability during the likelihood computation, use a maximum cumulative\n        likelihood based on a 'guessing' model\n    experimental_wrap_type2_integration_window : bool (default: False)\n        Ensure constant window size for likelihood integration at the bounds.\n        Only applies in case of type2_fitting_type='continuous' and experimental_disable_type2_binsize=False\n    experimental_include_incongruent_y_decval : bool (default: False)\n        Include incongruent decision values (i.e., sign(actual choice) != sign(decision value)) for the likelihood\n        computation\n    experimental_disable_type2_binsize : bool (default: None)\n        Do not use an integegration window for likelihood computation.\n        Only applies in case of type2_fitting_type='continuous'\n\n\n    *** Other ***\n    true_params : Dict\n        Pass true (known) parameter values. This can be useful for testing to compare the likelihood of true and\n        fitted parameters. The likelihood of true parameters is returned (and printed).\n    initilialize_fitting_at_true_params : bool (default: False)\n        Option to initialize the parameter fitting procedure at the true parameters; this can be helpful for testing.\n    silence_configuration_warnings : bool (default: False)\n        If True, ignore warnings about user-specified settings.\n    print_configuration : bool (default: True)\n        If True, print the configuration at instatiation of the ReMeta class.\n    \"\"\"\n\n    type2_fitting_type: str = 'criteria'\n    type2_noise_type: str = 'noisy_report'\n    type2_noise_dist: str = None\n        # noisy-report + criteria -&gt; 'truncated_norm_mode'\n        # noisy-report + continuous -&gt; 'truncated_norm_mode'\n        # noisy-readout + criteria -&gt; 'truncated_norm_mode'\n        # noisy-readout + continuous -&gt; 'truncated_norm_mode'\n        # noisy-temperature + criteria -&gt; 'lognorm_mode'\n        # noisy-temperature + continuous -&gt; 'truncated_norm_mode'\n\n    enable_type1_param_noise: int = 1\n    enable_type1_param_thresh: int = 0\n    enable_type1_param_bias: int = 1\n    enable_type2_param_noise: int = 1\n    enable_type2_param_evidence_bias_mult: int = 0\n    enable_type2_param_criteria: int = 1\n    # Experimental:\n    enable_type1_param_noise_heteroscedastic: int = 0\n    enable_type1_param_nonlinear_encoding_gain: int = 0\n    enable_type1_param_nonlinear_encoding_transition: int = 0\n\n    n_discrete_confidence_levels: int = 5\n\n    paramset_type1: ParameterSet = None\n    paramset_type2: ParameterSet = None\n    paramset_all: ParameterSet = None\n\n    type1_param_noise_heteroscedastic: Parameter = Parameter(guess=0, bounds=(0, 10), grid_range=np.linspace(0, 1, 5))\n    type1_param_nonlinear_encoding_gain: Parameter = Parameter(guess=0, bounds=(-8/9, 10), grid_range=np.linspace(-0.5, 1, 5))\n    type1_param_nonlinear_encoding_transition: Parameter = Parameter(guess=1, bounds=(0.01, 10), grid_range=np.linspace(0.01, 2, 5))\n    type1_param_noise: Parameter = Parameter(guess=0.5, bounds=(0.001, 100), grid_range=np.linspace(0.1, 1, 8))\n    type1_param_thresh: Parameter = Parameter(guess=0, bounds=(0, 1), grid_range=np.linspace(0, 0.2, 5))\n    type1_param_bias: Parameter = Parameter(guess=0, bounds=(-1, 1), grid_range=np.linspace(-0.2, 0.2, 8))\n    type2_param_noise: Parameter = Parameter(guess=0.1, bounds=(0.05, 2), grid_range=np.linspace(0.1, 1, 8))\n    type2_param_evidence_bias_mult: Parameter = Parameter(guess=1, bounds=(0.5, 2), grid_range=np.linspace(0.5, 2, 8))\n    type2_param_criteria: Parameter = Parameter(bounds=(1e-8, 1))\n    type2_param_criteria_guesses: str | List[float] = 'equidistant'\n    type2_param_criteria_grid_ranges: str | List[np.ndarray] = 'equidistant'\n\n    type1_noise_signal_dependency: str = 'none'\n\n    skip_type2 = False\n\n    optim_type1_gridsearch: bool = False\n    optim_type1_fine_gridsearch: bool = False\n    optim_type1_minimize_along_grid: bool = False\n    optim_type1_global_minimization: str = None\n    _optim_type1_scipy_solvers_default = 'trust-constr'\n    optim_type1_scipy_solvers: str | List[str] | Tuple[str, ...] = 'trust-constr'\n    optim_type2_gridsearch: bool = True\n    optim_type2_fine_gridsearch: bool = False\n    optim_type2_minimize_along_grid: bool = False\n    optim_type2_global_minimization: str = None\n    optim_type2_scipy_solvers: str | List[str] | Tuple[str, ...] = ('slsqp', 'Nelder-Mead')\n    optim_type2_slsqp_epsilon: float = None\n    optim_multiproc: bool = False\n    optim_multiproc_cores: int = -1\n    _optim_multiproc_cores_effective: int = None\n\n    normalize_stimuli_by_max: bool = True\n    confidence_bounds_error: float = 0\n\n    min_type2_likelihood: float = 1e-10\n    min_type1_likelihood: float = 1e-10\n    type2_binsize: float = 0.01\n    y_decval_range_nsds: int = 5\n    y_decval_range_nbins: int = 101\n    resolution_noisy_temperature: float = 0.001\n\n    experimental_min_uniform_type2_likelihood: bool = False\n    experimental_wrap_type2_integration_window: bool = False\n    experimental_include_incongruent_y_decval: bool = False\n    experimental_disable_type2_binsize: bool = False\n\n    true_params: Dict = None\n    initilialize_fitting_at_true_params: bool = False\n    silence_configuration_warnings: bool = False\n    print_configuration: bool = False\n\n    type2_param_noise_min: float = 0.001\n\n    # setup_called = False\n\n    _type1_param_noise: Parameter | List[Parameter] = None\n    _type1_param_noise_heteroscedastic: Parameter | List[Parameter] = None\n    _type1_param_nonlinear_encoding_transition: Parameter | List[Parameter] = None\n    _type1_param_nonlinear_encoding_gain: Parameter | List[Parameter] = None\n    _type1_param_thresh: Parameter | List[Parameter] = None\n    _type1_param_bias: Parameter | List[Parameter] = None\n    _type2_param_noise: Parameter = None\n    _type2_param_evidence_bias_mult: Parameter = None\n    _type2_param_criteria: List[Parameter] = None\n\n    def setup(self, generative_mode=False):\n\n        if find_spec('multiprocessing_on_dill') is None:\n            warnings.warn(f'Multiprocessing on dill is not installed. Setting grid_multiproc is changed to False.')\n            self.optim_multiproc = False\n\n        if self.optim_multiproc:\n            from multiprocessing import cpu_count\n            self._optim_multiproc_cores_effective = max(1, (cpu_count() or 1) - 1) if self.optim_multiproc_cores == -1 \\\n                else self.optim_multiproc_cores\n\n        self._prepare_params_type1()\n        if self.skip_type2:\n            if self.optim_type2_slsqp_epsilon is None:\n                self.optim_type2_slsqp_epsilon = 1e-5\n        else:\n\n            if self.enable_type1_param_thresh and \\\n                (self.optim_type1_scipy_solvers == self._optim_type1_scipy_solvers_default):\n                self.optim_type1_scipy_solvers = ('trust-constr', 'Powell')\n\n\n            if self.type2_noise_dist is None:\n                if generative_mode:\n                    raise ValueError('In generative mode, you need to explicitly specify a type 2 noise distribution.')\n                else:\n                    if self.type2_noise_type == 'noisy_report':\n                        if (self.type2_fitting_type == 'criteria'):\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                    elif (self.type2_noise_type == 'noisy_readout'):\n                        if self.type2_fitting_type == 'criteria':\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                    elif self.type2_noise_type == 'noisy_temperature':\n                        if self.type2_fitting_type == 'criteria':\n                            self.type2_noise_dist = 'lognorm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n\n            self._prepare_params_type2()\n            if self.optim_type2_slsqp_epsilon is None:\n                self.optim_type2_slsqp_epsilon = 1e-5\n\n            if self.type2_binsize is None:\n                self.type2_binsize = 0.01\n\n        self._prepare_params_all()\n\n        self._check_compatibility(generative_mode=generative_mode)\n\n        if self.print_configuration:\n            self.print()\n        # self.setup_called = True\n\n    def _check_compatibility(self, generative_mode=False):\n\n        if not self.silence_configuration_warnings:\n\n            if not self.skip_type2:\n                if not self.enable_type2_param_noise:\n                    warnings.warn(f'Setting enable_type2_param_noise=False was provided -&gt; type2_param_noise is set to its default value '\n                                  f'({self._type2_param_noise_default}). You may change this value via the configuration.')\n\n                if (self.type2_noise_type == 'noisy_temperature') and self.type2_param_noise.default_changed and \\\n                    (self.type2_param_noise.bounds[0] &lt; 1e-5):\n                    warnings.warn('You manually changed the lower bound of the type 2 noise parameter for a '\n                                  'noisy-temperature model to a very low value (&lt;1e-5). Be warned that this may result '\n                                  'in numerical instabilities that severely distort the likelihood computation.')\n\n                if not generative_mode:\n                    # If the configuration instance is used for generating data, we should not complain\n                    # about fitting issues.\n\n                    if self.enable_type2_param_criteria and self.enable_type2_param_evidence_bias_mult:\n                        warnings.warn(\n                            'enable_type2_param_criteria=True in combination with enable_type2_param_evidence_bias_mult=True\\n'\n                            'can lead to biased parameter inferences. Use with caution.')\n\n                    if (self.type2_fitting_type == 'continuous') and self.enable_type2_param_criteria:\n                        raise ValueError(\"Setting type2_fitting_type='continuous' conflicts with enable_type2_param_criteria=1.'\")\n\n                    if (self.type2_fitting_type == 'criteria') and not self.enable_type2_param_criteria:\n                        warnings.warn(\"You selected type2_fitting_type='criteria', but did not enable type 2 criteria\\n\"\n                                      \"(enable_type2_param_criteria=0). This works, but be mindful that the model\\n\"\n                                      \"will assume equispaced ideal Bayesian observer criteria (respecting \\n\"\n                                      \"the setting n_discrete_confidence_levels).\")\n\n    def _prepare_params_type1(self):\n        # if self.paramset_type1 is None:\n\n            param_names_type1 = []\n            params_type1 = ('noise', 'noise_heteroscedastic', 'nonlinear_encoding_gain', 'nonlinear_encoding_transition', 'thresh', 'bias')\n            for param in params_type1:\n                if getattr(self, f'enable_type1_param_{param}'):\n                    param_names_type1 += [f'type1_{param}']\n                    if getattr(self, f'_type1_param_{param}') is None:\n                        param_definition = getattr(self, f'type1_param_{param}')\n                        if getattr(self, f'enable_type1_param_{param}') == 2:\n                            setattr(self, f'_type1_param_{param}', [param_definition, param_definition])\n                        else:\n                            setattr(self, f'_type1_param_{param}', param_definition)\n                        if self.true_params is not None and self.initilialize_fitting_at_true_params and f'type1_{param}' in self.true_params:\n                            getattr(self, f'_type1_param_{param}').guess = self.true_params[f'type1_{param}']\n\n            parameters = {k: getattr(self, f\"_type1_param_{k.split('type1_')[1]}\") for k in param_names_type1}\n            self.paramset_type1 = ParameterSet(parameters, param_names_type1)\n\n    def _prepare_params_type2(self):\n\n        # if self.paramset_type2 is None:\n\n            if self.enable_type2_param_noise and self._type2_param_noise is None and not self.type2_param_noise.default_changed:\n\n                lb = 0.05\n                self.type2_param_noise.bounds = dict(\n                    noisy_report = dict(\n                        beta_mean_std=(lb, 0.5),\n                        beta_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_norm_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_gumbel_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_lognorm_mode_std=(lb, 1 / np.sqrt(12)),\n                        beta_mode=(lb, 1),\n                        truncated_norm_mode=(lb, 1),\n                        truncated_gumbel_mode=(lb, 1),\n                        truncated_lognorm_mode=(lb, 4),\n                        truncated_lognorm_mean=(lb, 4),\n                        truncated_lognorm=(lb, 4)\n                    ),\n                    noisy_readout = dict(\n                        lognorm_mean=(lb, 1),\n                        lognorm_mode=(lb, 1),\n                        gamma_mean_std=(lb, 1),\n                        lognorm_mean_std=(lb, 2),\n                        lognorm_mode_std=(lb, 2),\n                        lognorm_median_std=(lb, 2),\n                        gamma_mean_cv=(lb, 2),\n                        gamma_mean=(lb, 2),\n                        gamma_mode_std=(lb, 2),\n                        gamma_mode=(lb, 2),\n                        betaprime_mean_std=(lb, 2),\n                        truncated_norm_mode_std=(lb, 2),\n                        truncated_norm_mode=(lb, 2),\n                        truncated_gumbel_mode_std=(lb, 2),\n                        truncated_gumbel_mode=(lb, 2)\n                    ),\n                    noisy_temperature = dict(\n                        lognorm_mean=(lb, 1),\n                        gamma_mean_std=(lb, 1),\n                        lognorm_mean_std=(lb, 2),\n                        lognorm_median_std=(lb, 2),\n                        gamma_mean_cv=(lb, 2),\n                        gamma_mean=(lb, 2),\n                        gamma_mode_std=(lb, 2),\n                        gamma_mode=(lb, 2),\n                        betaprime_mean_std=(lb, 2),\n                        truncated_norm_mode_std=(lb, 2),\n                        truncated_norm_mode=(lb, 2),\n                        truncated_gumbel_mode_std=(lb, 2),\n                        truncated_gumbel_mode=(lb, 2),\n                        lognorm_mode=(lb, 4),\n                        lognorm_mode_std=(lb, 10),\n                    )\n                )[self.type2_noise_type][self.type2_noise_dist]\n                self.type2_param_noise.grid_range = np.exp(np.linspace(np.log(self.type2_param_noise.bounds[0]),\n                                                                       np.log(self.type2_param_noise.bounds[1]), 10)[1:-1])\n\n            param_names_type2 = []\n            params_type2 = ('noise', 'evidence_bias_mult')\n            for param in params_type2:\n                if getattr(self, f'enable_type2_param_{param}'):\n                    param_names_type2 += [f'type2_{param}']\n                    if getattr(self, f'_type2_param_{param}') is None:\n                        param_definition = getattr(self, f'type2_param_{param}')\n                        setattr(self, f'_type2_param_{param}', param_definition.copy())\n                        if self.true_params is not None and self.initilialize_fitting_at_true_params and f'type2_{param}' in self.true_params:\n                            getattr(self, f'_type2_param_{param}').guess = self.true_params[f'type2_{param}']\n\n\n            if self.enable_type2_param_criteria:\n                param_names_type2 += [f'type2_criteria']\n                initialize_true = (self.initilialize_fitting_at_true_params and\n                                   self.true_params is not None and 'type2_criteria' in self.true_params)\n                setattr(self, f'_type2_param_criteria',\n                        [Parameter(\n                           guess=self.true_params['type2_criteria'][i] if initialize_true\n                                    else (1 / self.n_discrete_confidence_levels if self.type2_param_criteria_guesses == 'equidistant'\n                                          else self.type2_param_criteria_guesses[i]),\n                           bounds=self.type2_param_criteria.bounds,\n                           grid_range=np.linspace(0.05, 2 / self.n_discrete_confidence_levels, 4) if\n                                self.type2_param_criteria_grid_ranges == 'equidistant' else self.type2_param_criteria_grid_ranges[i]\n                        )\n                         for i in range(self.n_discrete_confidence_levels - 1)]\n                        )\n                if self.true_params is not None:\n                    if isinstance(self.true_params, dict):\n                        # if 'type2_criteria' not in self.true_params:\n                        #     raise ValueError('type2_criteria are missing from cfg.true_params')\n                        if 'type2_criteria' in self.true_params:\n                            self.true_params.update(\n                                type2_criteria_absolute=[np.sum(self.true_params['type2_criteria'][:i+1]) for i in range(len(self.true_params['type2_criteria']))],\n                                type2_criteria_bias=np.mean(self.true_params['type2_criteria'])*(len(self.true_params['type2_criteria'])+1)-1\n                            )\n                    elif isinstance(self.true_params, list):\n                        for s in range(len(self.true_params)):\n                            # if 'type2_criteria' not in self.true_params[s]:\n                            #     raise ValueError(f'type2_criteria are missing from cfg.true_params (subject {s})')\n                            if 'type2_criteria' in self.true_params[s]:\n                                self.true_params[s].update(\n                                    type2_criteria_absolute=[np.sum(self.true_params[s]['type2_criteria'][:i+1]) for i in range(len(self.true_params[s]['type2_criteria']))],\n                                    type2_criteria_bias=np.mean(self.true_params[s]['type2_criteria'])*(len(self.true_params[s]['type2_criteria'])+1)-1\n                                )\n\n            parameters = {k: getattr(self, f\"_type2_param_{k.split('type2_')[1]}\") for k in param_names_type2}\n            self.paramset_type2 = ParameterSet(parameters, param_names_type2)\n\n\n            self.check_type2_constraints()\n\n\n    def _prepare_params_all(self):\n\n        if self.skip_type2:\n            self.paramset = self.paramset_type1\n        else:\n            parameters_all = {**self.paramset_type1.parameters, **self.paramset_type2.parameters}\n            param_names_all = self.paramset_type1.param_names + self.paramset_type2.param_names\n            self.paramset = ParameterSet(parameters_all, param_names_all)\n            # for k, attr in self.paramset_type2.__dict__.items():\n            #     attr_old = getattr(self.paramset, k)\n            #     if isinstance(attr, list):\n            #         attr_new = attr_old + attr\n            #     elif isinstance(attr, dict):\n            #         attr_new = {**attr_old, **attr}\n            #     elif isinstance(attr, np.ndarray):\n            #         if attr.ndim == 1:\n            #             attr_new = np.hstack((attr_old, attr))\n            #         else:\n            #             attr_new = np.vstack((attr_old, attr))\n            #     elif isinstance(attr, int):\n            #         attr_new = attr_old + attr\n            #     elif attr is None:\n            #         if attr_old is None:\n            #             attr_new = None\n            #         else:\n            #             raise ValueError(f'Type 2 attribute is None, but type 1 attribute is not.')\n            #     else:\n            #         raise ValueError(f'Unexpected type {type(attr)}')\n            #     setattr(self.paramset, k, attr_new)\n\n\n\n\n    def print(self):\n        # print('***********************')\n        print(f'{self.__class__.__name__}')\n        for k, v in self.__dict__.items():\n            # if not self.skip_type2 or ('type2' not in k):\n            print('\\n'.join([f'\\t{k}: {v}']))\n        # print('***********************')\n\n    def __repr__(self):\n        txt = f'{self.__class__.__name__}\\n'\n        txt += '\\n'.join([f'\\t{k}: {v}' for k, v in self.__dict__.items()])\n        return txt\n\n    def check_type2_constraints(self):\n        pass\n</code></pre>"},{"location":"configuration/#remeta.configuration.Parameter","title":"Parameter","text":"Source code in <code>remeta/modelspec.py</code> <pre><code>class Parameter(ReprMixin):\n    def __init__(self, guess=None, bounds=None, grid_range=None, group=None, prior=None):\n        \"\"\"\n        Class that defines the fitting characteristics of a Parameter.\n\n        Parameters\n        ----------\n        guess : None | float | np.floating\n            Initial guess for the parameter value.\n        bounds: None | array-like of length 2\n            Parameter bounds. The first and second element indicate the lower and upper bound of the parameter.\n        grid_range: None | array-like (1d)\n            1-d grid for initial gridsearch in the parameter optimization procedure\n        group: None | str\n            None: no group-level estimate for the parameter\n            'fixed': fit parameter as a group fixed effect (i.e., single value for the group)\n            'random': fit parameter as a random effect (enforces shrinkage towards a group mean)\n       prior: None | tuple[float, float]\n            None: no prior for the parameter\n            (group_mean, group_sd): apply a Normal prior defined by mean and standard deviation\n\n        \"\"\"\n        self.guess = guess\n        self.bounds = bounds\n        self.grid_range = np.linspace(bounds[0], bounds[1], 4) if grid_range is None else grid_range\n        self.group = group\n        self.prior = prior\n        self.default_changed = False\n\n    def copy(self):\n        return Parameter(self.guess, self.bounds, self.grid_range, self.group, self.prior)\n\n    def __setattr__(self, name, value):\n        if name != \"default_changed\" and hasattr(self, name):\n            old_value = getattr(self, name)\n            if isinstance(old_value, np.ndarray) and isinstance(value, np.ndarray):\n                changed = not np.array_equal(old_value, value)\n            else:\n                changed = old_value != value\n            if changed:\n                super().__setattr__(\"default_changed\", True)\n\n        super().__setattr__(name, value)\n</code></pre>"},{"location":"configuration/#remeta.configuration.Parameter.__init__","title":"__init__","text":"<pre><code>__init__(guess=None, bounds=None, grid_range=None, group=None, prior=None)\n</code></pre> <p>Class that defines the fitting characteristics of a Parameter.</p> <p>Parameters</p> <p>guess : None | float | np.floating      Initial guess for the parameter value.  bounds: None | array-like of length 2      Parameter bounds. The first and second element indicate the lower and upper bound of the parameter.  grid_range: None | array-like (1d)      1-d grid for initial gridsearch in the parameter optimization procedure  group: None | str      None: no group-level estimate for the parameter      'fixed': fit parameter as a group fixed effect (i.e., single value for the group)      'random': fit parameter as a random effect (enforces shrinkage towards a group mean) prior: None | tuple[float, float]      None: no prior for the parameter      (group_mean, group_sd): apply a Normal prior defined by mean and standard deviation</p> Source code in <code>remeta/modelspec.py</code> <pre><code>def __init__(self, guess=None, bounds=None, grid_range=None, group=None, prior=None):\n    \"\"\"\n    Class that defines the fitting characteristics of a Parameter.\n\n    Parameters\n    ----------\n    guess : None | float | np.floating\n        Initial guess for the parameter value.\n    bounds: None | array-like of length 2\n        Parameter bounds. The first and second element indicate the lower and upper bound of the parameter.\n    grid_range: None | array-like (1d)\n        1-d grid for initial gridsearch in the parameter optimization procedure\n    group: None | str\n        None: no group-level estimate for the parameter\n        'fixed': fit parameter as a group fixed effect (i.e., single value for the group)\n        'random': fit parameter as a random effect (enforces shrinkage towards a group mean)\n   prior: None | tuple[float, float]\n        None: no prior for the parameter\n        (group_mean, group_sd): apply a Normal prior defined by mean and standard deviation\n\n    \"\"\"\n    self.guess = guess\n    self.bounds = bounds\n    self.grid_range = np.linspace(bounds[0], bounds[1], 4) if grid_range is None else grid_range\n    self.group = group\n    self.prior = prior\n    self.default_changed = False\n</code></pre>"},{"location":"configuration/#remeta.configuration.ParameterSet","title":"ParameterSet","text":"Source code in <code>remeta/modelspec.py</code> <pre><code>class ParameterSet(ReprMixin):\n    def __init__(self, parameters, param_names, constraints=None):\n        \"\"\"\n        Container class for all Parameters of a model.\n\n        Parameters\n        ----------\n        parameters : dict[str, Parameter]\n            The dictionary must have the form {parameter_name1: Parameter(..), parameter_name2: Parameter(..), ..}\n        param_names: List[str]\n            List of parameter names of a model.\n        constraints: List[Dict]\n            List of scipy minimize constraints. Each constraint is a dictionary with keys 'type' and 'fun', where\n            'type' is \u2018eq\u2019 for equality and \u2018ineq\u2019 for inequality, and where fun is a function defining the constraint.\n        \"\"\"\n\n        self.parameters = parameters\n        self.param_names = param_names\n        self.param_is_list = [isinstance(parameters[name], list) for name in param_names]\n        self.param_len = {name: len(parameters[name]) if self.param_is_list[p] else 1 for p, name in enumerate(param_names)}  # noqa\n        self.param_len_list = [len(parameters[name]) if self.param_is_list[p] else 1 for p, name in enumerate(param_names)]  # noqa\n        self.param_names_flat = sum([[f'{name}_{i}' for i in range(len(parameters[name]))] if self.param_is_list[p]  # noqa\n                          else [name] for p, name in enumerate(param_names)], [])\n        parameters_flat_list_ = sum([[param[i] for i in range(len(param))] if self.param_is_list[p] else [param] for\n                                     p, param in enumerate(parameters.values())], [])\n        self.parameters_flat = {name: param for name, param in zip(self.param_names_flat, parameters_flat_list_)}\n        self.guess = np.array(sum([[parameters[name][i].guess for i in range(len(parameters[name]))] if  # noqa\n                                   self.param_is_list[p] else [parameters[name].guess] for p, name in enumerate(param_names)], []))\n        self.bounds = np.array(sum([[parameters[name][i].bounds for i in range(len(parameters[name]))] if  # noqa\n                                   self.param_is_list[p] else [parameters[name].bounds] for p, name in enumerate(param_names)], []))\n        self.grid_range = sum([[parameters[name][i].grid_range for i in range(len(parameters[name]))] if  # noqa\n                               self.param_is_list[p] else [parameters[name].grid_range] for p, name in enumerate(param_names)], [])\n        self.constraints = constraints\n        self.nparams = len(param_names)\n        self.nparams_flat = len(self.param_names_flat)\n        self.param_ind = {name: np.arange(int(np.sum(self.param_len_list[:i])), np.sum(self.param_len_list[:i + 1])).squeeze() for i, name in enumerate(self.param_names)}\n        self.param_ind_list = [np.arange(int(np.sum(self.param_len_list[:i])), np.sum(self.param_len_list[:i + 1])).squeeze() for i in range(self.nparams)]\n        self.param_revind_re = {k: np.arange(l := (0 if i == 0 else l), l := (l + self.param_len_list[j])) for i, (k, j) in\n                                enumerate({k_: j_ for j_, (k_, param) in enumerate(self.parameters.items()) if (param[0] if self.param_is_list[j_] else param).group == 'random'}.items())}\n        self.param_revind_fe = {k: np.arange(l := (0 if i == 0 else l), l := (l + self.param_len_list[j])) for i, (k, j) in\n                                enumerate({k_: j_ for j_, (k_, param) in enumerate(self.parameters.items()) if (param[0] if self.param_is_list[j_] else param).group == 'fixed'}.items())}\n</code></pre>"},{"location":"configuration/#remeta.configuration.ParameterSet.__init__","title":"__init__","text":"<pre><code>__init__(parameters, param_names, constraints=None)\n</code></pre> <p>Container class for all Parameters of a model.</p>"},{"location":"configuration/#remeta.configuration.ParameterSet.__init__--parameters","title":"Parameters","text":"<p>parameters : dict[str, Parameter]     The dictionary must have the form {parameter_name1: Parameter(..), parameter_name2: Parameter(..), ..} param_names: List[str]     List of parameter names of a model. constraints: List[Dict]     List of scipy minimize constraints. Each constraint is a dictionary with keys 'type' and 'fun', where     'type' is \u2018eq\u2019 for equality and \u2018ineq\u2019 for inequality, and where fun is a function defining the constraint.</p> Source code in <code>remeta/modelspec.py</code> <pre><code>def __init__(self, parameters, param_names, constraints=None):\n    \"\"\"\n    Container class for all Parameters of a model.\n\n    Parameters\n    ----------\n    parameters : dict[str, Parameter]\n        The dictionary must have the form {parameter_name1: Parameter(..), parameter_name2: Parameter(..), ..}\n    param_names: List[str]\n        List of parameter names of a model.\n    constraints: List[Dict]\n        List of scipy minimize constraints. Each constraint is a dictionary with keys 'type' and 'fun', where\n        'type' is \u2018eq\u2019 for equality and \u2018ineq\u2019 for inequality, and where fun is a function defining the constraint.\n    \"\"\"\n\n    self.parameters = parameters\n    self.param_names = param_names\n    self.param_is_list = [isinstance(parameters[name], list) for name in param_names]\n    self.param_len = {name: len(parameters[name]) if self.param_is_list[p] else 1 for p, name in enumerate(param_names)}  # noqa\n    self.param_len_list = [len(parameters[name]) if self.param_is_list[p] else 1 for p, name in enumerate(param_names)]  # noqa\n    self.param_names_flat = sum([[f'{name}_{i}' for i in range(len(parameters[name]))] if self.param_is_list[p]  # noqa\n                      else [name] for p, name in enumerate(param_names)], [])\n    parameters_flat_list_ = sum([[param[i] for i in range(len(param))] if self.param_is_list[p] else [param] for\n                                 p, param in enumerate(parameters.values())], [])\n    self.parameters_flat = {name: param for name, param in zip(self.param_names_flat, parameters_flat_list_)}\n    self.guess = np.array(sum([[parameters[name][i].guess for i in range(len(parameters[name]))] if  # noqa\n                               self.param_is_list[p] else [parameters[name].guess] for p, name in enumerate(param_names)], []))\n    self.bounds = np.array(sum([[parameters[name][i].bounds for i in range(len(parameters[name]))] if  # noqa\n                               self.param_is_list[p] else [parameters[name].bounds] for p, name in enumerate(param_names)], []))\n    self.grid_range = sum([[parameters[name][i].grid_range for i in range(len(parameters[name]))] if  # noqa\n                           self.param_is_list[p] else [parameters[name].grid_range] for p, name in enumerate(param_names)], [])\n    self.constraints = constraints\n    self.nparams = len(param_names)\n    self.nparams_flat = len(self.param_names_flat)\n    self.param_ind = {name: np.arange(int(np.sum(self.param_len_list[:i])), np.sum(self.param_len_list[:i + 1])).squeeze() for i, name in enumerate(self.param_names)}\n    self.param_ind_list = [np.arange(int(np.sum(self.param_len_list[:i])), np.sum(self.param_len_list[:i + 1])).squeeze() for i in range(self.nparams)]\n    self.param_revind_re = {k: np.arange(l := (0 if i == 0 else l), l := (l + self.param_len_list[j])) for i, (k, j) in\n                            enumerate({k_: j_ for j_, (k_, param) in enumerate(self.parameters.items()) if (param[0] if self.param_is_list[j_] else param).group == 'random'}.items())}\n    self.param_revind_fe = {k: np.arange(l := (0 if i == 0 else l), l := (l + self.param_len_list[j])) for i, (k, j) in\n                            enumerate({k_: j_ for j_, (k_, param) in enumerate(self.parameters.items()) if (param[0] if self.param_is_list[j_] else param).group == 'fixed'}.items())}\n</code></pre>"},{"location":"configuration/#remeta.configuration.ReprMixin","title":"ReprMixin","text":"Source code in <code>remeta/util.py</code> <pre><code>class ReprMixin:\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}\\n' + '\\n'.join([f'\\t{k}: {v}' for k, v in self.__dict__.items()])\n</code></pre>"},{"location":"configuration/#remeta.configuration.reset_dataclass_on_init","title":"reset_dataclass_on_init","text":"<pre><code>reset_dataclass_on_init(cls)\n</code></pre> <p>For a @dataclass:   - On each new instance, reset:       * non-field class attributes       * dataclass fields that have a direct default (not default_factory)     back to the values they had at class definition time.</p> Source code in <code>remeta/util.py</code> <pre><code>def reset_dataclass_on_init(cls):\n    \"\"\"\n    For a @dataclass:\n      - On each new instance, reset:\n          * non-field class attributes\n          * dataclass fields that have a direct default (not default_factory)\n        back to the values they had at class definition time.\n    \"\"\"\n\n    # ---- 1) Capture non-field class attributes (true class attrs) ----\n    dataclass_field_names = set(cls.__dataclass_fields__.keys())\n\n    original_class_attrs = {}\n    for name, value in cls.__dict__.items():\n        if name.startswith(\"__\") and name.endswith(\"__\"):\n            continue\n        if name in dataclass_field_names:\n            # handled separately as dataclass fields\n            continue\n        if callable(value):\n            continue\n\n        original_class_attrs[name] = deepcopy(value)\n\n    # ---- 2) Capture dataclass field defaults (for __init__ defaults + class attrs) ----\n    dc_fields = list(fields(cls))\n\n    # fields that appear as parameters with defaults in __init__\n    fields_with_any_default = []\n    field_default_kinds = []       # \"value\" or \"factory\"\n    stored_field_defaults = []     # original defaults or factories\n\n    for f in dc_fields:\n        if f.default is not MISSING or f.default_factory is not MISSING:\n            fields_with_any_default.append(f)\n            if f.default_factory is not MISSING:\n                field_default_kinds.append(\"factory\")\n                stored_field_defaults.append(f.default_factory)\n            else:\n                field_default_kinds.append(\"value\")\n                stored_field_defaults.append(deepcopy(f.default))\n\n    # For fields with a *value* default, there is usually a class attribute with that name.\n    # We'll reset that class attribute too.\n    field_class_default_indices = {}\n    for idx, (f, kind, stored) in enumerate(\n        zip(fields_with_any_default, field_default_kinds, stored_field_defaults)\n    ):\n        if kind == \"value\" and hasattr(cls, f.name):\n            field_class_default_indices[f.name] = idx\n\n    # ---- 3) Wrap the original __init__ ----\n    original_init = cls.__init__\n\n    # Sanity: number of __init__ defaults should match number of fields with defaults\n    orig_defaults_len = len(original_init.__defaults__ or ())\n    if orig_defaults_len != len(fields_with_any_default):\n        # You *can* turn this into an assert if you prefer it to fail loudly\n        pass\n\n    def __init__(self, *args, **kwargs):\n        # 3a) reset non-field class attributes\n        for name, value in original_class_attrs.items():\n            setattr(cls, name, deepcopy(value))\n\n        # 3b) reset class attributes for dataclass fields with direct defaults\n        for fname, idx in field_class_default_indices.items():\n            setattr(cls, fname, deepcopy(stored_field_defaults[idx]))\n\n        # 3c) rebuild the original __init__ defaults for this call\n        if fields_with_any_default:\n            new_defaults = []\n            for kind, stored in zip(field_default_kinds, stored_field_defaults):\n                if kind == \"factory\":\n                    # default_factory fields keep their original factory\n                    new_defaults.append(stored)\n                else:\n                    # direct defaults get a fresh deep copy\n                    new_defaults.append(deepcopy(stored))\n            original_init.__defaults__ = tuple(new_defaults)\n\n        # Call the real dataclass-generated __init__\n        original_init(self, *args, **kwargs)\n\n    cls.__init__ = __init__\n    return cls\n</code></pre>"},{"location":"gendata/","title":"remeta.gendata","text":""},{"location":"gendata/#remeta.gendata.TAB","title":"TAB  <code>module-attribute</code>","text":"<pre><code>TAB = '    '\n</code></pre>"},{"location":"gendata/#remeta.gendata.m","title":"m  <code>module-attribute</code>","text":"<pre><code>m = simu_data(params, nsubjects=1, nsamples=1000, **options)\n</code></pre>"},{"location":"gendata/#remeta.gendata.options","title":"options  <code>module-attribute</code>","text":"<pre><code>options = dict(meta_noise_type='noisy_report', enable_type1_param_thresh=1, enable_type1_param_bias=1, enable_type2_param_evidence_bias_mult=0, type2_noise_dist='beta_mode')\n</code></pre>"},{"location":"gendata/#remeta.gendata.params","title":"params  <code>module-attribute</code>","text":"<pre><code>params = dict(type1_noise=0.2, type1_thresh=0.2, type1_bias=0.2, type2_noise=0.2)\n</code></pre>"},{"location":"gendata/#remeta.gendata.Configuration","title":"Configuration  <code>dataclass</code>","text":"<p>Configuration for the ReMeta toolbox</p>"},{"location":"gendata/#remeta.gendata.Configuration--parameters","title":"Parameters","text":"<p>*** Basic definition of the model *** type2_fitting_type : str (default: 'criteria')     Whether confidence is fitted with discrete criteria or as a continuous variable.     Possible values: 'criteria', 'continuous' type2_noise_type : str (default: 'noisy-report)     Whether the model considers noise at readout or report.     Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature' type2_noise_dist : str         (default: noisy-report + criteria -&gt; 'truncated_norm_mode'                   noisy-report + continuous -&gt; 'truncated_norm_mode'                   noisy-readout + criteria -&gt; 'truncated_norm_mode'                   noisy-readout + continuous -&gt; 'truncated_norm_mode'                   noisy-temperature + criteria -&gt; 'lognorm_mode'                   noisy-temperature + continuous -&gt; 'truncated_norm_mode'         )     Metacognitive noise distribution.     Possible values:         noisy_report: 'beta_mean_std', 'beta_mode_std', 'beta_mode',                       'truncated_norm_mode_std', 'truncated_norm_mode' (default),                       'truncated_gumbel_mode_std', 'truncated_gumbel_mode',                       'truncated_lognorm_mode_std', 'truncated_lognorm', 'truncated_lognorm_mode',                       'truncated_lognorm_mean'         noisy_readout: 'lognorm_median_std', 'lognorm_mean', 'lognorm_mode', 'lognorm_mode_std', 'lognorm_mean_std',                        'gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv',                        'betaprime_mean_std',                        'truncated_norm_mode_std', 'truncated_norm_mode',                        'truncated_gumbel_mode_std', 'truncated_gumbel_mode'         noisy_temperature: same as noisy_readout</p> <p>*** Enable or disable specific parameters *** * Each setting can take the values 0, 1 or 2: *    0: Disable parameter. *    1: Enable parameter. *    2: Enable parameter and fit separate values for the negative and positive stimulus category         (works only for type 1 parameters!) enable_type1_param_noise : int (default: 1)     Fit separate type 1 noise parameters for both stimulus categories. enable_type1_param_noise_heteroscedastic : int (default: 0)     Fit an additional type 1 noise parameter for signal-dependent type 1 noise (the type of dependency is     defined via <code>type1_noise_signal_dependency</code>). enable_type1_param_nonlinear_encoding_gain : int (default: 0) enable_type1_param_nonlinear_encoding_transition : int (default: 0) enable_type1_param_thresh : int (default: 0)     Fit a type 1 threshold. enable_type1_param_bias : int (default: 1)     Fit a type 1 bias towards one of the stimulus categories. enable_type2_param_noise : int (default: 1)     Fit a metacognitive noise parameter enable_type2_param_evidence_bias_mult : int (default: 0)     Fit a multiplicative metacognitive bias loading on evidence. enable_type2_param_criteria : int (default: 0)     Fit confidence criteria.</p> <p>*** Additional options to specify the nature of type 2 fitting *** n_discrete_confidence_levels : int (default: 5)     Number of confidence criteria. Only applies in case of type2_fitting_type='criteria'.</p> <p>*** Define fitting characteristics of the parameters *** * The fitting of each parameter is characzerized as follows: *     1) An initial guess. *     2) Lower and upper bound. *     3) Grid range, i.e. list of values that are tested during the initial gridsearch search. * Sensible default values are provided for all parameters. To tweak those, one can either define an entire * ParameterSet, which is a container for a set of parameters, or each parameter individually. Note that the * parameters must be either defined as a Parameter instance or as List[Parameter] in case when separate values are * fitted for the positive and negative stimulus category/decision value). paramset_type1 : ParameterSet     Parameter set for the type 1 stage. paramset_type2 : ParameterSet     Parameter set for the type 2 stage. paramset : ParameterSet     Parameter set for both stages.</p> Union[Parameter, List[Parameter]]  (default: 1) <p>Parameter for type 1 noise.</p> <p>_type1_param_noise_heteroscedastic : Union[Parameter, List[Parameter]]  (default: 0)     Parameter for signal-dependent type 1 noise. _type1_param_nonlinear_encoding_gain : Union[Parameter, List[Parameter]]  (default: 0)     Gain parameter for nonlinear encoding (higher values -&gt; stronger nonlinearity). _type1_param_nonlinear_encoding_transition : Union[Parameter, List[Parameter]]  (default: 0)     Transition Parameter for nonlinear encoding (). type1_noise_signal_dependency: str (default: 'none')     Can be one of 'none', 'multiplicative', 'power', 'exponential' or 'logarithm'. _type1_param_thresh : Union[Parameter, List[Parameter]] (default: 0)     Parameter for the type 1 threshold. _type1_param_bias : Union[Parameter, List[Parameter]]  (default: 1)     Parameter for the type 1 bias. _type2_param_noise : Union[Parameter, List[Parameter]]  (default: 1)     Parameter for metacognitive noise. _type2_param_evidence_bias_mult : Union[Parameter, List[Parameter]]  (default: 0)     Parameter for a multiplicative metacognitive bias loading on evidence. type2_param_confidence_criteria : List[Parameter]  (default: 1)     List of parameter specifying the confidence criteria.</p> <p>*** Skip type 2 fitting *** skip_type2 : bool (default: False)     If True, ignore type 2 settings in the setup of the model configuration &amp; don't fit type 2 stage.</p> <p>*** Methodoligcal aspects of parameter fitting *** optim_type1_gridsearch : bool (default: False)     If True, perform initial (usually coarse) gridsearch search for type 1 fitting, based on the gridsearch defined     for a Parameter. optim_type1_fine_gridsearch : bool (default: False)     If True, perform an iteratively finer gridsearch search for each parameter (type 1). optim_type1_minimize_along_grid : bool (default: False)     If True, do sqlqp minimization for at each grid point (type 1). optim_type1_global_minimization : str (default: None)     Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with     a global minimizer (type 1). optim_type1_scipy_solvers : str or Tuple/List (default: 'trust-constr')     Set scipy.optimize.minimize gradient method (type 1)     If provided as Tuple/List, test different gradient methods and take the best optim_type2_gridsearch : bool (default: True)     If True, perform initial (usually coarse) gridsearch search for type 2 fitting, based on the gridsearch defined     for a Parameter. optim_type2_fine_gridsearch : bool (default: False)     If True, perform an iteratively finer gridsearch search for each parameter (type 2). optim_type2_minimize_along_grid : bool (default: False)     If True, do sqlqp minimization for at each grid point (type 2). optim_type2_global_minimization : str (default: None)     Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with     a global minimizer (type 2). optim_type2_scipy_solvers : str or Tuple/List (default: ('slsqp', 'Nelder-Mead'))     Set scipy.optimize.minimize gradient method (type 2)     If provided as Tuple/List, test different gradient methods and take the best optim_type2_slsqp_epsilon : float or Tuple/List (default: None)     Set parameter epsilon parameter for the SLSQP optimization method (type 2).     If provided as Tuple/List, test different eps parameters and take the best optim_multiproc : bool (default: False)     If True, use all optim_multiproc_cores cores for parameter estimation. If False, use a single core. optim_multiproc_cores : int (default: -1)     If multicore processing es enabled via optim_multiproc, use optim_multiproc_cores cores for parameter estimation     (-1 for all cores minus 1).</p> <p>*** Preprocessing *** normalize_stimuli_by_max : bool (default: True)     If True, normalize provided stimuli by their maximum value.</p> <p>*** Parameters for the type 2 likelihood computation *** min_type1_likelihood : float     Minimum probability used during the type 1 likelihood computation min_type2_likelihood : float     Minimum probability used during the type 2 likelihood computation type2_binsize : float     Integration bin size for the computation of the likelihood around empirical confidence values y_decval_range_nsds : int     Number of standard deviations around the mean considered for type 1 uncertainty. y_decval_range_nbins : int     Number of discrete decision values bins that are considered to represent type 1 uncertainty. resolution_noisy_temperature : float     Quintile resolution for the marginalization of type 1 noise in case of type2_noise_type 'noisy_temperature'. experimental_min_uniform_type2_likelihood : bool     Instead of using a minimum probability during the likelihood computation, use a maximum cumulative     likelihood based on a 'guessing' model experimental_wrap_type2_integration_window : bool (default: False)     Ensure constant window size for likelihood integration at the bounds.     Only applies in case of type2_fitting_type='continuous' and experimental_disable_type2_binsize=False experimental_include_incongruent_y_decval : bool (default: False)     Include incongruent decision values (i.e., sign(actual choice) != sign(decision value)) for the likelihood     computation experimental_disable_type2_binsize : bool (default: None)     Do not use an integegration window for likelihood computation.     Only applies in case of type2_fitting_type='continuous'</p> <p>*** Other *** true_params : Dict     Pass true (known) parameter values. This can be useful for testing to compare the likelihood of true and     fitted parameters. The likelihood of true parameters is returned (and printed). initilialize_fitting_at_true_params : bool (default: False)     Option to initialize the parameter fitting procedure at the true parameters; this can be helpful for testing. silence_configuration_warnings : bool (default: False)     If True, ignore warnings about user-specified settings. print_configuration : bool (default: True)     If True, print the configuration at instatiation of the ReMeta class.</p> Source code in <code>remeta/configuration.py</code> <pre><code>@reset_dataclass_on_init\n@dataclass\nclass Configuration(ReprMixin):\n    \"\"\"\n    Configuration for the ReMeta toolbox\n\n    Parameters\n    ----------\n    *** Basic definition of the model ***\n    type2_fitting_type : str (default: 'criteria')\n        Whether confidence is fitted with discrete *criteria* or as a continuous variable.\n        Possible values: 'criteria', 'continuous'\n    type2_noise_type : str (default: 'noisy-report)\n        Whether the model considers noise at readout or report.\n        Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'\n    type2_noise_dist : str\n            (default: noisy-report + criteria -&gt; 'truncated_norm_mode'\n                      noisy-report + continuous -&gt; 'truncated_norm_mode'\n                      noisy-readout + criteria -&gt; 'truncated_norm_mode'\n                      noisy-readout + continuous -&gt; 'truncated_norm_mode'\n                      noisy-temperature + criteria -&gt; 'lognorm_mode'\n                      noisy-temperature + continuous -&gt; 'truncated_norm_mode'\n            )\n        Metacognitive noise distribution.\n        Possible values:\n            noisy_report: 'beta_mean_std', 'beta_mode_std', 'beta_mode',\n                          'truncated_norm_mode_std', 'truncated_norm_mode' (default),\n                          'truncated_gumbel_mode_std', 'truncated_gumbel_mode',\n                          'truncated_lognorm_mode_std', 'truncated_lognorm', 'truncated_lognorm_mode',\n                          'truncated_lognorm_mean'\n            noisy_readout: 'lognorm_median_std', 'lognorm_mean', 'lognorm_mode', 'lognorm_mode_std', 'lognorm_mean_std',\n                           'gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv',\n                           'betaprime_mean_std',\n                           'truncated_norm_mode_std', 'truncated_norm_mode',\n                           'truncated_gumbel_mode_std', 'truncated_gumbel_mode'\n            noisy_temperature: same as noisy_readout\n\n\n    *** Enable or disable specific parameters ***\n    * Each setting can take the values 0, 1 or 2:\n    *    0: Disable parameter.\n    *    1: Enable parameter.\n    *    2: Enable parameter and fit separate values for the negative and positive stimulus category\n            (works only for type 1 parameters!)\n    enable_type1_param_noise : int (default: 1)\n        Fit separate type 1 noise parameters for both stimulus categories.\n    enable_type1_param_noise_heteroscedastic : int (default: 0)\n        Fit an additional type 1 noise parameter for signal-dependent type 1 noise (the type of dependency is\n        defined via `type1_noise_signal_dependency`).\n    enable_type1_param_nonlinear_encoding_gain : int (default: 0)\n    enable_type1_param_nonlinear_encoding_transition : int (default: 0)\n    enable_type1_param_thresh : int (default: 0)\n        Fit a type 1 threshold.\n    enable_type1_param_bias : int (default: 1)\n        Fit a type 1 bias towards one of the stimulus categories.\n    enable_type2_param_noise : int (default: 1)\n        Fit a metacognitive noise parameter\n    enable_type2_param_evidence_bias_mult : int (default: 0)\n        Fit a multiplicative metacognitive bias loading on evidence.\n    enable_type2_param_criteria : int (default: 0)\n        Fit confidence criteria.\n\n    *** Additional options to specify the nature of type 2 fitting ***\n    n_discrete_confidence_levels : int (default: 5)\n        Number of confidence criteria. Only applies in case of type2_fitting_type='criteria'.\n\n    *** Define fitting characteristics of the parameters ***\n    * The fitting of each parameter is characzerized as follows:\n    *     1) An initial guess.\n    *     2) Lower and upper bound.\n    *     3) Grid range, i.e. list of values that are tested during the initial gridsearch search.\n    * Sensible default values are provided for all parameters. To tweak those, one can either define an entire\n    * ParameterSet, which is a container for a set of parameters, or each parameter individually. Note that the\n    * parameters must be either defined as a Parameter instance or as List[Parameter] in case when separate values are\n    * fitted for the positive and negative stimulus category/decision value).\n    paramset_type1 : ParameterSet\n        Parameter set for the type 1 stage.\n    paramset_type2 : ParameterSet\n        Parameter set for the type 2 stage.\n    paramset : ParameterSet\n        Parameter set for both stages.\n\n    _type1_param_noise : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for type 1 noise.\n    _type1_param_noise_heteroscedastic : Union[Parameter, List[Parameter]]  (default: 0)\n        Parameter for signal-dependent type 1 noise.\n    _type1_param_nonlinear_encoding_gain : Union[Parameter, List[Parameter]]  (default: 0)\n        Gain parameter for nonlinear encoding (higher values -&gt; stronger nonlinearity).\n    _type1_param_nonlinear_encoding_transition : Union[Parameter, List[Parameter]]  (default: 0)\n        Transition Parameter for nonlinear encoding ().\n    type1_noise_signal_dependency: str (default: 'none')\n        Can be one of 'none', 'multiplicative', 'power', 'exponential' or 'logarithm'.\n    _type1_param_thresh : Union[Parameter, List[Parameter]] (default: 0)\n        Parameter for the type 1 threshold.\n    _type1_param_bias : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for the type 1 bias.\n    _type2_param_noise : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for metacognitive noise.\n    _type2_param_evidence_bias_mult : Union[Parameter, List[Parameter]]  (default: 0)\n        Parameter for a multiplicative metacognitive bias loading on evidence.\n    type2_param_confidence_criteria : List[Parameter]  (default: 1)\n        List of parameter specifying the confidence criteria.\n\n    *** Skip type 2 fitting ***\n    skip_type2 : bool (default: False)\n        If True, ignore type 2 settings in the setup of the model configuration &amp; don't fit type 2 stage.\n\n    *** Methodoligcal aspects of parameter fitting ***\n    optim_type1_gridsearch : bool (default: False)\n        If True, perform initial (usually coarse) gridsearch search for type 1 fitting, based on the gridsearch defined\n        for a Parameter.\n    optim_type1_fine_gridsearch : bool (default: False)\n        If True, perform an iteratively finer gridsearch search for each parameter (type 1).\n    optim_type1_minimize_along_grid : bool (default: False)\n        If True, do sqlqp minimization for at each grid point (type 1).\n    optim_type1_global_minimization : str (default: None)\n        Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with\n        a global minimizer (type 1).\n    optim_type1_scipy_solvers : str or Tuple/List (default: 'trust-constr')\n        Set scipy.optimize.minimize gradient method (type 1)\n        If provided as Tuple/List, test different gradient methods and take the best\n    optim_type2_gridsearch : bool (default: True)\n        If True, perform initial (usually coarse) gridsearch search for type 2 fitting, based on the gridsearch defined\n        for a Parameter.\n    optim_type2_fine_gridsearch : bool (default: False)\n        If True, perform an iteratively finer gridsearch search for each parameter (type 2).\n    optim_type2_minimize_along_grid : bool (default: False)\n        If True, do sqlqp minimization for at each grid point (type 2).\n    optim_type2_global_minimization : str (default: None)\n        Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with\n        a global minimizer (type 2).\n    optim_type2_scipy_solvers : str or Tuple/List (default: ('slsqp', 'Nelder-Mead'))\n        Set scipy.optimize.minimize gradient method (type 2)\n        If provided as Tuple/List, test different gradient methods and take the best\n    optim_type2_slsqp_epsilon : float or Tuple/List (default: None)\n        Set parameter epsilon parameter for the SLSQP optimization method (type 2).\n        If provided as Tuple/List, test different eps parameters and take the best\n    optim_multiproc : bool (default: False)\n        If True, use all optim_multiproc_cores cores for parameter estimation. If False, use a single core.\n    optim_multiproc_cores : int (default: -1)\n        If multicore processing es enabled via optim_multiproc, use optim_multiproc_cores cores for parameter estimation\n        (-1 for all cores minus 1).\n\n    *** Preprocessing ***\n    normalize_stimuli_by_max : bool (default: True)\n        If True, normalize provided stimuli by their maximum value.\n\n    *** Parameters for the type 2 likelihood computation ***\n    min_type1_likelihood : float\n        Minimum probability used during the type 1 likelihood computation\n    min_type2_likelihood : float\n        Minimum probability used during the type 2 likelihood computation\n    type2_binsize : float\n        Integration bin size for the computation of the likelihood around empirical confidence values\n    y_decval_range_nsds : int\n        Number of standard deviations around the mean considered for type 1 uncertainty.\n    y_decval_range_nbins : int\n        Number of discrete decision values bins that are considered to represent type 1 uncertainty.\n    resolution_noisy_temperature : float\n        Quintile resolution for the marginalization of type 1 noise in case of type2_noise_type 'noisy_temperature'.\n    experimental_min_uniform_type2_likelihood : bool\n        Instead of using a minimum probability during the likelihood computation, use a maximum cumulative\n        likelihood based on a 'guessing' model\n    experimental_wrap_type2_integration_window : bool (default: False)\n        Ensure constant window size for likelihood integration at the bounds.\n        Only applies in case of type2_fitting_type='continuous' and experimental_disable_type2_binsize=False\n    experimental_include_incongruent_y_decval : bool (default: False)\n        Include incongruent decision values (i.e., sign(actual choice) != sign(decision value)) for the likelihood\n        computation\n    experimental_disable_type2_binsize : bool (default: None)\n        Do not use an integegration window for likelihood computation.\n        Only applies in case of type2_fitting_type='continuous'\n\n\n    *** Other ***\n    true_params : Dict\n        Pass true (known) parameter values. This can be useful for testing to compare the likelihood of true and\n        fitted parameters. The likelihood of true parameters is returned (and printed).\n    initilialize_fitting_at_true_params : bool (default: False)\n        Option to initialize the parameter fitting procedure at the true parameters; this can be helpful for testing.\n    silence_configuration_warnings : bool (default: False)\n        If True, ignore warnings about user-specified settings.\n    print_configuration : bool (default: True)\n        If True, print the configuration at instatiation of the ReMeta class.\n    \"\"\"\n\n    type2_fitting_type: str = 'criteria'\n    type2_noise_type: str = 'noisy_report'\n    type2_noise_dist: str = None\n        # noisy-report + criteria -&gt; 'truncated_norm_mode'\n        # noisy-report + continuous -&gt; 'truncated_norm_mode'\n        # noisy-readout + criteria -&gt; 'truncated_norm_mode'\n        # noisy-readout + continuous -&gt; 'truncated_norm_mode'\n        # noisy-temperature + criteria -&gt; 'lognorm_mode'\n        # noisy-temperature + continuous -&gt; 'truncated_norm_mode'\n\n    enable_type1_param_noise: int = 1\n    enable_type1_param_thresh: int = 0\n    enable_type1_param_bias: int = 1\n    enable_type2_param_noise: int = 1\n    enable_type2_param_evidence_bias_mult: int = 0\n    enable_type2_param_criteria: int = 1\n    # Experimental:\n    enable_type1_param_noise_heteroscedastic: int = 0\n    enable_type1_param_nonlinear_encoding_gain: int = 0\n    enable_type1_param_nonlinear_encoding_transition: int = 0\n\n    n_discrete_confidence_levels: int = 5\n\n    paramset_type1: ParameterSet = None\n    paramset_type2: ParameterSet = None\n    paramset_all: ParameterSet = None\n\n    type1_param_noise_heteroscedastic: Parameter = Parameter(guess=0, bounds=(0, 10), grid_range=np.linspace(0, 1, 5))\n    type1_param_nonlinear_encoding_gain: Parameter = Parameter(guess=0, bounds=(-8/9, 10), grid_range=np.linspace(-0.5, 1, 5))\n    type1_param_nonlinear_encoding_transition: Parameter = Parameter(guess=1, bounds=(0.01, 10), grid_range=np.linspace(0.01, 2, 5))\n    type1_param_noise: Parameter = Parameter(guess=0.5, bounds=(0.001, 100), grid_range=np.linspace(0.1, 1, 8))\n    type1_param_thresh: Parameter = Parameter(guess=0, bounds=(0, 1), grid_range=np.linspace(0, 0.2, 5))\n    type1_param_bias: Parameter = Parameter(guess=0, bounds=(-1, 1), grid_range=np.linspace(-0.2, 0.2, 8))\n    type2_param_noise: Parameter = Parameter(guess=0.1, bounds=(0.05, 2), grid_range=np.linspace(0.1, 1, 8))\n    type2_param_evidence_bias_mult: Parameter = Parameter(guess=1, bounds=(0.5, 2), grid_range=np.linspace(0.5, 2, 8))\n    type2_param_criteria: Parameter = Parameter(bounds=(1e-8, 1))\n    type2_param_criteria_guesses: str | List[float] = 'equidistant'\n    type2_param_criteria_grid_ranges: str | List[np.ndarray] = 'equidistant'\n\n    type1_noise_signal_dependency: str = 'none'\n\n    skip_type2 = False\n\n    optim_type1_gridsearch: bool = False\n    optim_type1_fine_gridsearch: bool = False\n    optim_type1_minimize_along_grid: bool = False\n    optim_type1_global_minimization: str = None\n    _optim_type1_scipy_solvers_default = 'trust-constr'\n    optim_type1_scipy_solvers: str | List[str] | Tuple[str, ...] = 'trust-constr'\n    optim_type2_gridsearch: bool = True\n    optim_type2_fine_gridsearch: bool = False\n    optim_type2_minimize_along_grid: bool = False\n    optim_type2_global_minimization: str = None\n    optim_type2_scipy_solvers: str | List[str] | Tuple[str, ...] = ('slsqp', 'Nelder-Mead')\n    optim_type2_slsqp_epsilon: float = None\n    optim_multiproc: bool = False\n    optim_multiproc_cores: int = -1\n    _optim_multiproc_cores_effective: int = None\n\n    normalize_stimuli_by_max: bool = True\n    confidence_bounds_error: float = 0\n\n    min_type2_likelihood: float = 1e-10\n    min_type1_likelihood: float = 1e-10\n    type2_binsize: float = 0.01\n    y_decval_range_nsds: int = 5\n    y_decval_range_nbins: int = 101\n    resolution_noisy_temperature: float = 0.001\n\n    experimental_min_uniform_type2_likelihood: bool = False\n    experimental_wrap_type2_integration_window: bool = False\n    experimental_include_incongruent_y_decval: bool = False\n    experimental_disable_type2_binsize: bool = False\n\n    true_params: Dict = None\n    initilialize_fitting_at_true_params: bool = False\n    silence_configuration_warnings: bool = False\n    print_configuration: bool = False\n\n    type2_param_noise_min: float = 0.001\n\n    # setup_called = False\n\n    _type1_param_noise: Parameter | List[Parameter] = None\n    _type1_param_noise_heteroscedastic: Parameter | List[Parameter] = None\n    _type1_param_nonlinear_encoding_transition: Parameter | List[Parameter] = None\n    _type1_param_nonlinear_encoding_gain: Parameter | List[Parameter] = None\n    _type1_param_thresh: Parameter | List[Parameter] = None\n    _type1_param_bias: Parameter | List[Parameter] = None\n    _type2_param_noise: Parameter = None\n    _type2_param_evidence_bias_mult: Parameter = None\n    _type2_param_criteria: List[Parameter] = None\n\n    def setup(self, generative_mode=False):\n\n        if find_spec('multiprocessing_on_dill') is None:\n            warnings.warn(f'Multiprocessing on dill is not installed. Setting grid_multiproc is changed to False.')\n            self.optim_multiproc = False\n\n        if self.optim_multiproc:\n            from multiprocessing import cpu_count\n            self._optim_multiproc_cores_effective = max(1, (cpu_count() or 1) - 1) if self.optim_multiproc_cores == -1 \\\n                else self.optim_multiproc_cores\n\n        self._prepare_params_type1()\n        if self.skip_type2:\n            if self.optim_type2_slsqp_epsilon is None:\n                self.optim_type2_slsqp_epsilon = 1e-5\n        else:\n\n            if self.enable_type1_param_thresh and \\\n                (self.optim_type1_scipy_solvers == self._optim_type1_scipy_solvers_default):\n                self.optim_type1_scipy_solvers = ('trust-constr', 'Powell')\n\n\n            if self.type2_noise_dist is None:\n                if generative_mode:\n                    raise ValueError('In generative mode, you need to explicitly specify a type 2 noise distribution.')\n                else:\n                    if self.type2_noise_type == 'noisy_report':\n                        if (self.type2_fitting_type == 'criteria'):\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                    elif (self.type2_noise_type == 'noisy_readout'):\n                        if self.type2_fitting_type == 'criteria':\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                    elif self.type2_noise_type == 'noisy_temperature':\n                        if self.type2_fitting_type == 'criteria':\n                            self.type2_noise_dist = 'lognorm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n\n            self._prepare_params_type2()\n            if self.optim_type2_slsqp_epsilon is None:\n                self.optim_type2_slsqp_epsilon = 1e-5\n\n            if self.type2_binsize is None:\n                self.type2_binsize = 0.01\n\n        self._prepare_params_all()\n\n        self._check_compatibility(generative_mode=generative_mode)\n\n        if self.print_configuration:\n            self.print()\n        # self.setup_called = True\n\n    def _check_compatibility(self, generative_mode=False):\n\n        if not self.silence_configuration_warnings:\n\n            if not self.skip_type2:\n                if not self.enable_type2_param_noise:\n                    warnings.warn(f'Setting enable_type2_param_noise=False was provided -&gt; type2_param_noise is set to its default value '\n                                  f'({self._type2_param_noise_default}). You may change this value via the configuration.')\n\n                if (self.type2_noise_type == 'noisy_temperature') and self.type2_param_noise.default_changed and \\\n                    (self.type2_param_noise.bounds[0] &lt; 1e-5):\n                    warnings.warn('You manually changed the lower bound of the type 2 noise parameter for a '\n                                  'noisy-temperature model to a very low value (&lt;1e-5). Be warned that this may result '\n                                  'in numerical instabilities that severely distort the likelihood computation.')\n\n                if not generative_mode:\n                    # If the configuration instance is used for generating data, we should not complain\n                    # about fitting issues.\n\n                    if self.enable_type2_param_criteria and self.enable_type2_param_evidence_bias_mult:\n                        warnings.warn(\n                            'enable_type2_param_criteria=True in combination with enable_type2_param_evidence_bias_mult=True\\n'\n                            'can lead to biased parameter inferences. Use with caution.')\n\n                    if (self.type2_fitting_type == 'continuous') and self.enable_type2_param_criteria:\n                        raise ValueError(\"Setting type2_fitting_type='continuous' conflicts with enable_type2_param_criteria=1.'\")\n\n                    if (self.type2_fitting_type == 'criteria') and not self.enable_type2_param_criteria:\n                        warnings.warn(\"You selected type2_fitting_type='criteria', but did not enable type 2 criteria\\n\"\n                                      \"(enable_type2_param_criteria=0). This works, but be mindful that the model\\n\"\n                                      \"will assume equispaced ideal Bayesian observer criteria (respecting \\n\"\n                                      \"the setting n_discrete_confidence_levels).\")\n\n    def _prepare_params_type1(self):\n        # if self.paramset_type1 is None:\n\n            param_names_type1 = []\n            params_type1 = ('noise', 'noise_heteroscedastic', 'nonlinear_encoding_gain', 'nonlinear_encoding_transition', 'thresh', 'bias')\n            for param in params_type1:\n                if getattr(self, f'enable_type1_param_{param}'):\n                    param_names_type1 += [f'type1_{param}']\n                    if getattr(self, f'_type1_param_{param}') is None:\n                        param_definition = getattr(self, f'type1_param_{param}')\n                        if getattr(self, f'enable_type1_param_{param}') == 2:\n                            setattr(self, f'_type1_param_{param}', [param_definition, param_definition])\n                        else:\n                            setattr(self, f'_type1_param_{param}', param_definition)\n                        if self.true_params is not None and self.initilialize_fitting_at_true_params and f'type1_{param}' in self.true_params:\n                            getattr(self, f'_type1_param_{param}').guess = self.true_params[f'type1_{param}']\n\n            parameters = {k: getattr(self, f\"_type1_param_{k.split('type1_')[1]}\") for k in param_names_type1}\n            self.paramset_type1 = ParameterSet(parameters, param_names_type1)\n\n    def _prepare_params_type2(self):\n\n        # if self.paramset_type2 is None:\n\n            if self.enable_type2_param_noise and self._type2_param_noise is None and not self.type2_param_noise.default_changed:\n\n                lb = 0.05\n                self.type2_param_noise.bounds = dict(\n                    noisy_report = dict(\n                        beta_mean_std=(lb, 0.5),\n                        beta_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_norm_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_gumbel_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_lognorm_mode_std=(lb, 1 / np.sqrt(12)),\n                        beta_mode=(lb, 1),\n                        truncated_norm_mode=(lb, 1),\n                        truncated_gumbel_mode=(lb, 1),\n                        truncated_lognorm_mode=(lb, 4),\n                        truncated_lognorm_mean=(lb, 4),\n                        truncated_lognorm=(lb, 4)\n                    ),\n                    noisy_readout = dict(\n                        lognorm_mean=(lb, 1),\n                        lognorm_mode=(lb, 1),\n                        gamma_mean_std=(lb, 1),\n                        lognorm_mean_std=(lb, 2),\n                        lognorm_mode_std=(lb, 2),\n                        lognorm_median_std=(lb, 2),\n                        gamma_mean_cv=(lb, 2),\n                        gamma_mean=(lb, 2),\n                        gamma_mode_std=(lb, 2),\n                        gamma_mode=(lb, 2),\n                        betaprime_mean_std=(lb, 2),\n                        truncated_norm_mode_std=(lb, 2),\n                        truncated_norm_mode=(lb, 2),\n                        truncated_gumbel_mode_std=(lb, 2),\n                        truncated_gumbel_mode=(lb, 2)\n                    ),\n                    noisy_temperature = dict(\n                        lognorm_mean=(lb, 1),\n                        gamma_mean_std=(lb, 1),\n                        lognorm_mean_std=(lb, 2),\n                        lognorm_median_std=(lb, 2),\n                        gamma_mean_cv=(lb, 2),\n                        gamma_mean=(lb, 2),\n                        gamma_mode_std=(lb, 2),\n                        gamma_mode=(lb, 2),\n                        betaprime_mean_std=(lb, 2),\n                        truncated_norm_mode_std=(lb, 2),\n                        truncated_norm_mode=(lb, 2),\n                        truncated_gumbel_mode_std=(lb, 2),\n                        truncated_gumbel_mode=(lb, 2),\n                        lognorm_mode=(lb, 4),\n                        lognorm_mode_std=(lb, 10),\n                    )\n                )[self.type2_noise_type][self.type2_noise_dist]\n                self.type2_param_noise.grid_range = np.exp(np.linspace(np.log(self.type2_param_noise.bounds[0]),\n                                                                       np.log(self.type2_param_noise.bounds[1]), 10)[1:-1])\n\n            param_names_type2 = []\n            params_type2 = ('noise', 'evidence_bias_mult')\n            for param in params_type2:\n                if getattr(self, f'enable_type2_param_{param}'):\n                    param_names_type2 += [f'type2_{param}']\n                    if getattr(self, f'_type2_param_{param}') is None:\n                        param_definition = getattr(self, f'type2_param_{param}')\n                        setattr(self, f'_type2_param_{param}', param_definition.copy())\n                        if self.true_params is not None and self.initilialize_fitting_at_true_params and f'type2_{param}' in self.true_params:\n                            getattr(self, f'_type2_param_{param}').guess = self.true_params[f'type2_{param}']\n\n\n            if self.enable_type2_param_criteria:\n                param_names_type2 += [f'type2_criteria']\n                initialize_true = (self.initilialize_fitting_at_true_params and\n                                   self.true_params is not None and 'type2_criteria' in self.true_params)\n                setattr(self, f'_type2_param_criteria',\n                        [Parameter(\n                           guess=self.true_params['type2_criteria'][i] if initialize_true\n                                    else (1 / self.n_discrete_confidence_levels if self.type2_param_criteria_guesses == 'equidistant'\n                                          else self.type2_param_criteria_guesses[i]),\n                           bounds=self.type2_param_criteria.bounds,\n                           grid_range=np.linspace(0.05, 2 / self.n_discrete_confidence_levels, 4) if\n                                self.type2_param_criteria_grid_ranges == 'equidistant' else self.type2_param_criteria_grid_ranges[i]\n                        )\n                         for i in range(self.n_discrete_confidence_levels - 1)]\n                        )\n                if self.true_params is not None:\n                    if isinstance(self.true_params, dict):\n                        # if 'type2_criteria' not in self.true_params:\n                        #     raise ValueError('type2_criteria are missing from cfg.true_params')\n                        if 'type2_criteria' in self.true_params:\n                            self.true_params.update(\n                                type2_criteria_absolute=[np.sum(self.true_params['type2_criteria'][:i+1]) for i in range(len(self.true_params['type2_criteria']))],\n                                type2_criteria_bias=np.mean(self.true_params['type2_criteria'])*(len(self.true_params['type2_criteria'])+1)-1\n                            )\n                    elif isinstance(self.true_params, list):\n                        for s in range(len(self.true_params)):\n                            # if 'type2_criteria' not in self.true_params[s]:\n                            #     raise ValueError(f'type2_criteria are missing from cfg.true_params (subject {s})')\n                            if 'type2_criteria' in self.true_params[s]:\n                                self.true_params[s].update(\n                                    type2_criteria_absolute=[np.sum(self.true_params[s]['type2_criteria'][:i+1]) for i in range(len(self.true_params[s]['type2_criteria']))],\n                                    type2_criteria_bias=np.mean(self.true_params[s]['type2_criteria'])*(len(self.true_params[s]['type2_criteria'])+1)-1\n                                )\n\n            parameters = {k: getattr(self, f\"_type2_param_{k.split('type2_')[1]}\") for k in param_names_type2}\n            self.paramset_type2 = ParameterSet(parameters, param_names_type2)\n\n\n            self.check_type2_constraints()\n\n\n    def _prepare_params_all(self):\n\n        if self.skip_type2:\n            self.paramset = self.paramset_type1\n        else:\n            parameters_all = {**self.paramset_type1.parameters, **self.paramset_type2.parameters}\n            param_names_all = self.paramset_type1.param_names + self.paramset_type2.param_names\n            self.paramset = ParameterSet(parameters_all, param_names_all)\n            # for k, attr in self.paramset_type2.__dict__.items():\n            #     attr_old = getattr(self.paramset, k)\n            #     if isinstance(attr, list):\n            #         attr_new = attr_old + attr\n            #     elif isinstance(attr, dict):\n            #         attr_new = {**attr_old, **attr}\n            #     elif isinstance(attr, np.ndarray):\n            #         if attr.ndim == 1:\n            #             attr_new = np.hstack((attr_old, attr))\n            #         else:\n            #             attr_new = np.vstack((attr_old, attr))\n            #     elif isinstance(attr, int):\n            #         attr_new = attr_old + attr\n            #     elif attr is None:\n            #         if attr_old is None:\n            #             attr_new = None\n            #         else:\n            #             raise ValueError(f'Type 2 attribute is None, but type 1 attribute is not.')\n            #     else:\n            #         raise ValueError(f'Unexpected type {type(attr)}')\n            #     setattr(self.paramset, k, attr_new)\n\n\n\n\n    def print(self):\n        # print('***********************')\n        print(f'{self.__class__.__name__}')\n        for k, v in self.__dict__.items():\n            # if not self.skip_type2 or ('type2' not in k):\n            print('\\n'.join([f'\\t{k}: {v}']))\n        # print('***********************')\n\n    def __repr__(self):\n        txt = f'{self.__class__.__name__}\\n'\n        txt += '\\n'.join([f'\\t{k}: {v}' for k, v in self.__dict__.items()])\n        return txt\n\n    def check_type2_constraints(self):\n        pass\n</code></pre>"},{"location":"gendata/#remeta.gendata.Simulation","title":"Simulation","text":"Source code in <code>remeta/gendata.py</code> <pre><code>class Simulation:\n    def __init__(self, nsubjects=None, nsamples=None, params=None, params_extra=None, cfg=None,\n                 x_stim=None, x_stim_category=None, d_dec=None, y_decval_latent=None, y_decval=None,\n                 z1_type1_evidence_latent=None, z1_type1_evidence_base=None, z1_type1_evidence=None,\n                 c_conf_latent=None, c_conf_base=None, c_conf=None,\n                 likelihood_dist=None, type1_stats=None, type2_stats=None):\n        self.nsubjects = nsubjects\n        self.nsamples = nsamples\n        self.params = params\n        self.params_type1 = {k: v for k, v in self.params.items() if k.startswith('type1_')}\n        self.params_type2 = {k: v for k, v in self.params.items() if k.startswith('type2_')}\n        self.params_extra = params_extra\n        self.cfg = cfg\n        self.stimuli = x_stim\n        self.stimuli_category = x_stim_category\n        self.choices = d_dec\n        self.accuracy = x_stim_category == d_dec\n        self.y_decval_latent = y_decval_latent\n        self.y_decval = y_decval\n        self.z1_type1_evidence_latent = z1_type1_evidence_latent\n        self.z1_type1_evidence_base = z1_type1_evidence_base\n        self.z1_type1_evidence = z1_type1_evidence\n        self.confidence_latent = c_conf_latent\n        self.confidence_base = c_conf_base\n        self.confidence = c_conf\n        self.likelihood_dist = likelihood_dist\n        self.type1_stats = type1_stats\n        self.type2_stats = type2_stats\n\n    def squeeze(self):\n        # for var in ('x_stim', 'x_stim_category', 'd_dec', 'accuracy'):\n        #     if getattr(self, var) is not None:\n        #         setattr(self, var, getattr(self, var).squeeze())\n        for name, value in self.__dict__.items():\n            if isinstance(value, np.ndarray):\n                setattr(self, name, value.squeeze())\n        return self\n</code></pre>"},{"location":"gendata/#remeta.gendata._check_param","title":"_check_param","text":"<pre><code>_check_param(x)\n</code></pre> Source code in <code>remeta/util.py</code> <pre><code>def _check_param(x):\n    if hasattr(x, '__len__'):\n        if len(x) == 2:\n            return x\n        elif len(x) == 1:\n            return [x[0], x[0]]\n        else:\n            print(f'Something went wrong, parameter array has length {len(x)}')\n    else:\n        return [x, x]\n</code></pre>"},{"location":"gendata/#remeta.gendata.check_criteria_sum","title":"check_criteria_sum","text":"<pre><code>check_criteria_sum(criteria)\n</code></pre> <p>Ensure that criteria sum up to at most 1</p>"},{"location":"gendata/#remeta.gendata.check_criteria_sum--parameters","title":"Parameters","text":"<p>criteria : array-like of dtype float     Confidence criteria (typically provided as a list)</p>"},{"location":"gendata/#remeta.gendata.check_criteria_sum--returns","title":"Returns","text":"<p>criteria : array-like of dtype float     Transformed confidence criteria</p> Source code in <code>remeta/transform.py</code> <pre><code>def check_criteria_sum(criteria):\n    \"\"\"\n    Ensure that criteria sum up to at most 1\n\n    Parameters\n    ----------\n    criteria : array-like of dtype float\n        Confidence criteria (typically provided as a list)\n\n    Returns\n    ----------\n    criteria : array-like of dtype float\n        Transformed confidence criteria\n    \"\"\"\n    ind1 = np.where(np.cumsum(criteria) &gt; 1)[0]\n    if len(ind1):\n        for i in ind1[1:]:\n            criteria[i] = 0\n        criteria[ind1[0]] = 1 - np.sum(criteria[:ind1[0]]) + 1e-8\n        # Note: we add 1e-8 to avoid edge cases due to floating point precision\n    return criteria\n</code></pre>"},{"location":"gendata/#remeta.gendata.compute_nonlinear_encoding","title":"compute_nonlinear_encoding","text":"<pre><code>compute_nonlinear_encoding(x_stim, type1_nonlinear_encoding_gain=None, type1_nonlinear_encoding_transition=None)\n</code></pre> Source code in <code>remeta/transform.py</code> <pre><code>def compute_nonlinear_encoding(x_stim, type1_nonlinear_encoding_gain=None, type1_nonlinear_encoding_transition=None):\n\n    gain = _check_param(0 if type1_nonlinear_encoding_gain is None else type1_nonlinear_encoding_gain)\n    transition = _check_param(1 if type1_nonlinear_encoding_transition is None else type1_nonlinear_encoding_transition)\n\n    neg, pos = x_stim &lt; 0, x_stim &gt;= 0\n    x_stim_nonlinear = np.empty_like(x_stim)\n    x_stim_nonlinear[neg] = x_stim[neg] * (1 + gain[0] * (x_stim[neg] / transition[0])**2 /\n                                           (1 + (x_stim[neg] / transition[0])**2))\n    x_stim_nonlinear[pos] = x_stim[pos] * (1 + gain[1] * (x_stim[pos] / transition[1])**2 /\n                                           (1 + (x_stim[pos] / transition[1])**2))\n\n    return x_stim_nonlinear\n</code></pre>"},{"location":"gendata/#remeta.gendata.compute_signal_dependent_type1_noise","title":"compute_signal_dependent_type1_noise","text":"<pre><code>compute_signal_dependent_type1_noise(x_stim, type1_noise=None, type1_thresh=None, type1_noise_heteroscedastic=None, type1_noise_signal_dependency='none', **kwargs)\n</code></pre> <p>Compute signal-dependent type 1 noise.</p>"},{"location":"gendata/#remeta.gendata.compute_signal_dependent_type1_noise--parameters","title":"Parameters","text":"<p>x_stim : array-like     Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value     codes the intensity. Must be normalized to [-1; 1]. type1_noise : float or array-like     Type 1 noise parameter. type1_thresh: float or array-like     Type 1 threshold. type1_noise_heteroscedastic : float or array-like     Signal-dependent type 1 noise parameter. type1_noise_signal_dependency : str     Define the signal dependency of type 1 noise. One of 'none', 'multiplicative', 'power', 'exponential', 'logarithm'. kwargs : dict     Convenience parameter to avoid an error if irrelevant parameters are passed.</p>"},{"location":"gendata/#remeta.gendata.compute_signal_dependent_type1_noise--returns","title":"Returns","text":"<p>type1_noise_heteroscedastic : array-like     Signal-dependent (heteroscedastic) type 1 noise of shape stimuli.shape.</p> Source code in <code>remeta/transform.py</code> <pre><code>def compute_signal_dependent_type1_noise(x_stim, type1_noise=None, type1_thresh=None, type1_noise_heteroscedastic=None,\n                                         type1_noise_signal_dependency='none', **kwargs):  # noqa\n    \"\"\"\n    Compute signal-dependent type 1 noise.\n\n    Parameters\n    ----------\n    x_stim : array-like\n        Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value\n        codes the intensity. Must be normalized to [-1; 1].\n    type1_noise : float or array-like\n        Type 1 noise parameter.\n    type1_thresh: float or array-like\n        Type 1 threshold.\n    type1_noise_heteroscedastic : float or array-like\n        Signal-dependent type 1 noise parameter.\n    type1_noise_signal_dependency : str\n        Define the signal dependency of type 1 noise. One of 'none', 'multiplicative', 'power', 'exponential', 'logarithm'.\n    kwargs : dict\n        Convenience parameter to avoid an error if irrelevant parameters are passed.\n\n    Returns\n    ----------\n    type1_noise_heteroscedastic : array-like\n        Signal-dependent (heteroscedastic) type 1 noise of shape stimuli.shape.\n    \"\"\"\n\n    type1_noise_ = _check_param(type1_noise)\n    type1_heteroscedastic_ = _check_param(type1_noise_heteroscedastic)\n    type1_thresh_ = (0, 0) if type1_thresh is None else _check_param(type1_thresh)\n    type1_noise_heteroscedastic = np.ones(x_stim.shape)\n    neg, pos = x_stim &lt; 0, x_stim &gt;= 0\n    if type1_noise_signal_dependency == 'none':\n        type1_noise_heteroscedastic[neg] *= type1_noise_[0]\n        type1_noise_heteroscedastic[pos] *= type1_noise_[1]\n    elif type1_noise_signal_dependency == 'multiplicative':\n        type1_noise_heteroscedastic[neg] = np.sqrt(type1_noise_[0] ** 2 +\n                                                         ((np.abs(x_stim[neg]) - type1_thresh_[0]) * type1_heteroscedastic_[0]) ** 2)  # noqa\n        type1_noise_heteroscedastic[pos] = np.sqrt(type1_noise_[1] ** 2 +\n                                                         ((np.abs(x_stim[pos]) - type1_thresh_[1]) * type1_heteroscedastic_[1]) ** 2)  # noqa\n    elif type1_noise_signal_dependency == 'power':\n        type1_noise_heteroscedastic[neg] = np.sqrt(type1_noise_[0] ** 2 +\n                                                         (np.abs(x_stim[neg]) - type1_thresh_[0]) ** (2 * type1_heteroscedastic_[0]))  # noqa\n        type1_noise_heteroscedastic[pos] = np.sqrt(type1_noise_[1] ** 2 +\n                                                         (np.abs(x_stim[pos]) - type1_thresh_[1]) ** (2 * type1_heteroscedastic_[1]))  # noqa\n    elif type1_noise_signal_dependency == 'exponential':\n        type1_noise_heteroscedastic[neg] = np.sqrt(type1_noise_[0] ** 2 +\n                                                         (np.exp(type1_heteroscedastic_[0] * (np.abs(x_stim[neg]) - type1_thresh_[0])) - 1) ** 2)  # noqa\n        type1_noise_heteroscedastic[pos] = np.sqrt(type1_noise_[1] ** 2 +\n                                                         (np.exp(type1_heteroscedastic_[1] * (np.abs(x_stim[pos]) - type1_thresh_[1])) - 1) ** 2)  # noqa\n    elif type1_noise_signal_dependency == 'logarithm':\n        type1_noise_heteroscedastic[neg] = np.sqrt(type1_noise_[0] ** 2 +\n                                                         np.log(type1_heteroscedastic_[0] * (np.abs(x_stim[neg]) - type1_thresh_[0]) + 1) ** 2)  # noqa\n        type1_noise_heteroscedastic[pos] = np.sqrt(type1_noise_[1] ** 2 +\n                                                         np.log(type1_heteroscedastic_[1] * (np.abs(x_stim[pos]) - type1_thresh_[1]) + 1) ** 2)  # noqa\n    else:\n        raise ValueError(f'{type1_noise_signal_dependency} is not a valid function for type1_noise_signal_dependency')\n\n    return type1_noise_heteroscedastic\n</code></pre>"},{"location":"gendata/#remeta.gendata.discretize_confidence_with_bounds","title":"discretize_confidence_with_bounds","text":"<pre><code>discretize_confidence_with_bounds(x, bounds)\n</code></pre> Source code in <code>remeta/util.py</code> <pre><code>def discretize_confidence_with_bounds(x, bounds):\n    confidence = np.full(x.shape, np.nan)\n    bounds = np.hstack((bounds, np.inf))\n    for i, b in enumerate(bounds[:-1]):\n        confidence[(bounds[i] &lt;= x) &amp; (x &lt; bounds[i + 1])] = i + 1\n    return confidence\n</code></pre>"},{"location":"gendata/#remeta.gendata.generate_stimuli","title":"generate_stimuli","text":"<pre><code>generate_stimuli(nsubjects, nsamples, stepsize=0.02, warn_in_case_of_nondivisible_stepsize=False)\n</code></pre> Source code in <code>remeta/gendata.py</code> <pre><code>def generate_stimuli(nsubjects, nsamples, stepsize=0.02, warn_in_case_of_nondivisible_stepsize=False):\n    levels = np.hstack((-np.arange(stepsize, 1.01, stepsize)[::-1], np.arange(stepsize, 1.01, stepsize)))\n    if warn_in_case_of_nondivisible_stepsize and ((nsamples % (2/stepsize)) != 0):\n        warnings.warn(f'At the chosen stepsize of {stepsize} there are {2/stepsize} stimulus levels,'\n                      f'which is not a divisor of the chosen sample size {nsamples}', UserWarning)\n    x_stim = np.array([np.random.permutation(np.tile(levels, int(np.ceil(nsamples / len(levels)))))[:nsamples] for _ in range(nsubjects)])\n    return x_stim\n</code></pre>"},{"location":"gendata/#remeta.gendata.get_type2_dist","title":"get_type2_dist","text":"<pre><code>get_type2_dist(type2_dist, type2_center, type2_noise, type2_noise_type='noisy_report')\n</code></pre> <p>Helper function to select appropriately parameterized type 2 noise distributions.</p>"},{"location":"gendata/#remeta.gendata.get_type2_dist--parameters","title":"Parameters:","text":"<p>type2_dist : str     Name of the type 2 noise distribution. Check TYPE2_NOISE_DISTS for possible values. type2_noise : float or array-like of dtype float     \"True\" value of a noisy type 2 variable. Corresponds to type 1 noise (noisy-temperature), metacognitve     evidence (noisy-readout) or confidence (noisy-report).     For mean- and mode preserving distributions, the center corresponds to the mean (suffix _mean) and mode (suffix     _mode), respectively. type2_noise : float or array-like of dtype float with mode.shape     Spread parameter that represents metacognitve noise. For standard-deviation-preserving distributions, this     parameter corresponds to the standard deviation when sampling data from the distribution (suffix _std). type2_noise_type : str (default='noisy_report')     Metacognitive noise type. Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'.</p>"},{"location":"gendata/#remeta.gendata.get_type2_dist--returns","title":"Returns:","text":"<p>scipy.stats continuous distribution instance</p> Source code in <code>remeta/type2_dist.py</code> <pre><code>def get_type2_dist(type2_dist, type2_center, type2_noise, type2_noise_type='noisy_report'):\n    \"\"\"\n    Helper function to select appropriately parameterized type 2 noise distributions.\n\n    Parameters:\n    -----------\n    type2_dist : str\n        Name of the type 2 noise distribution. Check TYPE2_NOISE_DISTS for possible values.\n    type2_noise : float or array-like of dtype float\n        \"True\" value of a noisy type 2 variable. Corresponds to type 1 noise (noisy-temperature), metacognitve\n        evidence (noisy-readout) or confidence (noisy-report).\n        For mean- and mode preserving distributions, the center corresponds to the mean (suffix _mean) and mode (suffix\n        _mode), respectively.\n    type2_noise : float or array-like of dtype float with mode.shape\n        Spread parameter that represents metacognitve noise. For standard-deviation-preserving distributions, this\n        parameter corresponds to the standard deviation when sampling data from the distribution (suffix _std).\n    type2_noise_type : str (default='noisy_report')\n        Metacognitive noise type. Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'.\n\n    Returns:\n    --------\n    scipy.stats continuous distribution instance\n    \"\"\"\n\n    if type2_dist not in TYPE2_NOISE_DISTS:\n        raise ValueError(f\"Unkonwn distribution '{type2_dist}'.\")\n    elif (type2_noise_type == 'noisy_report') and type2_dist in TYPE2_NOISE_DISTS_READOUT_TEMPERATURE_ONLY:\n        raise ValueError(f\"Distribution '{type2_dist}' is only valid for noisy-readout or noisy-temperature models.\")\n    elif (type2_noise_type in ('noisy_readout', 'noisy_temperature')) and type2_dist in TYPE2_NOISE_DISTS_REPORT_ONLY:\n        raise ValueError(f\"Distribution '{type2_dist}' is only valid for noisy-report models.\")\n\n    if type2_noise &lt; 1e-10:\n        warnings.warn('Type 2 noise is smaller than 1e-10, which can lead to unstable numerical results. It is set'\n                      'to the hard minimum of 1e-10.')\n        type2_noise = 1e-10\n\n    if type2_dist == 'lognorm_median_std':\n        # type2_center = median, type2_noise = SD\n        s = np.maximum(1e-12, np.sqrt(np.log((1 + np.sqrt(1 + 4 * (type2_noise / type2_center) ** 2)) / 2)))\n        dist = lognorm(s=s, scale=type2_center)\n    elif type2_dist == 'lognorm_mean':\n        # type2_center = mean, type2_noise = SD in log space\n        dist = lognorm(s=type2_noise, scale=np.exp(-type2_noise ** 2 / 2) * type2_center)\n    elif type2_dist == 'lognorm_mode':\n        # type2_center = mode, type2_noise = SD in log space\n        if type2_noise &gt; 23:\n            # warnings.warn(f'Type 2 noise for a {type2_dist} distribution is too high. Capping at 23')\n            type2_noise = 23\n        dist = lognorm(s=type2_noise, scale=np.exp(type2_noise ** 2) * type2_center)\n    elif type2_dist == 'lognorm_mode_std':\n        # type2_center = mode, type2_noise = SD\n        shape, type2_noise = _params_lognorm_mode_std(np.maximum(1e-5, type2_center), type2_noise)\n        dist = lognorm(loc=0, scale=type2_noise, s=shape)\n    elif type2_dist == 'lognorm_mean_std':\n        # Corresponds to the CASSANDRE/LogN setup\n        # type2_center = mean, type2_noise = SD\n        type2_center = np.maximum(type2_center, 1e-12)\n        sigma2 = np.log1p((np.maximum(type2_noise, 0) / type2_center) ** 2)\n        scale = np.exp(np.log(type2_center) - sigma2 / 2)\n        dist = lognorm(loc=0, scale=scale, s=np.sqrt(sigma2))\n    elif type2_dist == 'beta_mean_std':\n        # Canonical \"precision\" parameterization of the beta distribution, where precision = 1 / noise**2, i.e.\n        # inverse variance.\n        # type2_center = mean, type2_noise = SD\n\n        type2_center = np.clip(type2_center, 1e-12, 1 - 1e-12)          # avoid a=0 or b=0\n        type2_noise = np.maximum(type2_noise, 1e-12)\n\n        phi = type2_center * (1 - type2_center) / (type2_noise ** 2) - 1\n        phi = np.maximum(phi, 1e-12)\n\n        a = type2_center * phi\n        b = (1 - type2_center) * phi\n\n        dist = beta(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'beta_mode_std':\n        # type2_center = mode, type2_noise = SD\n\n        type2_center = np.clip(type2_center, 1e-8, 1 - 1e-8)\n\n        var_target = np.maximum(type2_noise ** 2, 1e-8)\n        # Initial closed-form approximation\n        kappa = type2_center * (1 - type2_center) / var_target\n        # One-step variance correction\n        num = type2_center * (1 - type2_center) * kappa ** 2 + kappa + 1\n        den = (kappa + 2)**2 * (kappa + 3)\n        var_actual = num / den\n        kappa *= var_actual / var_target\n\n        a = type2_center * kappa + 1\n        b = (1 - type2_center) * kappa + 1\n        dist = beta(loc=0, a=a, b=b, scale=1)\n\n    elif type2_dist == 'beta_mode':\n        # type2_center = mode, type2_noise != SD\n        type2_center = np.maximum(1e-5, np.minimum(1 - 1e-5, type2_center))\n        a = 1 + (1 - type2_center) * type2_center ** 2 / type2_noise ** 2\n        b = (1 / type2_center - 1) * a - 1 / type2_center + 2\n        dist = beta(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'betaprime_mean_std':\n        # type2_center = mean, type2_noise = SD\n        b = 2 + (type2_center * (type2_center + 1)) / type2_noise ** 2\n        a = type2_center * (b - 1)\n        dist = betaprime(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'gamma_mode_std':\n        # type2_center = mode, type2_noise = SD\n        u = (type2_center + np.sqrt(type2_center * type2_center + 4 * type2_noise ** 2)) / (2 * type2_noise)\n        dist = gamma(loc=0, a=u**2, scale=type2_noise / u)\n    elif type2_dist == 'gamma_mean_std':\n        # type2_center = mean, type2_noise = SD\n        dist = gamma(loc=0, a=(type2_center / type2_noise) ** 2, scale=(type2_noise ** 2) / type2_center)\n    elif type2_dist == 'gamma_mean':\n        # type2_center = mean, type2_noise != SD\n        dist = gamma(loc=0, a=type2_center / type2_noise, scale=type2_noise)\n    elif type2_dist == 'gamma_mean_cv':\n        # Corresponds to the CASSANDRE setup, i.e. standard multiplicative noise\n        # type2_center = center, type2_noise = CV (relative noise)\n        a = 1 / (type2_noise ** 2)\n        dist = gamma(loc=0, a=a, scale=type2_center / a)\n    elif type2_dist == 'gamma_mode':\n        # type2_center = mode; type2_noise != SD\n        dist = gamma(loc=0, a=(type2_center / type2_noise) + 1, scale=type2_noise)\n    elif type2_dist.startswith('truncated_'):\n        if type2_noise_type == 'noisy_report':\n            if type2_dist == 'truncated_norm_mode_std':\n                # type2_center = mode, type2_noise = SD of the truncated distribution\n                sigma = _params_truncnorm_mode_std(low=0, high=1, x=type2_center, noise=type2_noise)\n                dist = truncnorm(a=-type2_center / sigma, b=(1 - type2_center) / sigma, loc=type2_center, scale=sigma)\n            elif type2_dist == 'truncated_norm_mode':\n                # type2_center = mode, type2_noise = SD of the *un*truncated distribution\n                dist = truncnorm(a=-type2_center / type2_noise, b=(1 - type2_center) / type2_noise, loc=type2_center, scale=type2_noise)\n            elif type2_dist == 'truncated_gumbel_mode_std':\n                # type2_center = mode, type2_noise = SD of the truncated distribution\n                dist = TruncatedGumbelModeSD(mode=type2_center, noise=type2_noise, upper=1, n_newton=3)\n            elif type2_dist == 'truncated_gumbel_mode':\n                # type2_center = mode, type2_noise = SD of the *un*truncated distribution\n                dist = TruncatedGumbelMode(mode=type2_center, noise=type2_noise * np.sqrt(6) / np.pi, upper=1)\n            elif type2_dist == 'truncated_lognorm_mode_std':\n                # type2_center = mode, type2_noise = SD\n                dist = TruncatedLognormalModeSD(mode=type2_center, noise=type2_noise, upper=1, n_newton=5)\n            elif type2_dist == 'truncated_lognorm':\n                # type2_center = median of the untruncated lognormal, type2_noise = SD in log space\n                dist = TruncatedLognormal(median_untrunc=type2_center, noise=type2_noise, upper=1)\n            elif type2_dist == 'truncated_lognorm_mode':\n                # type2_center = mode, type2_noise = SD in log space\n                if type2_noise &gt; 25:\n                    # warnings.warn(f'Type 2 noise for a {type2_dist} distribution is too high. Capping at 25')\n                    type2_noise = 25\n                dist = TruncatedLognormalMode(mode=type2_center, noise=type2_noise, b=1)\n            elif type2_dist == 'truncated_lognorm_mean':\n                # type2_center = mean, type2_noise = SD in log space\n                dist = TruncatedLognormalMean(mean=type2_center, noise=type2_noise, upper=1)\n        elif type2_noise_type in ('noisy_readout', 'noisy_temperature'):\n            if type2_dist == 'truncated_norm_mode_std':\n                # type2_center = mode, type2_noise = SD\n                sigma = _params_truncnorm_mode_std(low=0, high=np.inf, x=type2_center, noise=type2_noise)\n                dist = truncnorm(a=-type2_center / sigma, b=np.inf, loc=type2_center, scale=sigma)\n            elif type2_dist == 'truncated_norm_mode':\n                # type2_center = mode, type2_noise != SD\n                dist = truncnorm(a=-type2_center / type2_noise, b=np.inf, loc=type2_center, scale=type2_noise)\n            elif type2_dist == 'truncated_gumbel_mode_std':\n                # type2_center = mode, type2_noise = SD\n                dist = TruncatedGumbelModeSD(mode=type2_center, noise=type2_noise, upper=np.inf, n_newton=3)\n            elif type2_dist == 'truncated_gumbel_mode':\n                # type2_center = mode, type2_noise != SD\n                dist = TruncatedGumbelMode(mode=type2_center, noise=type2_noise * np.sqrt(6) / np.pi, upper=np.inf)\n\n        else:\n            raise ValueError(f\"'{type2_noise_type}' is an unknown type 2 noise type\")\n\n    return dist  # noqa\n</code></pre>"},{"location":"gendata/#remeta.gendata.print_dataset_characteristics","title":"print_dataset_characteristics","text":"<pre><code>print_dataset_characteristics(sim)\n</code></pre> Source code in <code>remeta/util.py</code> <pre><code>def print_dataset_characteristics(sim):\n    print('----------------------------------')\n    if sim.cfg.skip_type2:\n        print('..Generative parameters:')\n        for p, v in sim.params_type1.items():\n            print(f'{TAB}{p}: {np.array2string(np.array(v), precision=3)}')\n    else:\n        print('..Generative model:')\n        print(f'{TAB}Type 2 noise type: {sim.cfg.type2_noise_type}')\n        print(f'{TAB}Type 2 noise distribution: {sim.cfg.type2_noise_dist}')\n        print('..Generative parameters:')\n        for p, v in sim.params.items():\n            print(f'{TAB}{p}: {np.array2string(np.array(v), precision=3)}')\n        if sim.params_extra is not None:\n            if 'type2_criteria_absolute' in sim.params_extra:\n                print(f\"{TAB}Type 2 criteria (absolute): [{', '.join([f'{c:.5g}' for c in sim.params_extra['type2_criteria_absolute']])}]\")\n            if 'type2_criteria_bias' in sim.params_extra:\n                print(f\"{TAB}Criterion bias: {sim.params_extra['type2_criteria_bias']:.5g}\")\n    print('..Descriptive statistics:')\n    print(f'{TAB}No. subjects: {sim.nsubjects}')\n    print(f'{TAB}No. samples: {sim.nsamples}')\n    if sim.type1_stats is not None:\n        print(f\"{TAB}Performance: {100 * sim.type1_stats['accuracy']:.1f}% correct\")\n        print(f\"{TAB}Choice bias: {('-', '+')[int(sim.type1_stats['choice_bias'] &gt; 0.5)]}{100*np.abs(sim.type1_stats['choice_bias'] - 0.5):.1f}%\")\n    if not sim.cfg.skip_type2 and sim.type2_stats is not None:\n        print(f\"{TAB}Confidence: {sim.type2_stats['confidence']:.2f}\")\n        print(f\"{TAB}M-Ratio: {sim.type2_stats['mratio']:.2f}\")\n        print(f\"{TAB}AUROC2: {sim.type2_stats['auroc2']:.2f}\")\n    print('----------------------------------')\n</code></pre>"},{"location":"gendata/#remeta.gendata.simu_data","title":"simu_data","text":"<pre><code>simu_data(params, nsubjects=1, nsamples=1000, cfg=None, stimuli_external=None, verbosity=True, stimuli_stepsize=0.02, squeeze=False, warn_in_case_of_nondivisible_stepsize=False, compute_stats=True, **kwargs)\n</code></pre> Source code in <code>remeta/gendata.py</code> <pre><code>def simu_data(params, nsubjects=1, nsamples=1000, cfg=None, stimuli_external=None, verbosity=True, stimuli_stepsize=0.02,\n              squeeze=False, warn_in_case_of_nondivisible_stepsize=False,\n              compute_stats=True, **kwargs):\n    params = params.copy()  # this variable can be modifed, thus better to make a copy\n    if cfg is None:\n        # Set configuration attributes that match keyword arguments\n        cfg_kwargs = {k: v for k, v in kwargs.items() if k in Configuration.__dict__}\n        cfg = Configuration(**cfg_kwargs)\n        for setting in cfg.__dict__:\n            if setting.startswith('enable_'):\n                if setting.split('enable_')[1].replace('_param_', '_') not in params:\n                    setattr(cfg, setting, 0)\n    # if not cfg.setup_called:\n    cfg.setup(generative_mode=True)\n\n    if cfg.type2_noise_dist is None:\n        cfg.type2_noise_dist = dict(noisy_report='truncated_norm_mode', noisy_readout='truncated_norm_mode', noisy_temperature='lognorm_mode')[cfg.type2_noise_type]\n\n    # Make sure no unwanted parameters have been passed\n    for p in ('thresh', 'bias', 'noise_heteroscedastic', 'nonlinear_encoding_gain', 'nonlinear_encoding_transition'):\n        if not getattr(cfg, f'enable_type1_param_{p}'):\n            params.pop(f'type1_{p}', None)\n    for p in ('evidence_bias_mult', 'criteria'):\n        if not getattr(cfg, f'enable_type2_param_{p}'):\n            params.pop(f'type2_{p}', None)\n\n    if stimuli_external is None:\n        x_stim = generate_stimuli(nsubjects, nsamples, stepsize=stimuli_stepsize,\n                                  warn_in_case_of_nondivisible_stepsize=warn_in_case_of_nondivisible_stepsize)\n    else:\n        x_stim = stimuli_external / np.max(np.abs(stimuli_external))\n        if stimuli_external.shape != (nsubjects, nsamples):\n            x_stim = np.tile(x_stim, (nsubjects, 1))\n    x_stim_category = (np.sign(x_stim) &gt; 0).astype(int)\n    y_decval_latent, y_decval, d_dec = simu_type1_responses(x_stim, params, cfg)\n\n    if not cfg.skip_type2:\n\n        z1_type1_evidence_latent = np.abs(y_decval_latent)\n        z1_type1_evidence_base = np.abs(y_decval)\n\n        if cfg.type2_noise_type == 'noisy_readout':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=z1_type1_evidence_base, type2_noise=params['type2_noise'],\n                                  type2_noise_type=cfg.type2_noise_type)\n\n            z1_type1_evidence = np.maximum(0, dist.rvs((nsubjects, nsamples)))\n        elif cfg.type2_noise_type == 'noisy_temperature':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=params['type1_noise'] * np.ones_like(z1_type1_evidence_base),\n                                  type2_noise=params['type2_noise'], type2_noise_type=cfg.type2_noise_type)\n            type1_noise_estimated = dist.rvs()\n            z1_type1_evidence = z1_type1_evidence_base\n        elif cfg.type2_noise_type == 'noisy_report':\n            z1_type1_evidence = z1_type1_evidence_base\n        else:\n            raise ValueError('Unknown type 2 noise type')\n\n        c_conf_latent = type1_evidence_to_confidence(\n            z1_type1_evidence=z1_type1_evidence_latent, y_decval=y_decval,\n            x_stim=x_stim, type1_noise_signal_dependency=cfg.type1_noise_signal_dependency,\n            **({**params, **dict(type1_noise=type1_noise_estimated)} if cfg.type2_noise_type == 'noisy_temperature' else params)\n        )\n\n        c_conf_base = type1_evidence_to_confidence(\n            z1_type1_evidence=z1_type1_evidence, y_decval=y_decval,\n            x_stim=x_stim, type1_noise_signal_dependency=cfg.type1_noise_signal_dependency,\n            **({**params, **dict(type1_noise=type1_noise_estimated)} if cfg.type2_noise_type == 'noisy_temperature' else params)\n        )\n\n        if cfg.type2_noise_type == 'noisy_report':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=c_conf_base, type2_noise=params['type2_noise'],\n                                  type2_noise_type=cfg.type2_noise_type)\n            c_conf = np.maximum(0, np.minimum(1, dist.rvs((nsubjects, nsamples))))\n        else:\n            c_conf = c_conf_base\n\n        if cfg.enable_type2_param_criteria:\n            if 'type2_criteria' in params:\n                sum_criteria = np.sum(params['type2_criteria'])\n                if sum_criteria &gt; 1.001:\n                    old_criteria = params['type2_criteria']\n                    params['type2_criteria'] = check_criteria_sum(params['type2_criteria'])\n                    warnings.warn(\n                       '\\nThe first entry of the criterion list is a criterion, whereas the subsequent entries encode\\n'\n                       'the gap to the respective previous criterion. Hence, the sum of all entries in the criterion\\n'\n                       f\"list must be smaller than 1, but sum([{', '.join([f'{c:.3f}' for c in old_criteria])}]) = {sum_criteria:.3f}).\"\n                       f\"Changing criteria to [{', '.join([f'{c:.3f}' for c in params['type2_criteria']])}].\", UserWarning)\n                first_criterion_and_gaps = params['type2_criteria']\n                criteria = [v if i == 0 else np.sum(first_criterion_and_gaps[:i+1]) for i, v in enumerate(first_criterion_and_gaps)]\n            else:\n                first_criterion_and_gaps = np.ones(cfg.n_discrete_confidence_levels - 1) / cfg.n_discrete_confidence_levels\n                criteria = [v if i == 0 else np.sum(first_criterion_and_gaps[:i+1]) for i, v in enumerate(first_criterion_and_gaps)]\n                warnings.warn(\n                    '\\nType 2 criteria enabled, but type2_criteria have not been specified. Using default values\\n'\n                    f\"of a Bayesian confidence observer for {cfg.n_discrete_confidence_levels} discrete ratings: [{', '.join([f'{v:.3g}' for v in first_criterion_and_gaps])}].\\n\"\n                    'Note that the first entry of the criterion list is a criterion, whereas the subsequent\\n'\n                    f'entries encode the gap to the respective previous criterion.\\n'\n                    f\"The final criteria are: [{', '.join([f'{v:.3g}' for v in criteria])}]\", UserWarning)\n\n            c_conf = (np.digitize(c_conf, criteria) + 0.5) / cfg.n_discrete_confidence_levels\n            c_conf_base = (np.digitize(c_conf_base, criteria) + 0.5) / cfg.n_discrete_confidence_levels\n\n    if squeeze:\n        x_stim_category = x_stim_category.squeeze()\n        x_stim = x_stim.squeeze()\n        d_dec = d_dec.squeeze()\n        y_decval_latent = y_decval_latent.squeeze()\n        y_decval = y_decval.squeeze()\n        if not cfg.skip_type2:\n            z1_type1_evidence_base = z1_type1_evidence_base.squeeze()  # noqa\n            z1_type1_evidence = z1_type1_evidence.squeeze()  # noqa\n            c_conf_base = c_conf_base.squeeze()  # noqa\n            c_conf = c_conf.squeeze()  # noqa\n\n    simargs = dict(\n        nsubjects=nsubjects, nsamples=nsamples, params=params, cfg=cfg,\n        x_stim_category=x_stim_category, x_stim=x_stim, d_dec=d_dec,\n        y_decval=y_decval, y_decval_latent=y_decval_latent\n    )\n    if not cfg.skip_type2:\n        simargs.update(\n            z1_type1_evidence_latent=z1_type1_evidence_latent, z1_type1_evidence_base=z1_type1_evidence_base, z1_type1_evidence=z1_type1_evidence,\n            c_conf_latent=c_conf_latent, c_conf_base=c_conf_base, c_conf=c_conf\n        )\n\n    if compute_stats:\n        accuracy = (x_stim_category == d_dec).astype(int)\n        type1_stats = dict(\n            accuracy=np.mean(accuracy),\n            d1 = norm.ppf(min(1 - 1e-3, max(1e-3, d_dec[x_stim_category == 1].mean()))) - \\\n                 norm.ppf(min(1 - 1e-3, max(1e-3, d_dec[x_stim_category == 0].mean().mean()))),\n            choice_bias=d_dec.mean(),\n        )\n        simargs.update(type1_stats=type1_stats)\n        if not cfg.skip_type2:\n            bounds = np.arange(0, 0.81, 0.2)\n            fit = type2_SDT_MLE(x_stim_category.flatten(), d_dec.flatten(), discretize_confidence_with_bounds(c_conf.flatten(), bounds), len(bounds))\n            type2_stats = dict(\n                confidence=c_conf.mean(),\n                auroc2=type2roc(accuracy.flatten(), c_conf.flatten()),\n                mratio=fit.M_ratio\n            )\n            simargs.update(type2_stats=type2_stats)\n            if 'type2_criteria' in params:\n                params_extra = dict(\n                    type2_criteria_absolute=[np.sum(params['type2_criteria'][:i + 1]) for i in range(len(params['type2_criteria']))],\n                    type2_criteria_bias=np.mean(params['type2_criteria']) * (len(params['type2_criteria']) + 1) - 1\n                )\n                simargs.update(params_extra=params_extra)\n\n\n    simulation = Simulation(**simargs)\n    if verbosity:\n        print_dataset_characteristics(simulation)\n\n    return simulation\n</code></pre>"},{"location":"gendata/#remeta.gendata.simu_type1_responses","title":"simu_type1_responses","text":"<pre><code>simu_type1_responses(x_stim, params, cfg)\n</code></pre> Source code in <code>remeta/gendata.py</code> <pre><code>def simu_type1_responses(x_stim, params, cfg):\n\n    if (cfg.type1_noise_signal_dependency != 'none') or (cfg.enable_type1_param_noise == 2):\n        type1_noise = compute_signal_dependent_type1_noise(\n            x_stim=x_stim, type1_noise_signal_dependency=cfg.type1_noise_signal_dependency, **params)\n    else:\n        type1_noise = params['type1_noise']\n\n    type1_param_thresh = _check_param(params['type1_thresh']) if cfg.enable_type1_param_thresh else (0, 0)\n    type1_param_bias = _check_param(params['type1_bias']) if cfg.enable_type1_param_bias else (0, 0)\n\n    if cfg.enable_type1_param_nonlinear_encoding_gain:\n        x_stim_transform = compute_nonlinear_encoding(\n            x_stim, params['type1_nonlinear_encoding_gain'],\n            params['type1_nonlinear_encoding_transition'] if cfg.enable_type1_param_nonlinear_encoding_transition else None)\n    else:\n        x_stim_transform = x_stim\n\n    y_decval_latent = np.full(x_stim_transform.shape, np.nan)\n    y_decval_latent[x_stim_transform &lt; 0] = (np.abs(x_stim_transform[x_stim_transform &lt; 0]) &gt; type1_param_thresh[0]) * \\\n                                   x_stim_transform[x_stim_transform &lt; 0] + type1_param_bias[0]\n    y_decval_latent[x_stim_transform &gt;= 0] = (np.abs(x_stim_transform[x_stim_transform &gt;= 0]) &gt; type1_param_thresh[1]) * \\\n                                    x_stim_transform[x_stim_transform &gt;= 0] + type1_param_bias[1]\n\n    y_decval = y_decval_latent + logistic_dist(scale=type1_noise * np.sqrt(3) / np.pi).rvs(size=x_stim_transform.shape)\n    d_dec = (y_decval &gt;= 0).astype(int)\n\n    return y_decval_latent, y_decval, d_dec\n</code></pre>"},{"location":"gendata/#remeta.gendata.type1_evidence_to_confidence","title":"type1_evidence_to_confidence","text":"<pre><code>type1_evidence_to_confidence(z1_type1_evidence, type2_evidence_bias_mult=1, type1_noise=None, type1_thresh=None, type1_noise_heteroscedastic=None, type1_noise_signal_dependency='none', y_decval=None, x_stim=None, **kwargs)\n</code></pre> <p>Transformation from type 1 evidence (z1) to confidence (c).</p>"},{"location":"gendata/#remeta.gendata.type1_evidence_to_confidence--parameters","title":"Parameters","text":"<p>z1_type1_evidence : array-like     Evidence at the type 1 level (= absolute decision value). type2_evidence_bias_mult : float or array-like     Multiplicative metacognitive bias parameter loading on evidence. type1_noise : float or array-like     Type 1 noise parameter. Can be array-like in case of signal-dependent type 1 noise. type1_noise_heteroscedastic : float or array-like     Signal-dependent type 1 noise parameter. type1_noise_signal_dependency : str     Signal-dependent type 1 noise type. One of 'linear', 'power', 'exponential', 'logarithm'. y_decval : array-like     Decision values. x_stim : array-like     Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value     codes the intensity. kwargs : dict     Convenience parameter to avoid an error if irrelevant parameters are passed.</p>"},{"location":"gendata/#remeta.gendata.type1_evidence_to_confidence--returns","title":"Returns","text":"<p>c_conf : array-like     Model-predicted confidence.</p> Source code in <code>remeta/transform.py</code> <pre><code>def type1_evidence_to_confidence(z1_type1_evidence,\n                                 type2_evidence_bias_mult=1,\n                                 type1_noise=None, type1_thresh=None,\n                                 type1_noise_heteroscedastic=None, type1_noise_signal_dependency='none',\n                                 y_decval=None, x_stim=None,\n                                 **kwargs):  # noqa\n    \"\"\"\n    Transformation from type 1 evidence (z1) to confidence (c).\n\n    Parameters\n    ----------\n    z1_type1_evidence : array-like\n        Evidence at the type 1 level (= absolute decision value).\n    type2_evidence_bias_mult : float or array-like\n        Multiplicative metacognitive bias parameter loading on evidence.\n    type1_noise : float or array-like\n        Type 1 noise parameter. Can be array-like in case of signal-dependent type 1 noise.\n    type1_noise_heteroscedastic : float or array-like\n        Signal-dependent type 1 noise parameter.\n    type1_noise_signal_dependency : str\n        Signal-dependent type 1 noise type. One of 'linear', 'power', 'exponential', 'logarithm'.\n    y_decval : array-like\n        Decision values.\n    x_stim : array-like\n        Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value\n        codes the intensity.\n    kwargs : dict\n        Convenience parameter to avoid an error if irrelevant parameters are passed.\n\n    Returns\n    ----------\n    c_conf : array-like\n        Model-predicted confidence.\n    \"\"\"\n    z1_type1_evidence = np.atleast_1d(z1_type1_evidence)\n\n    if ((type1_noise_signal_dependency != 'none') or (hasattr(type1_noise, '__len__') and len(type1_noise) == 2)):\n        if x_stim is None:\n            raise ValueError('Type 1 noise is signal-dependent, but stimuli (x_stim) have not been '\n                             'passed.')\n        type1_noise = compute_signal_dependent_type1_noise(\n            x_stim.reshape(-1, 1) if (x_stim.ndim == 1) and (z1_type1_evidence.ndim == 2) else x_stim,\n            type1_noise=type1_noise, type1_thresh=type1_thresh, type1_noise_heteroscedastic=type1_noise_heteroscedastic,\n            type1_noise_signal_dependency=type1_noise_signal_dependency)\n\n    z2_type2_evidence = type2_evidence_bias_mult * z1_type1_evidence\n    c_conf = np.tanh(np.pi * z2_type2_evidence / (2 * np.sqrt(3) * type1_noise))\n\n    return c_conf\n</code></pre>"},{"location":"gendata/#remeta.gendata.type2_SDT_MLE","title":"type2_SDT_MLE","text":"<pre><code>type2_SDT_MLE(stimID, response, rating, nRatings, cellpadding=None, equalVariance=1)\n</code></pre> Source code in <code>remeta/type2_SDT.py</code> <pre><code>def type2_SDT_MLE(stimID, response, rating, nRatings, cellpadding=None, equalVariance=1):\n    # out = type2_SDT(input)\n    #\n    # Given data from an experiment where an observer discriminates between two\n    # stimulus alternatives on every trial and provides confidence ratings,\n    # provides a type 2 SDT analysis of the data.\n    #\n    # The function estimates the parameters of the unequal variance SDT model,\n    # and uses those estimates to find a maximum likelihood estimate of\n    # meta-da.\n    #\n    # INPUTS\n    #\n    # format of the input may be either:\n    #\n    # 1) stimID, response, rating, nRatings, (cellpadding), (equalVariance)\n    #    where each of the first 3 inputs is a 1xN vector describing the outcome\n    #    of N trials. Contents of input should be as follows.\n    #\n    #    stimID   : 0=S1 stimulus presented, 1=S2 stimulus presented\n    #    response : 0=subject responded S1, 1=subject responded S2\n    #    rating   : values ranges from 1 to m where 1 is the lowest rating\n    #               and m is the highest.\n    #\n    #               All trials where any of these prescribed ranges of values\n    #               are violated are omitted from analysis.\n    #\n    #    nRatings : the number of ratings available to the subject (e.g. for a\n    #               confidence scale of 1-4, nRatings=4).\n    #    cellpadding : if any data cells (e.g. high confidence \"S2\" responses)\n    #               are empty, then the value of cellpadding will be added\n    #               to every data cell. If not specified, default = 1/(2*nRatings)\n    #    equalVariance : if 1, force analysis to use the equal variance SDT\n    #               model. If 0, use an estimate of s = sd(S1) / sd(S2) where\n    #               s is the slope of the zROC data (estimated using MLE).\n    #               If not specified, default = 0.\n    #\n    # 2) nR_S1, nR_S2, (cellpadding), (equalVariance)\n    #    where these are vectors containing the total number of responses in\n    #    each response modality, conditional on presentation of S1 and S2.\n    #    size of each array is 2*nRatings, where each element corresponds to a\n    #    count of responses in each response modality. Response categories are\n    #    ordered as follows:\n    #    highest conf \"S1\" ... lowest conf \"S1\", lowest conf \"S2\", ... highest conf \"S2\"\n    #\n    #    e.g. if nR_S1 = [100 50 20 10 5 1], then when stimulus S1 was\n    #    presented, the subject had the following response counts:\n    #    responded S1, rating=3 : 100 times\n    #    responded S1, rating=2 : 50 times\n    #    responded S1, rating=1 : 20 times\n    #    responded S2, rating=1 : 10 times\n    #    responded S2, rating=2 : 5 times\n    #    responded S2, rating=3 : 1 time\n    #\n    #    cellpadding and equalVariance are defined as above.\n    #\n    #\n    #\n    #\n    # OUTPUTS\n    #\n    # out.d_a       : d_a for input data. If s=1, d_a = d'\n    # out.meta_d_a  : meta_d_a for input data\n    # out.M_ratio   : meta_d_a / d_a; measure of metacognitive efficiency\n    # out.M_diff    : meta_d_a - d_a; measure of metacognitive efficiency\n    # out.c_a       : criterion c_a for input data. If s=1, c_a = c.\n    # out.cprime    : relative criterion used for type 2 estimates. c' = c_a / d_a\n    # out.s         : ratio of evidence distribution standard deviations assumed for the analysis.\n    # out.type2_fit : output of fit_meta_d_MLE for the type 2 SDT fit.\n\n    # 9/24/10 - bm - fixed program-crashing bug for (nR_S1, nR_S2) input\n    # 9/7/10 - bm - wrote it\n\n    ## parse inputs\n    if len(stimID) == 0:\n        raise ValueError(\"Empty data\")\n\n    if cellpadding is None:\n        cellpadding = 1 / (2 * nRatings)\n\n    # filters bad trials\n    f = ((stimID == 0) | (stimID == 1)) &amp; ((response == 0) | (response == 1)) &amp; ((rating &gt;= 1) &amp; (rating &lt;= nRatings))\n    stimID = stimID[f]\n    response = response[f]\n    rating = rating[f]\n\n    # convert to trial count format...\n    nR_S1, nR_S2 = np.full(nRatings*2, np.nan), np.full(nRatings*2, np.nan)\n\n    # get tallies of \"S1\" rating responses for S1 and S2 stim\n    for i in range(nRatings):\n        nR_S1[i] = np.sum((stimID == 0) &amp; (response == 0) &amp; (rating == nRatings - i))\n        nR_S2[i] = np.sum((stimID == 1) &amp; (response == 0) &amp; (rating == nRatings - i))\n\n    # get tallies of \"S2\" rating responses for S1 and S2 stim\n    for i in range(nRatings):\n        nR_S1[i + nRatings] = np.sum((stimID == 0) &amp; (response == 1) &amp; (rating == i + 1))\n        nR_S2[i + nRatings] = np.sum((stimID == 1) &amp; (response == 1) &amp; (rating == i + 1))\n\n    if np.any(nR_S1 == 0) | np.any(nR_S2 == 0):\n        nR_S1 = nR_S1 + cellpadding\n        nR_S2 = nR_S2 + cellpadding\n\n    ## standard SDT analysis\n\n    if equalVariance:\n        s = 1\n\n    ## type 2 SDT analysis\n\n    fit = fit_meta_d_MLE(nR_S1, nR_S2, s)\n\n    return fit\n</code></pre>"},{"location":"gendata/#remeta.gendata.type2roc","title":"type2roc","text":"<pre><code>type2roc(correct, conf, nbins=5)\n</code></pre> Source code in <code>remeta/type2_SDT.py</code> <pre><code>def type2roc(correct, conf, nbins=5):\n    # Calculate area under type 2 ROC\n    #\n    # correct - vector of 1 x ntrials, 0 for error, 1 for correct\n    # conf - vector of continuous confidence ratings between 0 and 1\n    # nbins - how many bins to use for discretization\n\n    bs = 1 / nbins\n    H2, FA2 = np.full(nbins, np.nan), np.full(nbins, np.nan)\n    for c in range(nbins):\n        if c:\n            H2[nbins - c - 1] = np.sum((conf &gt; c*bs) &amp; (conf &lt;= (c+1)*bs) &amp; (correct).astype(bool)) + 0.5\n            FA2[nbins - c - 1] = np.sum((conf &gt; c*bs) &amp; (conf &lt;= (c+1)*bs) &amp; ~(correct).astype(bool)) + 0.5\n        else:\n            H2[nbins - c - 1] = np.sum((conf &gt;= c * bs) &amp; (conf &lt;= (c + 1) * bs) &amp; (correct).astype(bool)) + 0.5\n            FA2[nbins - c - 1] = np.sum((conf &gt;= c * bs) &amp; (conf &lt;= (c + 1) * bs) &amp; ~(correct).astype(bool)) + 0.5\n\n    H2 /= np.sum(H2)\n    FA2 /= np.sum(FA2)\n    cum_H2 = np.hstack((0, np.cumsum(H2)))\n    cum_FA2 = np.hstack((0, np.cumsum(FA2)))\n\n    k = np.full(nbins, np.nan)\n    for c in range(nbins):\n        k[c] = (cum_H2[c+1] - cum_FA2[c])**2 - (cum_H2[c] - cum_FA2[c+1])**2\n\n    auroc2 = 0.5 + 0.25*np.sum(k)\n\n    return auroc2\n</code></pre>"},{"location":"plot/","title":"remeta.plot","text":""},{"location":"plot/#remeta.plot.color_data","title":"color_data  <code>module-attribute</code>","text":"<pre><code>color_data = [0.6, 0.6, 0.6]\n</code></pre>"},{"location":"plot/#remeta.plot.color_generative_type2","title":"color_generative_type2  <code>module-attribute</code>","text":"<pre><code>color_generative_type2 = array([231, 168, 116]) / 255\n</code></pre>"},{"location":"plot/#remeta.plot.color_generative_type2b","title":"color_generative_type2b  <code>module-attribute</code>","text":"<pre><code>color_generative_type2b = array([47, 158, 47]) / 255\n</code></pre>"},{"location":"plot/#remeta.plot.color_logistic","title":"color_logistic  <code>module-attribute</code>","text":"<pre><code>color_logistic = (0.55, 0.55, 0.69)\n</code></pre>"},{"location":"plot/#remeta.plot.color_model","title":"color_model  <code>module-attribute</code>","text":"<pre><code>color_model = array([57, 127, 95]) / 255\n</code></pre>"},{"location":"plot/#remeta.plot.color_model_wrong","title":"color_model_wrong  <code>module-attribute</code>","text":"<pre><code>color_model_wrong = array([152, 75, 75]) / 255\n</code></pre>"},{"location":"plot/#remeta.plot.symbols","title":"symbols  <code>module-attribute</code>","text":"<pre><code>symbols = dict(type1_noise='$\\\\sigma_\\\\mathrm{s}$', type1_noise_heteroscedastic='$\\\\sigma_\\\\mathrm{s,1}$', type1_thresh='$\\\\vartheta_\\\\mathrm{s}$', type1_bias='$\\\\delta_\\\\mathrm{s}$', type2_noise='$\\\\sigma_\\\\mathrm{m}$', type2_evidence_bias_mult='$\\\\varphi_\\\\mathrm{m}$', type2_criteria='$c_\\\\mathrm{i}$')\n</code></pre>"},{"location":"plot/#remeta.plot.Configuration","title":"Configuration  <code>dataclass</code>","text":"<p>Configuration for the ReMeta toolbox</p>"},{"location":"plot/#remeta.plot.Configuration--parameters","title":"Parameters","text":"<p>*** Basic definition of the model *** type2_fitting_type : str (default: 'criteria')     Whether confidence is fitted with discrete criteria or as a continuous variable.     Possible values: 'criteria', 'continuous' type2_noise_type : str (default: 'noisy-report)     Whether the model considers noise at readout or report.     Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature' type2_noise_dist : str         (default: noisy-report + criteria -&gt; 'truncated_norm_mode'                   noisy-report + continuous -&gt; 'truncated_norm_mode'                   noisy-readout + criteria -&gt; 'truncated_norm_mode'                   noisy-readout + continuous -&gt; 'truncated_norm_mode'                   noisy-temperature + criteria -&gt; 'lognorm_mode'                   noisy-temperature + continuous -&gt; 'truncated_norm_mode'         )     Metacognitive noise distribution.     Possible values:         noisy_report: 'beta_mean_std', 'beta_mode_std', 'beta_mode',                       'truncated_norm_mode_std', 'truncated_norm_mode' (default),                       'truncated_gumbel_mode_std', 'truncated_gumbel_mode',                       'truncated_lognorm_mode_std', 'truncated_lognorm', 'truncated_lognorm_mode',                       'truncated_lognorm_mean'         noisy_readout: 'lognorm_median_std', 'lognorm_mean', 'lognorm_mode', 'lognorm_mode_std', 'lognorm_mean_std',                        'gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv',                        'betaprime_mean_std',                        'truncated_norm_mode_std', 'truncated_norm_mode',                        'truncated_gumbel_mode_std', 'truncated_gumbel_mode'         noisy_temperature: same as noisy_readout</p> <p>*** Enable or disable specific parameters *** * Each setting can take the values 0, 1 or 2: *    0: Disable parameter. *    1: Enable parameter. *    2: Enable parameter and fit separate values for the negative and positive stimulus category         (works only for type 1 parameters!) enable_type1_param_noise : int (default: 1)     Fit separate type 1 noise parameters for both stimulus categories. enable_type1_param_noise_heteroscedastic : int (default: 0)     Fit an additional type 1 noise parameter for signal-dependent type 1 noise (the type of dependency is     defined via <code>type1_noise_signal_dependency</code>). enable_type1_param_nonlinear_encoding_gain : int (default: 0) enable_type1_param_nonlinear_encoding_transition : int (default: 0) enable_type1_param_thresh : int (default: 0)     Fit a type 1 threshold. enable_type1_param_bias : int (default: 1)     Fit a type 1 bias towards one of the stimulus categories. enable_type2_param_noise : int (default: 1)     Fit a metacognitive noise parameter enable_type2_param_evidence_bias_mult : int (default: 0)     Fit a multiplicative metacognitive bias loading on evidence. enable_type2_param_criteria : int (default: 0)     Fit confidence criteria.</p> <p>*** Additional options to specify the nature of type 2 fitting *** n_discrete_confidence_levels : int (default: 5)     Number of confidence criteria. Only applies in case of type2_fitting_type='criteria'.</p> <p>*** Define fitting characteristics of the parameters *** * The fitting of each parameter is characzerized as follows: *     1) An initial guess. *     2) Lower and upper bound. *     3) Grid range, i.e. list of values that are tested during the initial gridsearch search. * Sensible default values are provided for all parameters. To tweak those, one can either define an entire * ParameterSet, which is a container for a set of parameters, or each parameter individually. Note that the * parameters must be either defined as a Parameter instance or as List[Parameter] in case when separate values are * fitted for the positive and negative stimulus category/decision value). paramset_type1 : ParameterSet     Parameter set for the type 1 stage. paramset_type2 : ParameterSet     Parameter set for the type 2 stage. paramset : ParameterSet     Parameter set for both stages.</p> Union[Parameter, List[Parameter]]  (default: 1) <p>Parameter for type 1 noise.</p> <p>_type1_param_noise_heteroscedastic : Union[Parameter, List[Parameter]]  (default: 0)     Parameter for signal-dependent type 1 noise. _type1_param_nonlinear_encoding_gain : Union[Parameter, List[Parameter]]  (default: 0)     Gain parameter for nonlinear encoding (higher values -&gt; stronger nonlinearity). _type1_param_nonlinear_encoding_transition : Union[Parameter, List[Parameter]]  (default: 0)     Transition Parameter for nonlinear encoding (). type1_noise_signal_dependency: str (default: 'none')     Can be one of 'none', 'multiplicative', 'power', 'exponential' or 'logarithm'. _type1_param_thresh : Union[Parameter, List[Parameter]] (default: 0)     Parameter for the type 1 threshold. _type1_param_bias : Union[Parameter, List[Parameter]]  (default: 1)     Parameter for the type 1 bias. _type2_param_noise : Union[Parameter, List[Parameter]]  (default: 1)     Parameter for metacognitive noise. _type2_param_evidence_bias_mult : Union[Parameter, List[Parameter]]  (default: 0)     Parameter for a multiplicative metacognitive bias loading on evidence. type2_param_confidence_criteria : List[Parameter]  (default: 1)     List of parameter specifying the confidence criteria.</p> <p>*** Skip type 2 fitting *** skip_type2 : bool (default: False)     If True, ignore type 2 settings in the setup of the model configuration &amp; don't fit type 2 stage.</p> <p>*** Methodoligcal aspects of parameter fitting *** optim_type1_gridsearch : bool (default: False)     If True, perform initial (usually coarse) gridsearch search for type 1 fitting, based on the gridsearch defined     for a Parameter. optim_type1_fine_gridsearch : bool (default: False)     If True, perform an iteratively finer gridsearch search for each parameter (type 1). optim_type1_minimize_along_grid : bool (default: False)     If True, do sqlqp minimization for at each grid point (type 1). optim_type1_global_minimization : str (default: None)     Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with     a global minimizer (type 1). optim_type1_scipy_solvers : str or Tuple/List (default: 'trust-constr')     Set scipy.optimize.minimize gradient method (type 1)     If provided as Tuple/List, test different gradient methods and take the best optim_type2_gridsearch : bool (default: True)     If True, perform initial (usually coarse) gridsearch search for type 2 fitting, based on the gridsearch defined     for a Parameter. optim_type2_fine_gridsearch : bool (default: False)     If True, perform an iteratively finer gridsearch search for each parameter (type 2). optim_type2_minimize_along_grid : bool (default: False)     If True, do sqlqp minimization for at each grid point (type 2). optim_type2_global_minimization : str (default: None)     Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with     a global minimizer (type 2). optim_type2_scipy_solvers : str or Tuple/List (default: ('slsqp', 'Nelder-Mead'))     Set scipy.optimize.minimize gradient method (type 2)     If provided as Tuple/List, test different gradient methods and take the best optim_type2_slsqp_epsilon : float or Tuple/List (default: None)     Set parameter epsilon parameter for the SLSQP optimization method (type 2).     If provided as Tuple/List, test different eps parameters and take the best optim_multiproc : bool (default: False)     If True, use all optim_multiproc_cores cores for parameter estimation. If False, use a single core. optim_multiproc_cores : int (default: -1)     If multicore processing es enabled via optim_multiproc, use optim_multiproc_cores cores for parameter estimation     (-1 for all cores minus 1).</p> <p>*** Preprocessing *** normalize_stimuli_by_max : bool (default: True)     If True, normalize provided stimuli by their maximum value.</p> <p>*** Parameters for the type 2 likelihood computation *** min_type1_likelihood : float     Minimum probability used during the type 1 likelihood computation min_type2_likelihood : float     Minimum probability used during the type 2 likelihood computation type2_binsize : float     Integration bin size for the computation of the likelihood around empirical confidence values y_decval_range_nsds : int     Number of standard deviations around the mean considered for type 1 uncertainty. y_decval_range_nbins : int     Number of discrete decision values bins that are considered to represent type 1 uncertainty. resolution_noisy_temperature : float     Quintile resolution for the marginalization of type 1 noise in case of type2_noise_type 'noisy_temperature'. experimental_min_uniform_type2_likelihood : bool     Instead of using a minimum probability during the likelihood computation, use a maximum cumulative     likelihood based on a 'guessing' model experimental_wrap_type2_integration_window : bool (default: False)     Ensure constant window size for likelihood integration at the bounds.     Only applies in case of type2_fitting_type='continuous' and experimental_disable_type2_binsize=False experimental_include_incongruent_y_decval : bool (default: False)     Include incongruent decision values (i.e., sign(actual choice) != sign(decision value)) for the likelihood     computation experimental_disable_type2_binsize : bool (default: None)     Do not use an integegration window for likelihood computation.     Only applies in case of type2_fitting_type='continuous'</p> <p>*** Other *** true_params : Dict     Pass true (known) parameter values. This can be useful for testing to compare the likelihood of true and     fitted parameters. The likelihood of true parameters is returned (and printed). initilialize_fitting_at_true_params : bool (default: False)     Option to initialize the parameter fitting procedure at the true parameters; this can be helpful for testing. silence_configuration_warnings : bool (default: False)     If True, ignore warnings about user-specified settings. print_configuration : bool (default: True)     If True, print the configuration at instatiation of the ReMeta class.</p> Source code in <code>remeta/configuration.py</code> <pre><code>@reset_dataclass_on_init\n@dataclass\nclass Configuration(ReprMixin):\n    \"\"\"\n    Configuration for the ReMeta toolbox\n\n    Parameters\n    ----------\n    *** Basic definition of the model ***\n    type2_fitting_type : str (default: 'criteria')\n        Whether confidence is fitted with discrete *criteria* or as a continuous variable.\n        Possible values: 'criteria', 'continuous'\n    type2_noise_type : str (default: 'noisy-report)\n        Whether the model considers noise at readout or report.\n        Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'\n    type2_noise_dist : str\n            (default: noisy-report + criteria -&gt; 'truncated_norm_mode'\n                      noisy-report + continuous -&gt; 'truncated_norm_mode'\n                      noisy-readout + criteria -&gt; 'truncated_norm_mode'\n                      noisy-readout + continuous -&gt; 'truncated_norm_mode'\n                      noisy-temperature + criteria -&gt; 'lognorm_mode'\n                      noisy-temperature + continuous -&gt; 'truncated_norm_mode'\n            )\n        Metacognitive noise distribution.\n        Possible values:\n            noisy_report: 'beta_mean_std', 'beta_mode_std', 'beta_mode',\n                          'truncated_norm_mode_std', 'truncated_norm_mode' (default),\n                          'truncated_gumbel_mode_std', 'truncated_gumbel_mode',\n                          'truncated_lognorm_mode_std', 'truncated_lognorm', 'truncated_lognorm_mode',\n                          'truncated_lognorm_mean'\n            noisy_readout: 'lognorm_median_std', 'lognorm_mean', 'lognorm_mode', 'lognorm_mode_std', 'lognorm_mean_std',\n                           'gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv',\n                           'betaprime_mean_std',\n                           'truncated_norm_mode_std', 'truncated_norm_mode',\n                           'truncated_gumbel_mode_std', 'truncated_gumbel_mode'\n            noisy_temperature: same as noisy_readout\n\n\n    *** Enable or disable specific parameters ***\n    * Each setting can take the values 0, 1 or 2:\n    *    0: Disable parameter.\n    *    1: Enable parameter.\n    *    2: Enable parameter and fit separate values for the negative and positive stimulus category\n            (works only for type 1 parameters!)\n    enable_type1_param_noise : int (default: 1)\n        Fit separate type 1 noise parameters for both stimulus categories.\n    enable_type1_param_noise_heteroscedastic : int (default: 0)\n        Fit an additional type 1 noise parameter for signal-dependent type 1 noise (the type of dependency is\n        defined via `type1_noise_signal_dependency`).\n    enable_type1_param_nonlinear_encoding_gain : int (default: 0)\n    enable_type1_param_nonlinear_encoding_transition : int (default: 0)\n    enable_type1_param_thresh : int (default: 0)\n        Fit a type 1 threshold.\n    enable_type1_param_bias : int (default: 1)\n        Fit a type 1 bias towards one of the stimulus categories.\n    enable_type2_param_noise : int (default: 1)\n        Fit a metacognitive noise parameter\n    enable_type2_param_evidence_bias_mult : int (default: 0)\n        Fit a multiplicative metacognitive bias loading on evidence.\n    enable_type2_param_criteria : int (default: 0)\n        Fit confidence criteria.\n\n    *** Additional options to specify the nature of type 2 fitting ***\n    n_discrete_confidence_levels : int (default: 5)\n        Number of confidence criteria. Only applies in case of type2_fitting_type='criteria'.\n\n    *** Define fitting characteristics of the parameters ***\n    * The fitting of each parameter is characzerized as follows:\n    *     1) An initial guess.\n    *     2) Lower and upper bound.\n    *     3) Grid range, i.e. list of values that are tested during the initial gridsearch search.\n    * Sensible default values are provided for all parameters. To tweak those, one can either define an entire\n    * ParameterSet, which is a container for a set of parameters, or each parameter individually. Note that the\n    * parameters must be either defined as a Parameter instance or as List[Parameter] in case when separate values are\n    * fitted for the positive and negative stimulus category/decision value).\n    paramset_type1 : ParameterSet\n        Parameter set for the type 1 stage.\n    paramset_type2 : ParameterSet\n        Parameter set for the type 2 stage.\n    paramset : ParameterSet\n        Parameter set for both stages.\n\n    _type1_param_noise : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for type 1 noise.\n    _type1_param_noise_heteroscedastic : Union[Parameter, List[Parameter]]  (default: 0)\n        Parameter for signal-dependent type 1 noise.\n    _type1_param_nonlinear_encoding_gain : Union[Parameter, List[Parameter]]  (default: 0)\n        Gain parameter for nonlinear encoding (higher values -&gt; stronger nonlinearity).\n    _type1_param_nonlinear_encoding_transition : Union[Parameter, List[Parameter]]  (default: 0)\n        Transition Parameter for nonlinear encoding ().\n    type1_noise_signal_dependency: str (default: 'none')\n        Can be one of 'none', 'multiplicative', 'power', 'exponential' or 'logarithm'.\n    _type1_param_thresh : Union[Parameter, List[Parameter]] (default: 0)\n        Parameter for the type 1 threshold.\n    _type1_param_bias : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for the type 1 bias.\n    _type2_param_noise : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for metacognitive noise.\n    _type2_param_evidence_bias_mult : Union[Parameter, List[Parameter]]  (default: 0)\n        Parameter for a multiplicative metacognitive bias loading on evidence.\n    type2_param_confidence_criteria : List[Parameter]  (default: 1)\n        List of parameter specifying the confidence criteria.\n\n    *** Skip type 2 fitting ***\n    skip_type2 : bool (default: False)\n        If True, ignore type 2 settings in the setup of the model configuration &amp; don't fit type 2 stage.\n\n    *** Methodoligcal aspects of parameter fitting ***\n    optim_type1_gridsearch : bool (default: False)\n        If True, perform initial (usually coarse) gridsearch search for type 1 fitting, based on the gridsearch defined\n        for a Parameter.\n    optim_type1_fine_gridsearch : bool (default: False)\n        If True, perform an iteratively finer gridsearch search for each parameter (type 1).\n    optim_type1_minimize_along_grid : bool (default: False)\n        If True, do sqlqp minimization for at each grid point (type 1).\n    optim_type1_global_minimization : str (default: None)\n        Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with\n        a global minimizer (type 1).\n    optim_type1_scipy_solvers : str or Tuple/List (default: 'trust-constr')\n        Set scipy.optimize.minimize gradient method (type 1)\n        If provided as Tuple/List, test different gradient methods and take the best\n    optim_type2_gridsearch : bool (default: True)\n        If True, perform initial (usually coarse) gridsearch search for type 2 fitting, based on the gridsearch defined\n        for a Parameter.\n    optim_type2_fine_gridsearch : bool (default: False)\n        If True, perform an iteratively finer gridsearch search for each parameter (type 2).\n    optim_type2_minimize_along_grid : bool (default: False)\n        If True, do sqlqp minimization for at each grid point (type 2).\n    optim_type2_global_minimization : str (default: None)\n        Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with\n        a global minimizer (type 2).\n    optim_type2_scipy_solvers : str or Tuple/List (default: ('slsqp', 'Nelder-Mead'))\n        Set scipy.optimize.minimize gradient method (type 2)\n        If provided as Tuple/List, test different gradient methods and take the best\n    optim_type2_slsqp_epsilon : float or Tuple/List (default: None)\n        Set parameter epsilon parameter for the SLSQP optimization method (type 2).\n        If provided as Tuple/List, test different eps parameters and take the best\n    optim_multiproc : bool (default: False)\n        If True, use all optim_multiproc_cores cores for parameter estimation. If False, use a single core.\n    optim_multiproc_cores : int (default: -1)\n        If multicore processing es enabled via optim_multiproc, use optim_multiproc_cores cores for parameter estimation\n        (-1 for all cores minus 1).\n\n    *** Preprocessing ***\n    normalize_stimuli_by_max : bool (default: True)\n        If True, normalize provided stimuli by their maximum value.\n\n    *** Parameters for the type 2 likelihood computation ***\n    min_type1_likelihood : float\n        Minimum probability used during the type 1 likelihood computation\n    min_type2_likelihood : float\n        Minimum probability used during the type 2 likelihood computation\n    type2_binsize : float\n        Integration bin size for the computation of the likelihood around empirical confidence values\n    y_decval_range_nsds : int\n        Number of standard deviations around the mean considered for type 1 uncertainty.\n    y_decval_range_nbins : int\n        Number of discrete decision values bins that are considered to represent type 1 uncertainty.\n    resolution_noisy_temperature : float\n        Quintile resolution for the marginalization of type 1 noise in case of type2_noise_type 'noisy_temperature'.\n    experimental_min_uniform_type2_likelihood : bool\n        Instead of using a minimum probability during the likelihood computation, use a maximum cumulative\n        likelihood based on a 'guessing' model\n    experimental_wrap_type2_integration_window : bool (default: False)\n        Ensure constant window size for likelihood integration at the bounds.\n        Only applies in case of type2_fitting_type='continuous' and experimental_disable_type2_binsize=False\n    experimental_include_incongruent_y_decval : bool (default: False)\n        Include incongruent decision values (i.e., sign(actual choice) != sign(decision value)) for the likelihood\n        computation\n    experimental_disable_type2_binsize : bool (default: None)\n        Do not use an integegration window for likelihood computation.\n        Only applies in case of type2_fitting_type='continuous'\n\n\n    *** Other ***\n    true_params : Dict\n        Pass true (known) parameter values. This can be useful for testing to compare the likelihood of true and\n        fitted parameters. The likelihood of true parameters is returned (and printed).\n    initilialize_fitting_at_true_params : bool (default: False)\n        Option to initialize the parameter fitting procedure at the true parameters; this can be helpful for testing.\n    silence_configuration_warnings : bool (default: False)\n        If True, ignore warnings about user-specified settings.\n    print_configuration : bool (default: True)\n        If True, print the configuration at instatiation of the ReMeta class.\n    \"\"\"\n\n    type2_fitting_type: str = 'criteria'\n    type2_noise_type: str = 'noisy_report'\n    type2_noise_dist: str = None\n        # noisy-report + criteria -&gt; 'truncated_norm_mode'\n        # noisy-report + continuous -&gt; 'truncated_norm_mode'\n        # noisy-readout + criteria -&gt; 'truncated_norm_mode'\n        # noisy-readout + continuous -&gt; 'truncated_norm_mode'\n        # noisy-temperature + criteria -&gt; 'lognorm_mode'\n        # noisy-temperature + continuous -&gt; 'truncated_norm_mode'\n\n    enable_type1_param_noise: int = 1\n    enable_type1_param_thresh: int = 0\n    enable_type1_param_bias: int = 1\n    enable_type2_param_noise: int = 1\n    enable_type2_param_evidence_bias_mult: int = 0\n    enable_type2_param_criteria: int = 1\n    # Experimental:\n    enable_type1_param_noise_heteroscedastic: int = 0\n    enable_type1_param_nonlinear_encoding_gain: int = 0\n    enable_type1_param_nonlinear_encoding_transition: int = 0\n\n    n_discrete_confidence_levels: int = 5\n\n    paramset_type1: ParameterSet = None\n    paramset_type2: ParameterSet = None\n    paramset_all: ParameterSet = None\n\n    type1_param_noise_heteroscedastic: Parameter = Parameter(guess=0, bounds=(0, 10), grid_range=np.linspace(0, 1, 5))\n    type1_param_nonlinear_encoding_gain: Parameter = Parameter(guess=0, bounds=(-8/9, 10), grid_range=np.linspace(-0.5, 1, 5))\n    type1_param_nonlinear_encoding_transition: Parameter = Parameter(guess=1, bounds=(0.01, 10), grid_range=np.linspace(0.01, 2, 5))\n    type1_param_noise: Parameter = Parameter(guess=0.5, bounds=(0.001, 100), grid_range=np.linspace(0.1, 1, 8))\n    type1_param_thresh: Parameter = Parameter(guess=0, bounds=(0, 1), grid_range=np.linspace(0, 0.2, 5))\n    type1_param_bias: Parameter = Parameter(guess=0, bounds=(-1, 1), grid_range=np.linspace(-0.2, 0.2, 8))\n    type2_param_noise: Parameter = Parameter(guess=0.1, bounds=(0.05, 2), grid_range=np.linspace(0.1, 1, 8))\n    type2_param_evidence_bias_mult: Parameter = Parameter(guess=1, bounds=(0.5, 2), grid_range=np.linspace(0.5, 2, 8))\n    type2_param_criteria: Parameter = Parameter(bounds=(1e-8, 1))\n    type2_param_criteria_guesses: str | List[float] = 'equidistant'\n    type2_param_criteria_grid_ranges: str | List[np.ndarray] = 'equidistant'\n\n    type1_noise_signal_dependency: str = 'none'\n\n    skip_type2 = False\n\n    optim_type1_gridsearch: bool = False\n    optim_type1_fine_gridsearch: bool = False\n    optim_type1_minimize_along_grid: bool = False\n    optim_type1_global_minimization: str = None\n    _optim_type1_scipy_solvers_default = 'trust-constr'\n    optim_type1_scipy_solvers: str | List[str] | Tuple[str, ...] = 'trust-constr'\n    optim_type2_gridsearch: bool = True\n    optim_type2_fine_gridsearch: bool = False\n    optim_type2_minimize_along_grid: bool = False\n    optim_type2_global_minimization: str = None\n    optim_type2_scipy_solvers: str | List[str] | Tuple[str, ...] = ('slsqp', 'Nelder-Mead')\n    optim_type2_slsqp_epsilon: float = None\n    optim_multiproc: bool = False\n    optim_multiproc_cores: int = -1\n    _optim_multiproc_cores_effective: int = None\n\n    normalize_stimuli_by_max: bool = True\n    confidence_bounds_error: float = 0\n\n    min_type2_likelihood: float = 1e-10\n    min_type1_likelihood: float = 1e-10\n    type2_binsize: float = 0.01\n    y_decval_range_nsds: int = 5\n    y_decval_range_nbins: int = 101\n    resolution_noisy_temperature: float = 0.001\n\n    experimental_min_uniform_type2_likelihood: bool = False\n    experimental_wrap_type2_integration_window: bool = False\n    experimental_include_incongruent_y_decval: bool = False\n    experimental_disable_type2_binsize: bool = False\n\n    true_params: Dict = None\n    initilialize_fitting_at_true_params: bool = False\n    silence_configuration_warnings: bool = False\n    print_configuration: bool = False\n\n    type2_param_noise_min: float = 0.001\n\n    # setup_called = False\n\n    _type1_param_noise: Parameter | List[Parameter] = None\n    _type1_param_noise_heteroscedastic: Parameter | List[Parameter] = None\n    _type1_param_nonlinear_encoding_transition: Parameter | List[Parameter] = None\n    _type1_param_nonlinear_encoding_gain: Parameter | List[Parameter] = None\n    _type1_param_thresh: Parameter | List[Parameter] = None\n    _type1_param_bias: Parameter | List[Parameter] = None\n    _type2_param_noise: Parameter = None\n    _type2_param_evidence_bias_mult: Parameter = None\n    _type2_param_criteria: List[Parameter] = None\n\n    def setup(self, generative_mode=False):\n\n        if find_spec('multiprocessing_on_dill') is None:\n            warnings.warn(f'Multiprocessing on dill is not installed. Setting grid_multiproc is changed to False.')\n            self.optim_multiproc = False\n\n        if self.optim_multiproc:\n            from multiprocessing import cpu_count\n            self._optim_multiproc_cores_effective = max(1, (cpu_count() or 1) - 1) if self.optim_multiproc_cores == -1 \\\n                else self.optim_multiproc_cores\n\n        self._prepare_params_type1()\n        if self.skip_type2:\n            if self.optim_type2_slsqp_epsilon is None:\n                self.optim_type2_slsqp_epsilon = 1e-5\n        else:\n\n            if self.enable_type1_param_thresh and \\\n                (self.optim_type1_scipy_solvers == self._optim_type1_scipy_solvers_default):\n                self.optim_type1_scipy_solvers = ('trust-constr', 'Powell')\n\n\n            if self.type2_noise_dist is None:\n                if generative_mode:\n                    raise ValueError('In generative mode, you need to explicitly specify a type 2 noise distribution.')\n                else:\n                    if self.type2_noise_type == 'noisy_report':\n                        if (self.type2_fitting_type == 'criteria'):\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                    elif (self.type2_noise_type == 'noisy_readout'):\n                        if self.type2_fitting_type == 'criteria':\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                    elif self.type2_noise_type == 'noisy_temperature':\n                        if self.type2_fitting_type == 'criteria':\n                            self.type2_noise_dist = 'lognorm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n\n            self._prepare_params_type2()\n            if self.optim_type2_slsqp_epsilon is None:\n                self.optim_type2_slsqp_epsilon = 1e-5\n\n            if self.type2_binsize is None:\n                self.type2_binsize = 0.01\n\n        self._prepare_params_all()\n\n        self._check_compatibility(generative_mode=generative_mode)\n\n        if self.print_configuration:\n            self.print()\n        # self.setup_called = True\n\n    def _check_compatibility(self, generative_mode=False):\n\n        if not self.silence_configuration_warnings:\n\n            if not self.skip_type2:\n                if not self.enable_type2_param_noise:\n                    warnings.warn(f'Setting enable_type2_param_noise=False was provided -&gt; type2_param_noise is set to its default value '\n                                  f'({self._type2_param_noise_default}). You may change this value via the configuration.')\n\n                if (self.type2_noise_type == 'noisy_temperature') and self.type2_param_noise.default_changed and \\\n                    (self.type2_param_noise.bounds[0] &lt; 1e-5):\n                    warnings.warn('You manually changed the lower bound of the type 2 noise parameter for a '\n                                  'noisy-temperature model to a very low value (&lt;1e-5). Be warned that this may result '\n                                  'in numerical instabilities that severely distort the likelihood computation.')\n\n                if not generative_mode:\n                    # If the configuration instance is used for generating data, we should not complain\n                    # about fitting issues.\n\n                    if self.enable_type2_param_criteria and self.enable_type2_param_evidence_bias_mult:\n                        warnings.warn(\n                            'enable_type2_param_criteria=True in combination with enable_type2_param_evidence_bias_mult=True\\n'\n                            'can lead to biased parameter inferences. Use with caution.')\n\n                    if (self.type2_fitting_type == 'continuous') and self.enable_type2_param_criteria:\n                        raise ValueError(\"Setting type2_fitting_type='continuous' conflicts with enable_type2_param_criteria=1.'\")\n\n                    if (self.type2_fitting_type == 'criteria') and not self.enable_type2_param_criteria:\n                        warnings.warn(\"You selected type2_fitting_type='criteria', but did not enable type 2 criteria\\n\"\n                                      \"(enable_type2_param_criteria=0). This works, but be mindful that the model\\n\"\n                                      \"will assume equispaced ideal Bayesian observer criteria (respecting \\n\"\n                                      \"the setting n_discrete_confidence_levels).\")\n\n    def _prepare_params_type1(self):\n        # if self.paramset_type1 is None:\n\n            param_names_type1 = []\n            params_type1 = ('noise', 'noise_heteroscedastic', 'nonlinear_encoding_gain', 'nonlinear_encoding_transition', 'thresh', 'bias')\n            for param in params_type1:\n                if getattr(self, f'enable_type1_param_{param}'):\n                    param_names_type1 += [f'type1_{param}']\n                    if getattr(self, f'_type1_param_{param}') is None:\n                        param_definition = getattr(self, f'type1_param_{param}')\n                        if getattr(self, f'enable_type1_param_{param}') == 2:\n                            setattr(self, f'_type1_param_{param}', [param_definition, param_definition])\n                        else:\n                            setattr(self, f'_type1_param_{param}', param_definition)\n                        if self.true_params is not None and self.initilialize_fitting_at_true_params and f'type1_{param}' in self.true_params:\n                            getattr(self, f'_type1_param_{param}').guess = self.true_params[f'type1_{param}']\n\n            parameters = {k: getattr(self, f\"_type1_param_{k.split('type1_')[1]}\") for k in param_names_type1}\n            self.paramset_type1 = ParameterSet(parameters, param_names_type1)\n\n    def _prepare_params_type2(self):\n\n        # if self.paramset_type2 is None:\n\n            if self.enable_type2_param_noise and self._type2_param_noise is None and not self.type2_param_noise.default_changed:\n\n                lb = 0.05\n                self.type2_param_noise.bounds = dict(\n                    noisy_report = dict(\n                        beta_mean_std=(lb, 0.5),\n                        beta_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_norm_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_gumbel_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_lognorm_mode_std=(lb, 1 / np.sqrt(12)),\n                        beta_mode=(lb, 1),\n                        truncated_norm_mode=(lb, 1),\n                        truncated_gumbel_mode=(lb, 1),\n                        truncated_lognorm_mode=(lb, 4),\n                        truncated_lognorm_mean=(lb, 4),\n                        truncated_lognorm=(lb, 4)\n                    ),\n                    noisy_readout = dict(\n                        lognorm_mean=(lb, 1),\n                        lognorm_mode=(lb, 1),\n                        gamma_mean_std=(lb, 1),\n                        lognorm_mean_std=(lb, 2),\n                        lognorm_mode_std=(lb, 2),\n                        lognorm_median_std=(lb, 2),\n                        gamma_mean_cv=(lb, 2),\n                        gamma_mean=(lb, 2),\n                        gamma_mode_std=(lb, 2),\n                        gamma_mode=(lb, 2),\n                        betaprime_mean_std=(lb, 2),\n                        truncated_norm_mode_std=(lb, 2),\n                        truncated_norm_mode=(lb, 2),\n                        truncated_gumbel_mode_std=(lb, 2),\n                        truncated_gumbel_mode=(lb, 2)\n                    ),\n                    noisy_temperature = dict(\n                        lognorm_mean=(lb, 1),\n                        gamma_mean_std=(lb, 1),\n                        lognorm_mean_std=(lb, 2),\n                        lognorm_median_std=(lb, 2),\n                        gamma_mean_cv=(lb, 2),\n                        gamma_mean=(lb, 2),\n                        gamma_mode_std=(lb, 2),\n                        gamma_mode=(lb, 2),\n                        betaprime_mean_std=(lb, 2),\n                        truncated_norm_mode_std=(lb, 2),\n                        truncated_norm_mode=(lb, 2),\n                        truncated_gumbel_mode_std=(lb, 2),\n                        truncated_gumbel_mode=(lb, 2),\n                        lognorm_mode=(lb, 4),\n                        lognorm_mode_std=(lb, 10),\n                    )\n                )[self.type2_noise_type][self.type2_noise_dist]\n                self.type2_param_noise.grid_range = np.exp(np.linspace(np.log(self.type2_param_noise.bounds[0]),\n                                                                       np.log(self.type2_param_noise.bounds[1]), 10)[1:-1])\n\n            param_names_type2 = []\n            params_type2 = ('noise', 'evidence_bias_mult')\n            for param in params_type2:\n                if getattr(self, f'enable_type2_param_{param}'):\n                    param_names_type2 += [f'type2_{param}']\n                    if getattr(self, f'_type2_param_{param}') is None:\n                        param_definition = getattr(self, f'type2_param_{param}')\n                        setattr(self, f'_type2_param_{param}', param_definition.copy())\n                        if self.true_params is not None and self.initilialize_fitting_at_true_params and f'type2_{param}' in self.true_params:\n                            getattr(self, f'_type2_param_{param}').guess = self.true_params[f'type2_{param}']\n\n\n            if self.enable_type2_param_criteria:\n                param_names_type2 += [f'type2_criteria']\n                initialize_true = (self.initilialize_fitting_at_true_params and\n                                   self.true_params is not None and 'type2_criteria' in self.true_params)\n                setattr(self, f'_type2_param_criteria',\n                        [Parameter(\n                           guess=self.true_params['type2_criteria'][i] if initialize_true\n                                    else (1 / self.n_discrete_confidence_levels if self.type2_param_criteria_guesses == 'equidistant'\n                                          else self.type2_param_criteria_guesses[i]),\n                           bounds=self.type2_param_criteria.bounds,\n                           grid_range=np.linspace(0.05, 2 / self.n_discrete_confidence_levels, 4) if\n                                self.type2_param_criteria_grid_ranges == 'equidistant' else self.type2_param_criteria_grid_ranges[i]\n                        )\n                         for i in range(self.n_discrete_confidence_levels - 1)]\n                        )\n                if self.true_params is not None:\n                    if isinstance(self.true_params, dict):\n                        # if 'type2_criteria' not in self.true_params:\n                        #     raise ValueError('type2_criteria are missing from cfg.true_params')\n                        if 'type2_criteria' in self.true_params:\n                            self.true_params.update(\n                                type2_criteria_absolute=[np.sum(self.true_params['type2_criteria'][:i+1]) for i in range(len(self.true_params['type2_criteria']))],\n                                type2_criteria_bias=np.mean(self.true_params['type2_criteria'])*(len(self.true_params['type2_criteria'])+1)-1\n                            )\n                    elif isinstance(self.true_params, list):\n                        for s in range(len(self.true_params)):\n                            # if 'type2_criteria' not in self.true_params[s]:\n                            #     raise ValueError(f'type2_criteria are missing from cfg.true_params (subject {s})')\n                            if 'type2_criteria' in self.true_params[s]:\n                                self.true_params[s].update(\n                                    type2_criteria_absolute=[np.sum(self.true_params[s]['type2_criteria'][:i+1]) for i in range(len(self.true_params[s]['type2_criteria']))],\n                                    type2_criteria_bias=np.mean(self.true_params[s]['type2_criteria'])*(len(self.true_params[s]['type2_criteria'])+1)-1\n                                )\n\n            parameters = {k: getattr(self, f\"_type2_param_{k.split('type2_')[1]}\") for k in param_names_type2}\n            self.paramset_type2 = ParameterSet(parameters, param_names_type2)\n\n\n            self.check_type2_constraints()\n\n\n    def _prepare_params_all(self):\n\n        if self.skip_type2:\n            self.paramset = self.paramset_type1\n        else:\n            parameters_all = {**self.paramset_type1.parameters, **self.paramset_type2.parameters}\n            param_names_all = self.paramset_type1.param_names + self.paramset_type2.param_names\n            self.paramset = ParameterSet(parameters_all, param_names_all)\n            # for k, attr in self.paramset_type2.__dict__.items():\n            #     attr_old = getattr(self.paramset, k)\n            #     if isinstance(attr, list):\n            #         attr_new = attr_old + attr\n            #     elif isinstance(attr, dict):\n            #         attr_new = {**attr_old, **attr}\n            #     elif isinstance(attr, np.ndarray):\n            #         if attr.ndim == 1:\n            #             attr_new = np.hstack((attr_old, attr))\n            #         else:\n            #             attr_new = np.vstack((attr_old, attr))\n            #     elif isinstance(attr, int):\n            #         attr_new = attr_old + attr\n            #     elif attr is None:\n            #         if attr_old is None:\n            #             attr_new = None\n            #         else:\n            #             raise ValueError(f'Type 2 attribute is None, but type 1 attribute is not.')\n            #     else:\n            #         raise ValueError(f'Unexpected type {type(attr)}')\n            #     setattr(self.paramset, k, attr_new)\n\n\n\n\n    def print(self):\n        # print('***********************')\n        print(f'{self.__class__.__name__}')\n        for k, v in self.__dict__.items():\n            # if not self.skip_type2 or ('type2' not in k):\n            print('\\n'.join([f'\\t{k}: {v}']))\n        # print('***********************')\n\n    def __repr__(self):\n        txt = f'{self.__class__.__name__}\\n'\n        txt += '\\n'.join([f'\\t{k}: {v}' for k, v in self.__dict__.items()])\n        return txt\n\n    def check_type2_constraints(self):\n        pass\n</code></pre>"},{"location":"plot/#remeta.plot.LegendTitle","title":"LegendTitle","text":"Source code in <code>remeta/plot.py</code> <pre><code>class LegendTitle(object):\n    def __init__(self, text_props=None):\n        self.text_props = text_props or {}\n        super(LegendTitle, self).__init__()\n\n    def legend_artist(self, legend, orig_handle, fontsize, handlebox):  # noqa\n        x0, y0 = handlebox.xdescent, handlebox.ydescent\n        title = mtext.Text(x0, y0, orig_handle, **self.text_props)\n        # title = mtext.Text(x0, y0, r'\\underline{' + orig_handle + '}', usetex=True, **self.text_props)\n        handlebox.add_artist(title)\n        return title\n</code></pre>"},{"location":"plot/#remeta.plot._check_param","title":"_check_param","text":"<pre><code>_check_param(x)\n</code></pre> Source code in <code>remeta/util.py</code> <pre><code>def _check_param(x):\n    if hasattr(x, '__len__'):\n        if len(x) == 2:\n            return x\n        elif len(x) == 1:\n            return [x[0], x[0]]\n        else:\n            print(f'Something went wrong, parameter array has length {len(x)}')\n    else:\n        return [x, x]\n</code></pre>"},{"location":"plot/#remeta.plot.get_likelihood","title":"get_likelihood","text":"<pre><code>get_likelihood(x, type2_dist, type2_center, type2_noise, binsize_meta=0.001, logarithm=False)\n</code></pre> <p>Helper function to get the likelihood mass within type2_center \u00b1 binsize_meta for a type 2 noise distribution.</p> Source code in <code>remeta/type2_dist.py</code> <pre><code>def get_likelihood(x, type2_dist, type2_center, type2_noise, binsize_meta=1e-3, logarithm=False):\n    \"\"\"\n    Helper function to get the likelihood mass within type2_center \u00b1 binsize_meta for a type 2 noise distribution.\n    \"\"\"\n    dist = get_type2_dist(type2_dist=type2_dist, type2_center=type2_center, type2_noise=type2_noise)\n    likelihood = dist.cdf(x + binsize_meta) - dist.cdf(x - binsize_meta)\n    return np.log(np.maximum(1e-4, likelihood)) if logarithm else likelihood\n</code></pre>"},{"location":"plot/#remeta.plot.get_type2_dist","title":"get_type2_dist","text":"<pre><code>get_type2_dist(type2_dist, type2_center, type2_noise, type2_noise_type='noisy_report')\n</code></pre> <p>Helper function to select appropriately parameterized type 2 noise distributions.</p>"},{"location":"plot/#remeta.plot.get_type2_dist--parameters","title":"Parameters:","text":"<p>type2_dist : str     Name of the type 2 noise distribution. Check TYPE2_NOISE_DISTS for possible values. type2_noise : float or array-like of dtype float     \"True\" value of a noisy type 2 variable. Corresponds to type 1 noise (noisy-temperature), metacognitve     evidence (noisy-readout) or confidence (noisy-report).     For mean- and mode preserving distributions, the center corresponds to the mean (suffix _mean) and mode (suffix     _mode), respectively. type2_noise : float or array-like of dtype float with mode.shape     Spread parameter that represents metacognitve noise. For standard-deviation-preserving distributions, this     parameter corresponds to the standard deviation when sampling data from the distribution (suffix _std). type2_noise_type : str (default='noisy_report')     Metacognitive noise type. Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'.</p>"},{"location":"plot/#remeta.plot.get_type2_dist--returns","title":"Returns:","text":"<p>scipy.stats continuous distribution instance</p> Source code in <code>remeta/type2_dist.py</code> <pre><code>def get_type2_dist(type2_dist, type2_center, type2_noise, type2_noise_type='noisy_report'):\n    \"\"\"\n    Helper function to select appropriately parameterized type 2 noise distributions.\n\n    Parameters:\n    -----------\n    type2_dist : str\n        Name of the type 2 noise distribution. Check TYPE2_NOISE_DISTS for possible values.\n    type2_noise : float or array-like of dtype float\n        \"True\" value of a noisy type 2 variable. Corresponds to type 1 noise (noisy-temperature), metacognitve\n        evidence (noisy-readout) or confidence (noisy-report).\n        For mean- and mode preserving distributions, the center corresponds to the mean (suffix _mean) and mode (suffix\n        _mode), respectively.\n    type2_noise : float or array-like of dtype float with mode.shape\n        Spread parameter that represents metacognitve noise. For standard-deviation-preserving distributions, this\n        parameter corresponds to the standard deviation when sampling data from the distribution (suffix _std).\n    type2_noise_type : str (default='noisy_report')\n        Metacognitive noise type. Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'.\n\n    Returns:\n    --------\n    scipy.stats continuous distribution instance\n    \"\"\"\n\n    if type2_dist not in TYPE2_NOISE_DISTS:\n        raise ValueError(f\"Unkonwn distribution '{type2_dist}'.\")\n    elif (type2_noise_type == 'noisy_report') and type2_dist in TYPE2_NOISE_DISTS_READOUT_TEMPERATURE_ONLY:\n        raise ValueError(f\"Distribution '{type2_dist}' is only valid for noisy-readout or noisy-temperature models.\")\n    elif (type2_noise_type in ('noisy_readout', 'noisy_temperature')) and type2_dist in TYPE2_NOISE_DISTS_REPORT_ONLY:\n        raise ValueError(f\"Distribution '{type2_dist}' is only valid for noisy-report models.\")\n\n    if type2_noise &lt; 1e-10:\n        warnings.warn('Type 2 noise is smaller than 1e-10, which can lead to unstable numerical results. It is set'\n                      'to the hard minimum of 1e-10.')\n        type2_noise = 1e-10\n\n    if type2_dist == 'lognorm_median_std':\n        # type2_center = median, type2_noise = SD\n        s = np.maximum(1e-12, np.sqrt(np.log((1 + np.sqrt(1 + 4 * (type2_noise / type2_center) ** 2)) / 2)))\n        dist = lognorm(s=s, scale=type2_center)\n    elif type2_dist == 'lognorm_mean':\n        # type2_center = mean, type2_noise = SD in log space\n        dist = lognorm(s=type2_noise, scale=np.exp(-type2_noise ** 2 / 2) * type2_center)\n    elif type2_dist == 'lognorm_mode':\n        # type2_center = mode, type2_noise = SD in log space\n        if type2_noise &gt; 23:\n            # warnings.warn(f'Type 2 noise for a {type2_dist} distribution is too high. Capping at 23')\n            type2_noise = 23\n        dist = lognorm(s=type2_noise, scale=np.exp(type2_noise ** 2) * type2_center)\n    elif type2_dist == 'lognorm_mode_std':\n        # type2_center = mode, type2_noise = SD\n        shape, type2_noise = _params_lognorm_mode_std(np.maximum(1e-5, type2_center), type2_noise)\n        dist = lognorm(loc=0, scale=type2_noise, s=shape)\n    elif type2_dist == 'lognorm_mean_std':\n        # Corresponds to the CASSANDRE/LogN setup\n        # type2_center = mean, type2_noise = SD\n        type2_center = np.maximum(type2_center, 1e-12)\n        sigma2 = np.log1p((np.maximum(type2_noise, 0) / type2_center) ** 2)\n        scale = np.exp(np.log(type2_center) - sigma2 / 2)\n        dist = lognorm(loc=0, scale=scale, s=np.sqrt(sigma2))\n    elif type2_dist == 'beta_mean_std':\n        # Canonical \"precision\" parameterization of the beta distribution, where precision = 1 / noise**2, i.e.\n        # inverse variance.\n        # type2_center = mean, type2_noise = SD\n\n        type2_center = np.clip(type2_center, 1e-12, 1 - 1e-12)          # avoid a=0 or b=0\n        type2_noise = np.maximum(type2_noise, 1e-12)\n\n        phi = type2_center * (1 - type2_center) / (type2_noise ** 2) - 1\n        phi = np.maximum(phi, 1e-12)\n\n        a = type2_center * phi\n        b = (1 - type2_center) * phi\n\n        dist = beta(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'beta_mode_std':\n        # type2_center = mode, type2_noise = SD\n\n        type2_center = np.clip(type2_center, 1e-8, 1 - 1e-8)\n\n        var_target = np.maximum(type2_noise ** 2, 1e-8)\n        # Initial closed-form approximation\n        kappa = type2_center * (1 - type2_center) / var_target\n        # One-step variance correction\n        num = type2_center * (1 - type2_center) * kappa ** 2 + kappa + 1\n        den = (kappa + 2)**2 * (kappa + 3)\n        var_actual = num / den\n        kappa *= var_actual / var_target\n\n        a = type2_center * kappa + 1\n        b = (1 - type2_center) * kappa + 1\n        dist = beta(loc=0, a=a, b=b, scale=1)\n\n    elif type2_dist == 'beta_mode':\n        # type2_center = mode, type2_noise != SD\n        type2_center = np.maximum(1e-5, np.minimum(1 - 1e-5, type2_center))\n        a = 1 + (1 - type2_center) * type2_center ** 2 / type2_noise ** 2\n        b = (1 / type2_center - 1) * a - 1 / type2_center + 2\n        dist = beta(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'betaprime_mean_std':\n        # type2_center = mean, type2_noise = SD\n        b = 2 + (type2_center * (type2_center + 1)) / type2_noise ** 2\n        a = type2_center * (b - 1)\n        dist = betaprime(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'gamma_mode_std':\n        # type2_center = mode, type2_noise = SD\n        u = (type2_center + np.sqrt(type2_center * type2_center + 4 * type2_noise ** 2)) / (2 * type2_noise)\n        dist = gamma(loc=0, a=u**2, scale=type2_noise / u)\n    elif type2_dist == 'gamma_mean_std':\n        # type2_center = mean, type2_noise = SD\n        dist = gamma(loc=0, a=(type2_center / type2_noise) ** 2, scale=(type2_noise ** 2) / type2_center)\n    elif type2_dist == 'gamma_mean':\n        # type2_center = mean, type2_noise != SD\n        dist = gamma(loc=0, a=type2_center / type2_noise, scale=type2_noise)\n    elif type2_dist == 'gamma_mean_cv':\n        # Corresponds to the CASSANDRE setup, i.e. standard multiplicative noise\n        # type2_center = center, type2_noise = CV (relative noise)\n        a = 1 / (type2_noise ** 2)\n        dist = gamma(loc=0, a=a, scale=type2_center / a)\n    elif type2_dist == 'gamma_mode':\n        # type2_center = mode; type2_noise != SD\n        dist = gamma(loc=0, a=(type2_center / type2_noise) + 1, scale=type2_noise)\n    elif type2_dist.startswith('truncated_'):\n        if type2_noise_type == 'noisy_report':\n            if type2_dist == 'truncated_norm_mode_std':\n                # type2_center = mode, type2_noise = SD of the truncated distribution\n                sigma = _params_truncnorm_mode_std(low=0, high=1, x=type2_center, noise=type2_noise)\n                dist = truncnorm(a=-type2_center / sigma, b=(1 - type2_center) / sigma, loc=type2_center, scale=sigma)\n            elif type2_dist == 'truncated_norm_mode':\n                # type2_center = mode, type2_noise = SD of the *un*truncated distribution\n                dist = truncnorm(a=-type2_center / type2_noise, b=(1 - type2_center) / type2_noise, loc=type2_center, scale=type2_noise)\n            elif type2_dist == 'truncated_gumbel_mode_std':\n                # type2_center = mode, type2_noise = SD of the truncated distribution\n                dist = TruncatedGumbelModeSD(mode=type2_center, noise=type2_noise, upper=1, n_newton=3)\n            elif type2_dist == 'truncated_gumbel_mode':\n                # type2_center = mode, type2_noise = SD of the *un*truncated distribution\n                dist = TruncatedGumbelMode(mode=type2_center, noise=type2_noise * np.sqrt(6) / np.pi, upper=1)\n            elif type2_dist == 'truncated_lognorm_mode_std':\n                # type2_center = mode, type2_noise = SD\n                dist = TruncatedLognormalModeSD(mode=type2_center, noise=type2_noise, upper=1, n_newton=5)\n            elif type2_dist == 'truncated_lognorm':\n                # type2_center = median of the untruncated lognormal, type2_noise = SD in log space\n                dist = TruncatedLognormal(median_untrunc=type2_center, noise=type2_noise, upper=1)\n            elif type2_dist == 'truncated_lognorm_mode':\n                # type2_center = mode, type2_noise = SD in log space\n                if type2_noise &gt; 25:\n                    # warnings.warn(f'Type 2 noise for a {type2_dist} distribution is too high. Capping at 25')\n                    type2_noise = 25\n                dist = TruncatedLognormalMode(mode=type2_center, noise=type2_noise, b=1)\n            elif type2_dist == 'truncated_lognorm_mean':\n                # type2_center = mean, type2_noise = SD in log space\n                dist = TruncatedLognormalMean(mean=type2_center, noise=type2_noise, upper=1)\n        elif type2_noise_type in ('noisy_readout', 'noisy_temperature'):\n            if type2_dist == 'truncated_norm_mode_std':\n                # type2_center = mode, type2_noise = SD\n                sigma = _params_truncnorm_mode_std(low=0, high=np.inf, x=type2_center, noise=type2_noise)\n                dist = truncnorm(a=-type2_center / sigma, b=np.inf, loc=type2_center, scale=sigma)\n            elif type2_dist == 'truncated_norm_mode':\n                # type2_center = mode, type2_noise != SD\n                dist = truncnorm(a=-type2_center / type2_noise, b=np.inf, loc=type2_center, scale=type2_noise)\n            elif type2_dist == 'truncated_gumbel_mode_std':\n                # type2_center = mode, type2_noise = SD\n                dist = TruncatedGumbelModeSD(mode=type2_center, noise=type2_noise, upper=np.inf, n_newton=3)\n            elif type2_dist == 'truncated_gumbel_mode':\n                # type2_center = mode, type2_noise != SD\n                dist = TruncatedGumbelMode(mode=type2_center, noise=type2_noise * np.sqrt(6) / np.pi, upper=np.inf)\n\n        else:\n            raise ValueError(f\"'{type2_noise_type}' is an unknown type 2 noise type\")\n\n    return dist  # noqa\n</code></pre>"},{"location":"plot/#remeta.plot.linear","title":"linear","text":"<pre><code>linear(x, thresh, bias)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def linear(x, thresh, bias):\n    y = (np.abs(x) &gt; thresh) * (x - np.sign(x) * thresh) + bias\n    return y\n</code></pre>"},{"location":"plot/#remeta.plot.logistic","title":"logistic","text":"<pre><code>logistic(x, sigma, thresh, bias)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def logistic(x, sigma, thresh, bias):\n    beta = np.pi / (np.sqrt(3) * sigma)\n    return \\\n        (np.abs(x) &gt;= thresh) * (\n                1 / (1 + np.exp(-beta * (x + bias)))) + \\\n        (np.abs(x) &lt; thresh) * (1 / (1 + np.exp(-beta * bias)))\n</code></pre>"},{"location":"plot/#remeta.plot.logistic_old","title":"logistic_old","text":"<pre><code>logistic_old(x, sigma, thresh, bias)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def logistic_old(x, sigma, thresh, bias):\n    beta = np.pi / (np.sqrt(3) * sigma)\n    return \\\n        (np.abs(x) &gt;= thresh) * (\n                1 / (1 + np.exp(-beta * (x + bias - np.sign(x) * thresh)))) + \\\n        (np.abs(x) &lt; thresh) * (1 / (1 + np.exp(beta * bias)))\n</code></pre>"},{"location":"plot/#remeta.plot.plot_confidence","title":"plot_confidence","text":"<pre><code>plot_confidence(stimuli_or_data_object, confidence=None)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def plot_confidence(stimuli_or_data_object, confidence=None):\n    if confidence is None:\n        stimuli, confidence = stimuli_or_data_object.stimuli, stimuli_or_data_object.confidence\n    else:\n        stimuli = stimuli_or_data_object\n    ax = plt.gca()\n\n    for v in sorted(np.unique(stimuli)):\n        plt.errorbar(v, np.mean(confidence[stimuli == v]), yerr=sem(confidence[stimuli == v]), marker='o', markersize=5,\n                     mew=1, mec='k', color='None', ecolor='k', mfc=color_data, clip_on=False, elinewidth=1.5,\n                     capsize=5)\n    plt.plot([0, 0], [0, 1], 'k-', lw=0.5)\n    plt.ylim(0, 1)\n    plt.xlabel('Stimulus ($x$)')\n    plt.ylabel('Confidence ($c$)')\n    set_fontsize(label=13, tick=11)\n</code></pre>"},{"location":"plot/#remeta.plot.plot_confidence_dist","title":"plot_confidence_dist","text":"<pre><code>plot_confidence_dist(cfg, stimuli, confidence, params, nsamples_gen=1000, plot_likelihood=True, var_likelihood_grid=None, y_decval_grid=None, likelihood_weighting=None, dv_range=(45, 50, 55), nsamples_dist=10000, bw=0.03, figure_paper=False)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def plot_confidence_dist(cfg, stimuli, confidence, params, nsamples_gen=1000,\n                         plot_likelihood=True, var_likelihood_grid=None, y_decval_grid=None,\n                         likelihood_weighting=None, dv_range=(45, 50, 55), nsamples_dist=10000, bw=0.03,\n                         figure_paper=False):\n    generative = simu_data(params, nsamples=len(stimuli), nsubjects=nsamples_gen, cfg=cfg, stimuli_external=stimuli,\n                           verbosity=False)\n\n    nbins = 20\n    levels = np.unique(stimuli)\n    counts, counts_gen, bins = [[] for _ in range(2)], [[] for _ in range(2)], [[] for _ in range(2)]\n    for k in range(2):\n        levels_ = (levels[levels &lt; 0], levels[levels &gt; 0])[k]\n        for i, v in enumerate(levels_):\n            hist = np.histogram(confidence[stimuli == v], density=True, bins=nbins)\n            counts[k] += [hist[0]]\n            bins[k] += [hist[1]]\n            counts_gen[k] += [np.histogram(generative.c_conf[np.tile(stimuli, (nsamples_gen, 1)) == v],\n                                           density=True, bins=bins[k][i])[0] / (len(bins[k][i]) - 1)]\n    counts = [np.array(count) / np.max([np.max(c) for c in counts]) for count in counts]\n    counts_gen = [np.array(count) / np.max([np.max(c) for c in counts_gen]) for count in counts_gen]\n\n    ax = plt.gca()\n\n    dist_labels = [r'for $y_i^*$ $\u2212$ 0.5 SD', r'for $y_i^*$', r'for $y_i^*$ $+$ 0.5 SD']\n    for k in range(2):\n\n        levels_ = (levels[levels &lt; 0], levels[levels &gt; 0])[k]\n\n        if plot_likelihood:\n            confp_means = [[np.nanmean(var_likelihood_grid[stimuli == v, z]) for z in dv_range] for v in levels_]\n            weighting_p = np.array([[np.nanmean(likelihood_weighting[stimuli == v, z]) for z in dv_range] for v in\n                                    levels_])\n            weighting_p /= np.max(weighting_p)\n\n        for i, v in enumerate(levels_):\n\n            plt.barh(y=bins[k][i][:-1] + np.diff(bins[k][i]) / 2, width=((1, -1)[k]) * 0.26 * counts[k][i],\n                     height=1 / nbins, left=0.005 + v, color=color_data, linewidth=0, alpha=1, zorder=10,\n                     label='Data: histogram' if ((k == 0) &amp; (i == 0)) else None)\n\n            plt.plot(v + (1, -1)[k] * 0.26 * counts_gen[k][i], bins[k][i][:-1] + (bins[k][i][1] - bins[k][i][0]) / 2,\n                     color=color_generative_type2, zorder=11, lw=2,\n                     label='Generative model' if ((k == 0) &amp; (i == 0)) else None)\n\n            if plot_likelihood:\n                for j, dv in enumerate(dv_range):\n                    x = np.linspace(0, 1, 1000)\n                    if cfg.type2_noise_type == 'noisy_report':\n                        likelihood = get_likelihood(x, cfg.type2_noise_dist,\n                                                    np.maximum(1e-3, confp_means[i][j]),  # noqa\n                                                    params['type2_noise'], logarithm=False)\n                    else:\n                        dist = get_type2_dist(cfg.type2_noise_dist, np.maximum(1e-3, confp_means[i][j]), params['type2_noise'])\n                        z1_type1_evidence = dist.rvs(nsamples_dist)\n                        if 'censored_' in cfg.type2_noise_dist:\n                            z1_type1_evidence[z1_type1_evidence &lt; 0] = 0\n                        c_conf = type1_evidence_to_confidence(\n                            z1_type1_evidence, cfg.type1_evidence_to_confidence, x_stim=z1_type1_evidence,\n                            type1_noise_signal_dependency=cfg.type1_noise_transform,\n                            **params\n                        )\n                        likelihood = gaussian_kde(c_conf, bw_method=bw).evaluate(x)\n                    likelihood -= likelihood.min()\n                    likelihood_max = likelihood.max()\n                    likelihood_norm = likelihood / likelihood_max if likelihood_max &gt; 0 else np.zeros(likelihood.shape)\n                    likelihood_norm[likelihood_norm &lt; 0.05] = np.nan\n                    correct = np.sign(y_decval_grid[stimuli == v, dv][0]) == (-1, 1)[k]\n                    color_shade = [[0.175], [0], [0.175]][j]\n                    plt.plot(v + (weighting_p[i][j] * 0.26 * likelihood_norm + 0.005) * ((1, -1)[k]),  # noqa\n                             x, color=(color_model_wrong, color_model)[int(correct)] + color_shade,\n                             zorder=25, lw=2.5, dashes=[(2, 1), (None, None), (None, None)][j],\n                             label=(None, dist_labels[j])[int((k == 1) &amp; (i == 1))])\n\n    plt.xlim((-1.05 * np.abs(levels).max(), 1.05 * np.abs(levels).max()))\n    ylim = (-0.01, 1.01)\n    plt.plot([0, 0], ylim, 'k-', lw=0.5)\n    plt.ylim(ylim)\n\n    plt.xlabel('Stimulus ($x$)')\n    plt.ylabel('c_conf')\n    ax.xaxis.set_major_formatter(FormatStrFormatter('%.2g'))\n\n    if plot_likelihood:\n        handles, labels = plt.gca().get_legend_handles_labels()\n        handles += ['', 'Likelihood']\n        labels += ['', '']\n        if figure_paper:\n            labels += [r'for $y_i^*$ $+$ ' + '0.5 SD\\n(incorrect choice)']\n            handles += [Line2D([0], [0], color=color_model_wrong + 0.175,\n                               **{k: getattr(handles[3], f'_{k}') for k in ('linestyle', 'linewidth')})]\n            order = [4, 0, 5, 6, 2, 1, 3, 7]\n        else:\n            order = [4, 0, 5, 6, 2, 1, 3]\n        plt.legend([handles[i] for i in order], [labels[i] for i in order],\n                   bbox_to_anchor=(1.02, 1.1 if figure_paper else 1), loc=\"upper left\", fontsize=9,\n                   handler_map={str: LegendTitle()})\n    else:\n        plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=9)\n    ax.yaxis.grid('on', color=[0.9, 0.9, 0.9], zorder=-10)\n\n    set_fontsize(label=13, tick=11)\n</code></pre>"},{"location":"plot/#remeta.plot.plot_evidence_versus_confidence","title":"plot_evidence_versus_confidence","text":"<pre><code>plot_evidence_versus_confidence(stimuli, confidence, y_decval, params, cfg=None, type2_noise_type=None, type2_noise_dist=None, type1_noise_signal_dependency='none', plot_data=True, plot_generative_data=True, plot_likelihood=False, plot_bias_free=False, display_parameters=True, var_likelihood=None, y_decval_range=(45, 50, 55), nsamples_gen=1000, nsamples_dist=100000, bw=0.03, color_linkfunction=(0.55, 0.55, 0.69), label_linkfunction='Link function', figure_paper=False)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def plot_evidence_versus_confidence(stimuli, confidence, y_decval, params, cfg=None,\n                                    type2_noise_type=None, type2_noise_dist=None,\n                                    type1_noise_signal_dependency='none',\n                                    plot_data=True, plot_generative_data=True, plot_likelihood=False,\n                                    plot_bias_free=False, display_parameters=True,\n                                    var_likelihood=None, y_decval_range=(45, 50, 55),\n                                    nsamples_gen=1000, nsamples_dist=100000, bw=0.03, color_linkfunction=(0.55, 0.55, 0.69),\n                                    label_linkfunction='Link function',\n                                    figure_paper=False):\n\n    params = params.copy()\n\n    if cfg is not None:\n        type2_noise_dist = cfg.type2_noise_dist\n        type2_noise_type = cfg.type2_noise_type\n        type1_noise_signal_dependency = cfg.type1_noise_signal_dependency\n    else:\n        cfg = Configuration()\n        # We disable parameters that are not contained in params\n        for k, v in cfg.__dict__.items():\n            if k.startswith('enable_') and (v &gt; 0) and not ((k.replace('enable_type1_param', 'type1') in params) or\n                    (k.replace('enable_type2_param', 'type2') in params)):\n                setattr(cfg, k, 0)\n\n\n    generative = simu_data(params, nsamples=len(stimuli), nsubjects=nsamples_gen, cfg=cfg, stimuli_external=stimuli,\n                           verbosity=False, squeeze=True)\n\n    ax = plt.gca()\n    vals_decval = np.unique(y_decval)\n    vals_dv_gen = np.unique(generative.y_decval_latent)\n    for k in range(2):\n\n        vals_dv_ = vals_decval[vals_decval &lt; 0] if k == 0 else vals_decval[vals_decval &gt; 0]\n        vals_dv_gen_ = vals_dv_gen[vals_dv_gen &lt; 0] if k == 0 else vals_dv_gen[vals_dv_gen &gt; 0]\n\n        conf_data_means = [np.mean(confidence[y_decval == v]) for v in vals_dv_]\n        conf_data_std_neg = [np.std(confidence[(y_decval == v) &amp; (confidence &lt; conf_data_means[i])])\n                             for i, v in enumerate(vals_dv_)]\n        conf_data_std_pos = [np.std(confidence[(y_decval == v) &amp; (confidence &gt;= conf_data_means[i])])\n                             for i, v in enumerate(vals_dv_)]\n\n        conf_gen_means = [np.mean(generative.confidence[generative.y_decval_latent == v]) for v in vals_dv_gen_]\n        conf_gen_std_neg = [np.std(\n            generative.confidence[(generative.y_decval_latent == v) &amp; (generative.confidence &lt; conf_gen_means[i])])\n            for i, v in enumerate(vals_dv_gen_)]\n        conf_gen_std_pos = [np.std(\n            generative.confidence[(generative.y_decval_latent == v) &amp; (generative.confidence &gt; conf_gen_means[i])])\n            for i, v in enumerate(vals_dv_gen_)]\n\n        if plot_data:\n            _, cap, barlinecols = plt.errorbar(\n                vals_dv_-0.015, conf_data_means, yerr=[conf_data_std_neg, conf_data_std_pos],\n                label='Data: Mean (SD)' if k == 0 else None, marker='o', markersize=7, mew=1, mec='k', color='None',\n                ecolor='k', mfc=color_data, clip_on=False, zorder=35, elinewidth=1.5, capsize=5\n            )\n            [cap[i].set_markeredgewidth(1.5) for i in range(len(cap))]\n            [cap[i].set_clip_on(False) for i in range(len(cap))]\n            barlinecols[0].set_clip_on(False)\n\n        if plot_generative_data:\n            _, cap, barlinecols = plt.errorbar(\n                vals_dv_gen_+0.015, conf_gen_means, yerr=[conf_gen_std_neg, conf_gen_std_pos],\n                label='Generative model' if k == 0 else None, marker='o', markersize=7, mew=1, mec='k',\n                color='None', ecolor=color_generative_type2, mfc=color_generative_type2, clip_on=False, zorder=35,\n                elinewidth=1.5, capsize=5\n            )\n            [cap[i].set_markeredgewidth(1.5) for i in range(len(cap))]\n            [cap[i].set_clip_on(False) for i in range(len(cap))]\n            barlinecols[0].set_clip_on(False)\n\n        if plot_likelihood:\n            var_likelihood_means = [np.nanmean(var_likelihood[y_decval == v, y_decval_range[1]]) for v in\n                                    vals_dv_]\n\n            for i, v in enumerate(vals_dv_):\n\n                x = np.linspace(0, 1, 1000)\n\n                if type2_noise_type == 'noisy_report':\n                    likelihood = get_likelihood(x, type2_noise_dist, np.maximum(1e-3, var_likelihood_means[i]),\n                                                params['type2_noise'], logarithm=False)\n                else:\n                    dist = get_type2_dist(type2_noise_dist, np.maximum(1e-3, var_likelihood_means[i]), params['type2_noise'])\n                    z1_type1_evidence_generative = dist.rvs(nsamples_dist)\n                    c_conf_generative = type1_evidence_to_confidence(\n                        z1_type1_evidence_generative, x_stim=stimuli, y_decval=z1_type1_evidence_generative,\n                        type1_noise_signal_dependency=type1_noise_signal_dependency, **params\n                    )\n                    likelihood = gaussian_kde(c_conf_generative, bw_method=bw).evaluate(x)\n                likelihood -= likelihood.min()\n                like_max = likelihood.max()\n                likelihood_norm = likelihood / like_max if like_max &gt; 0 else np.zeros(likelihood.shape)\n                plt.plot(v + (0.26 * likelihood_norm + 0.005) * ((1, -1)[k]), x, color=color_model, zorder=25, lw=2.5,\n                         label=(None, r'Likelihood for $y_i^*$')[int((k == 0) &amp; (i == 0))])\n\n                ax.annotate(rf\"$\\mathbf{{y}}_{{{('', '+')[k]}{(i - len(vals_dv_), i + 1)[k]}}}^*$\",\n                            xy=(v, 1.008), xycoords='data', xytext=(v, 1.09), color=color_model, weight='bold',\n                            fontsize=9, ha='center', bbox=dict(pad=0, facecolor='w', lw=0),\n                            arrowprops=dict(facecolor=color_model, headwidth=7, lw=0, headlength=3, width=2))\n\n        xrange = np.arange(-5, 0.001, 0.001) if k == 0 else np.arange(0, 5.001, 0.001)\n\n        type2_evidence_bias_mult = params['type2_evidence_bias_mult'] if 'type2_evidence_bias_mult' in params else 1\n        type2_evidence_bias_add = params['type2_evidence_bias_add'] if 'type2_evidence_bias_add' in params else 0\n        conf_model = type1_evidence_to_confidence(\n            type2_evidence_bias_mult * np.abs(xrange) + type2_evidence_bias_add,\n            # np.abs(xrange),\n            x_stim=xrange, y_decval=xrange,\n            type1_noise_signal_dependency=type1_noise_signal_dependency, **params\n        )\n        if 'type2_criteria' in params:\n            criteria = [np.sum(params['type2_criteria'][:i+1]) for i in range(len(params['type2_criteria']))]\n            plt.plot(xrange, (np.digitize(conf_model, criteria) + 0.5) / (len(criteria) + 1),\n                     color=color_linkfunction, lw=3.5, zorder=5, alpha=0.9,\n                     label=label_linkfunction if k == 0 else None)\n        else:\n            plt.plot(xrange, conf_model, color=color_linkfunction, lw=3.5, zorder=5, alpha=0.9,\n                     label=label_linkfunction if k == 0 else None)\n        if plot_bias_free:\n            conf_model_bf = type1_evidence_to_confidence(\n                np.abs(xrange), x_stim=xrange, y_decval=xrange,\n                type1_noise_signal_dependency=type1_noise_signal_dependency, **params\n            )\n            if 'type2_criteria' in params:\n                criteria_bf = np.arange(1/(len(params['type2_criteria'])+1), 1, 1/(len(params['type2_criteria'])+1))\n                conf_model_bf = (np.digitize(conf_model_bf, criteria_bf) + 0.5) / (len(criteria) + 1)\n            plt.plot(xrange, conf_model_bf, color='green', lw=3.5, zorder=5, alpha=0.9,\n                     label='Link function (bias-free)' if k == 0 else None)\n\n    ylim = (-0.01, 1.025)\n    plt.plot([0, 0], ylim, 'k-', lw=0.5)\n    plt.ylim(ylim)\n    plt.xlim((-1.1 * np.abs(vals_decval).max(), 1.1 * np.abs(vals_decval).max()))\n    plt.xlabel(r'Type 1 decision value ($y$)')\n    plt.ylabel('Confidence ($c$)')\n    handles, labels = plt.gca().get_legend_handles_labels()\n    if plot_likelihood:\n        order = [2, 1, 0, 3]\n        plt.legend([handles[i] for i in order], [labels[i] for i in order], bbox_to_anchor=(1.02, 1), loc=\"upper left\",\n                   fontsize=9)\n    else:\n        plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=9)\n    ax.yaxis.grid('on', color=[0.9, 0.9, 0.9])\n\n    if display_parameters:\n        an_params = [p for p in params if 'type2_' in p]\n        an_type2 = []\n        for p in an_params:\n            if hasattr(params[p], '__len__'):\n                an_type2 += [f\"${symbols[p][1:-1]}=${[float(f'{v:.3f}') for v in params[p]]}\"]\n            else:\n                an_type2 += [f\"${symbols[p][1:-1]}={params[p]:{'.0f' if params[p] == 0 else ('.3f', '.2f')[figure_paper]}}$\"]  # noqa\n        plt.text(1.045, -0.2, r'Estimated parameters:' + '\\n' + '\\n'.join(an_type2), transform=plt.gca().transAxes,\n                 bbox=dict(fc=[1, 1, 1], ec=[0.5, 0.5, 0.5], lw=1, pad=5), fontsize=9)\n\n    set_fontsize(label=13, tick=11)\n</code></pre>"},{"location":"plot/#remeta.plot.plot_psychometric","title":"plot_psychometric","text":"<pre><code>plot_psychometric(stimuli, choices, params, cfg=None, figure_paper=False, fit_only=False, highlight_fit=False)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def plot_psychometric(stimuli, choices, params, cfg=None, figure_paper=False,\n                      fit_only=False, highlight_fit=False):\n\n    params_type1 = {k: v for k, v in params.items() if k.startswith('type1')}\n\n    type1_noise = _check_param(params_type1['type1_noise'])\n    if (cfg is None and 'type1_thresh' in params_type1) or (cfg is not None and cfg.enable_type1_param_thresh):\n        type1_thresh = _check_param(params_type1['type1_thresh'])\n    else:\n        type1_thresh = [0, 0]\n    if (cfg is None and 'type1_bias' in params_type1) or (cfg is not None and cfg.enable_type1_param_bias):\n        type1_bias = _check_param(params_type1['type1_bias'])\n    else:\n        type1_bias = [0, 0]\n\n    xrange_neg = np.arange(-1, 0.001, 0.001)\n    xrange_pos = np.arange(0.001, 1.001, 0.001)\n\n    posterior_neg = logistic(xrange_neg, type1_noise[0], type1_thresh[0], type1_bias[0])\n    posterior_pos = logistic(xrange_pos, type1_noise[1], type1_thresh[1], type1_bias[1])\n\n    ax = plt.gca()\n\n    if not fit_only:\n        stimulus_ids = (stimuli &gt; 0).astype(int)\n        levels = np.unique(stimuli)\n        choiceprob_neg = np.array([np.mean(choices[(stimuli == v) &amp; (stimulus_ids == 0)] ==\n                                           stimulus_ids[(stimuli == v) &amp; (stimulus_ids == 0)])\n                                   for v in levels[levels &lt; 0]])\n        choiceprob_pos = np.array([np.mean(choices[(stimuli == v) &amp; (stimulus_ids == 1)] ==\n                                           stimulus_ids[(stimuli == v) &amp; (stimulus_ids == 1)])\n                                   for v in levels[levels &gt; 0]])\n        plt.plot(levels[levels &lt; 0], 1 - choiceprob_neg, 'o', markersize=6.5, mew=1, mec='k', label='Data: $S^-$ Mean',\n                 color=color_data, clip_on=False, zorder=11, alpha=(1, 0.2)[highlight_fit])\n        plt.plot(levels[levels &gt; 0], choiceprob_pos, 's', markersize=5.5, mew=1, mec='k', label='Data: $S^+$ Mean',\n                 color=color_data, clip_on=False, zorder=11, alpha=(1, 0.2)[highlight_fit])\n\n    plt.plot(xrange_neg, posterior_neg, '-', lw=(2, 5)[highlight_fit], color=color_logistic, clip_on=False,\n             zorder=(10, 12)[highlight_fit], label=f'Model fit')\n    plt.plot(xrange_pos, posterior_pos, '-', lw=(2, 5)[highlight_fit], color=color_logistic, clip_on=False,\n             zorder=(10, 12)[highlight_fit])\n\n    plt.plot([-1, 1], [0.5, 0.5], 'k-', lw=0.5)\n    plt.plot([0, 0], [-0.02, 1.02], 'k-', lw=0.5)\n\n    ax.yaxis.grid('on', color=[0.9, 0.9, 0.9])\n    plt.xlim((-1, 1))\n    plt.ylim((0, 1))\n    plt.xlabel('Stimulus ($x$)')\n    plt.ylabel('Choice probability $S^+$')\n    ax.xaxis.set_major_formatter(FormatStrFormatter('%.2g'))\n    leg = plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=9, handlelength=0.5)\n    for lh in (leg.legendHandles if hasattr(leg, 'legendHandles') else leg.legend_handles):\n        if hasattr(lh, '_legmarker'):\n            lh._legmarker.set_alpha(1)  # noqa\n        elif hasattr(lh, 'legmarker'):\n            lh.legmarker.set_alpha(1)  # noqa\n    anot_type1 = []\n    for i, (k, v) in enumerate(params_type1.items()):\n        if (cfg is None and k in params_type1) or (cfg is not None and getattr(cfg, f\"enable_{k.replace('type1', 'type1_param')}\")):\n            if hasattr(v, '__len__'):\n                val = ', '.join([f\"{p:{'.0f' if p == 0 else '.3g'}}\" for p in v])\n                anot_type1 += [f\"${symbols[k][1:-1]}=\" + f\"[{val}]$\"]\n            else:\n                anot_type1 += [f\"${symbols[k][1:-1]}={v:{'.0f' if v == 0 else '.3g'}}$\"]\n    plt.text(1.045, -0.1, r'Estimated parameters:' + '\\n' + '\\n'.join(anot_type1), transform=plt.gca().transAxes,\n             bbox=dict(fc=[1, 1, 1], ec=[0.5, 0.5, 0.5], lw=1, pad=5), fontsize=9)\n    set_fontsize(label=13, tick=11)\n</code></pre>"},{"location":"plot/#remeta.plot.plot_psychometric_sim","title":"plot_psychometric_sim","text":"<pre><code>plot_psychometric_sim(data, figure_paper=False)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def plot_psychometric_sim(data, figure_paper=False):\n    plot_psychometric(data.choices, data.stimuli, data.params_type1, cfg=data.cfg, figure_paper=figure_paper)\n</code></pre>"},{"location":"plot/#remeta.plot.plot_type1_type2","title":"plot_type1_type2","text":"<pre><code>plot_type1_type2(m, plot_subject_id=False, nsamples_gen=1000, figure_paper=False)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def plot_type1_type2(m, plot_subject_id=False, nsamples_gen=1000, figure_paper=False):\n\n    if hasattr(m, 'model'):\n        # In case m is model fit to data\n        simulation = False\n        type1_params = m.model.params_type1\n        type2_params = m.model.params_type2\n        data = m.data.data\n        stimuli_norm = data.stimuli_norm\n        choices = data.d_dec\n        c_conf = data.c_conf\n        var_likelihood = dict(noisy_report=m.model.extended.c_conf,\n                              noisy_readout=m.model.extended.z1_type1_evidence_grid)[m.cfg.type2_noise_type]\n        likelihood_weighting = m.model.extended.y_decval_pmf_grid\n        y_decval = m.model.extended.y_decval_grid\n        y_decval_mode = m.model.extended.y_decval_grid\n    else:\n        # In case m is a simulation\n        simulation = True\n        type1_params = m.params_type1\n        type2_params = m.params_type2\n        choices = m.d_dec\n        stimuli_norm = m.x_stim\n        c_conf = m.c_conf\n        y_decval_mode = m.y_decval.squeeze()\n        y_decval = None\n        likelihood_weighting = None\n        var_likelihood = None\n\n    if 'type1_evidence_bias_mult' not in type2_params:\n        type2_params['type1_evidence_bias_mult'] = 1\n    if 'type1_thresh' not in type1_params:\n        type1_params['type1_thresh'] = 0\n    if 'type1_bias' not in type1_params:\n        type1_params['type1_bias'] = 0\n\n    params = {**type1_params, **type2_params}\n\n    fig = plt.figure(figsize=(8, 7))\n    if plot_subject_id and hasattr(m, 'subject_id') and (m.subject_id is not None):\n        fig.suptitle(f'Subject {m.subject_id}', fontsize=16)\n\n    plt.subplot(3, 1, 1)\n    ax1 = plot_psychometric(choices, stimuli_norm, type1_params, cfg=m.cfg, figure_paper=figure_paper)\n    ax1.yaxis.set_label_coords(-0.1, 0.43)\n    plt.text(-0.15, 1.01, 'A', transform=ax1.transAxes, fontsize=19)\n\n    plt.subplot(3, 1, 2)\n    ax2 = plot_evidence_versus_confidence(\n        stimuli_norm, c_conf, y_decval_mode, params, cfg=m.cfg,\n        plot_likelihood=not simulation, var_likelihood=var_likelihood,\n        y_decval_range=(0,) if simulation else (45, 50, 55),\n        figure_paper=figure_paper\n    )\n    plt.text(-0.15, 1.01, 'B', transform=ax2.transAxes, fontsize=19)\n\n    plt.subplot(3, 1, 3)\n    ax3 = plot_confidence_dist(\n        m.cfg, stimuli_norm, c_conf, params, nsamples_gen,\n        plot_likelihood=not simulation, var_likelihood_grid=var_likelihood,\n        y_decval_grid=y_decval, likelihood_weighting=likelihood_weighting, dv_range=(0,) if simulation else (45, 50, 55),\n        figure_paper=figure_paper\n    )\n    plt.text(-0.15, 1.01, 'C', transform=ax3.transAxes, fontsize=19)\n\n    # hack to not cut the right edges in saved images\n    # if figure_paper:\n    #     plt.text(1.29, 1.01, 'C', transform=plt.gca().transAxes, color='r', fontsize=9)\n\n    set_fontsize(label=11, tick=10)\n    plt.subplots_adjust(hspace=0.5, top=0.96, right=0.7, left=0.1)\n    ax2.set_position([*(np.array(ax2.get_position())[0] + (0, -0.02)),\n                      ax2.get_position().width, ax2.get_position().height])\n    ax3.set_position([*(np.array(ax3.get_position())[0] + (0, -0.02)),\n                      ax3.get_position().width, ax3.get_position().height])\n</code></pre>"},{"location":"plot/#remeta.plot.plot_type2_condensed","title":"plot_type2_condensed","text":"<pre><code>plot_type2_condensed(ax, s, m, m2=None, nsamples_gen=1000)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def plot_type2_condensed(ax, s, m, m2=None, nsamples_gen=1000):\n    cfg = m.cfg\n\n    if hasattr(m, 'model'):\n        type1_params = m.model.params_type1\n        type2_params = m.model.params_type2\n        if '_criteria' in cfg.type1_evidence_to_confidence:\n            type2_params['type2_evidence_bias_mult'] = [v for k, v in m.model.params_type2.items() if\n                                                        'type2_criterion' in k]\n        data = m.data.data\n        stimuli_norm = data.stimuli_norm\n        c_conf = data.c_conf\n    else:\n        type1_params = m.params_type1\n        type2_params = m.params_type2\n        stimuli_norm = m.x_stim\n        c_conf = m.c_conf\n\n    simu = simu_data({**type1_params, **type2_params}, nsamples=len(stimuli_norm), nsubjects=nsamples_gen,\n                     cfg=cfg, stimuli_external=stimuli_norm, verbosity=False)\n\n    if 'type1_thresh' not in type1_params:\n        type1_params['type1_thresh'] = 0\n    if 'type1_bias' not in type1_params:\n        type1_params['type1_bias'] = 0\n\n    levels = np.unique(stimuli_norm)\n    nbins = 20\n\n    if m2 is not None:\n        cfg2 = m2.cfg\n        if hasattr(m2, 'model'):\n            type1_params2 = m2.model.params_type1\n            type2_params2 = m2.model.params_type2\n            if '_criteria' in cfg2.type1_evidence_to_confidence:\n                type2_params2['type2_evidence_bias_mult'] = [v for k, v in m2.model.params_type2.items() if\n                                                                     'type2_criterion' in k]\n        else:\n            type1_params2 = m2.params_type1\n            type2_params2 = m2.params_type2\n        simu2 = simu_data({**type1_params2, **type2_params2}, nsamples=len(stimuli_norm), nsubjects=nsamples_gen,\n                          cfg=cfg2, stimuli_external=stimuli_norm, verbosity=False)\n\n        if 'type1_thresh' not in type1_params2:\n            type1_params2['type1_thresh'] = 0\n        if 'type1_bias' not in type1_params2:\n            type1_params2['type1_bias'] = 0\n        counts_gen2 = [[] for _ in range(2)]\n\n    counts, counts_gen, bins = [[] for _ in range(2)], [[] for _ in range(2)], [[] for _ in range(2)]\n    for k in range(2):\n        levels_ = (levels[levels &lt; 0], levels[levels &gt; 0])[k]\n        for i, v in enumerate(levels_):\n            hist = np.histogram(c_conf[stimuli_norm == v], density=True, bins=nbins)\n            counts[k] += [hist[0]]\n            bins[k] += [hist[1]]\n            counts_gen[k] += [np.histogram(simu.c_conf[np.tile(stimuli_norm, (nsamples_gen, 1)) == v], density=True,\n                                           bins=bins[k][i])[0] / (len(bins[k][i]) - 1)]\n            if m2 is not None:\n                counts_gen2[k] += [np.histogram(simu2.c_conf[np.tile(stimuli_norm, (nsamples_gen, 1)) == v],  # noqa\n                                                density=True, bins=bins[k][i])[0] / (len(bins[k][i]) - 1)]\n    counts = np.array(counts) / np.max(counts)\n    counts_gen = np.array(counts_gen) / np.max(counts_gen)\n    if m2 is not None:\n        counts_gen2 = np.array(counts_gen2) / np.max(counts_gen2)\n    bins = np.array(bins)\n\n    for k in range(2):\n        levels_ = (levels[levels &lt; 0], levels[levels &gt; 0])[k]\n        for i, v in enumerate(levels_):\n            plt.barh(y=bins[k, i][:-1] + np.diff(bins[k, i]) / 2, width=((1, -1)[k]) * 0.3 * counts[k, i],\n                     height=1 / nbins, left=0.005 + v, color=color_data, linewidth=0, alpha=1, zorder=10,\n                     label='Data: histogram' if ((k == 0) &amp; (i == 0)) else None)\n            plt.plot(v + (1, -1)[k] * 0.3 * counts_gen[k, i], bins[k, i][:-1] + (bins[k, i][1] - bins[k, i][0]) / 2,\n                     color=0.85 * color_generative_type2, zorder=11, lw=1.5,\n                     label='Model: density' if ((k == 0) &amp; (i == 0)) else None)\n            if m2 is not None:\n                plt.plot(v + (1, -1)[k] * 0.3 * counts_gen2[k, i],  # noqa\n                         bins[k, i][:-1] + (bins[k, i][1] - bins[k, i][0]) / 2, '--', dashes=(3, 2.4),\n                         color=0.85 * color_generative_type2b,\n                         zorder=11, lw=1.5, label='Model: density' if ((k == 0) &amp; (i == 0)) else None)\n\n    plt.xlim((-1, 1))\n    ylim = (-0.01, 1.01)\n    plt.plot([0, 0], ylim, 'k-', lw=0.5)\n    plt.ylim(ylim)\n\n    if s == 17:\n        plt.xlabel('Stimulus ($x$)', fontsize=11)\n        ax.xaxis.set_label_coords(1.1, -0.18)\n    if s == 8:\n        plt.ylabel('c_conf', fontsize=11)\n    if s &lt; 16:\n        plt.xticks([])\n    if np.mod(s, 4) != 0:\n        plt.yticks([])\n    title = r\"$\\varphi_\\mathrm{m}$=\" + f\"${type2_params['type2_evidence_bias_mult']:.2f}$ \" + \\\n            r\"$\\sigma_\\mathrm{m}$=\" + f\"${type2_params['type2_noise']:.2f}$\"\n    if m2 is not None:\n        params_type2_ = type2_params2  # noqa\n        title2 = r\"$\\varphi_\\mathrm{m}$=\" + f\"${params_type2_['type2_evidence_bias_mult']:.2f}$ \" + \\\n                 r\"$\\sigma_\\mathrm{m}$=\" + f\"${params_type2_['type2_noise']:.2f}$\"\n        plt.text(0, 1.23, title, fontsize=8.5, color=np.array([165, 110, 0])/255, ha='center')\n        plt.text(0, 1.13, title2, fontsize=8.5, color=np.array([30, 98, 38])/255, ha='center')\n    else:\n        plt.title(title, fontsize=9, y=0.97)\n    plt.text(0, 0.8, f'{s + 1}', bbox=dict(fc=[0.8, 0.8, 0.8], ec=[0.5, 0.5, 0.5], lw=0.5, pad=2, alpha=0.8),\n             fontsize=10, ha='center')\n</code></pre>"},{"location":"plot/#remeta.plot.set_fontsize","title":"set_fontsize","text":"<pre><code>set_fontsize(label=None, xlabel=None, ylabel=None, tick=None, xtick=None, ytick=None, title=None)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def set_fontsize(label=None, xlabel=None, ylabel=None, tick=None, xtick=None, ytick=None, title=None):\n\n    fig = plt.gcf()\n\n    for ax in fig.axes:\n        if xlabel is not None:\n            ax.xaxis.label.set_size(xlabel)\n        elif label is not None:\n            ax.xaxis.label.set_size(label)\n        if ylabel is not None:\n            ax.yaxis.label.set_size(ylabel)\n        elif label is not None:\n            ax.yaxis.label.set_size(label)\n\n        if xtick is not None:\n            for ticklabel in (ax.get_xticklabels()):\n                ticklabel.set_fontsize(xtick)\n        elif tick is not None:\n            for ticklabel in (ax.get_xticklabels()):\n                ticklabel.set_fontsize(tick)\n        if ytick is not None:\n            for ticklabel in (ax.get_yticklabels()):\n                ticklabel.set_fontsize(ytick)\n        elif tick is not None:\n            for ticklabel in (ax.get_yticklabels()):\n                ticklabel.set_fontsize(tick)\n\n        if title is not None:\n            ax.title.set_fontsize(title)\n</code></pre>"},{"location":"plot/#remeta.plot.simu_data","title":"simu_data","text":"<pre><code>simu_data(params, nsubjects=1, nsamples=1000, cfg=None, stimuli_external=None, verbosity=True, stimuli_stepsize=0.02, squeeze=False, warn_in_case_of_nondivisible_stepsize=False, compute_stats=True, **kwargs)\n</code></pre> Source code in <code>remeta/gendata.py</code> <pre><code>def simu_data(params, nsubjects=1, nsamples=1000, cfg=None, stimuli_external=None, verbosity=True, stimuli_stepsize=0.02,\n              squeeze=False, warn_in_case_of_nondivisible_stepsize=False,\n              compute_stats=True, **kwargs):\n    params = params.copy()  # this variable can be modifed, thus better to make a copy\n    if cfg is None:\n        # Set configuration attributes that match keyword arguments\n        cfg_kwargs = {k: v for k, v in kwargs.items() if k in Configuration.__dict__}\n        cfg = Configuration(**cfg_kwargs)\n        for setting in cfg.__dict__:\n            if setting.startswith('enable_'):\n                if setting.split('enable_')[1].replace('_param_', '_') not in params:\n                    setattr(cfg, setting, 0)\n    # if not cfg.setup_called:\n    cfg.setup(generative_mode=True)\n\n    if cfg.type2_noise_dist is None:\n        cfg.type2_noise_dist = dict(noisy_report='truncated_norm_mode', noisy_readout='truncated_norm_mode', noisy_temperature='lognorm_mode')[cfg.type2_noise_type]\n\n    # Make sure no unwanted parameters have been passed\n    for p in ('thresh', 'bias', 'noise_heteroscedastic', 'nonlinear_encoding_gain', 'nonlinear_encoding_transition'):\n        if not getattr(cfg, f'enable_type1_param_{p}'):\n            params.pop(f'type1_{p}', None)\n    for p in ('evidence_bias_mult', 'criteria'):\n        if not getattr(cfg, f'enable_type2_param_{p}'):\n            params.pop(f'type2_{p}', None)\n\n    if stimuli_external is None:\n        x_stim = generate_stimuli(nsubjects, nsamples, stepsize=stimuli_stepsize,\n                                  warn_in_case_of_nondivisible_stepsize=warn_in_case_of_nondivisible_stepsize)\n    else:\n        x_stim = stimuli_external / np.max(np.abs(stimuli_external))\n        if stimuli_external.shape != (nsubjects, nsamples):\n            x_stim = np.tile(x_stim, (nsubjects, 1))\n    x_stim_category = (np.sign(x_stim) &gt; 0).astype(int)\n    y_decval_latent, y_decval, d_dec = simu_type1_responses(x_stim, params, cfg)\n\n    if not cfg.skip_type2:\n\n        z1_type1_evidence_latent = np.abs(y_decval_latent)\n        z1_type1_evidence_base = np.abs(y_decval)\n\n        if cfg.type2_noise_type == 'noisy_readout':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=z1_type1_evidence_base, type2_noise=params['type2_noise'],\n                                  type2_noise_type=cfg.type2_noise_type)\n\n            z1_type1_evidence = np.maximum(0, dist.rvs((nsubjects, nsamples)))\n        elif cfg.type2_noise_type == 'noisy_temperature':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=params['type1_noise'] * np.ones_like(z1_type1_evidence_base),\n                                  type2_noise=params['type2_noise'], type2_noise_type=cfg.type2_noise_type)\n            type1_noise_estimated = dist.rvs()\n            z1_type1_evidence = z1_type1_evidence_base\n        elif cfg.type2_noise_type == 'noisy_report':\n            z1_type1_evidence = z1_type1_evidence_base\n        else:\n            raise ValueError('Unknown type 2 noise type')\n\n        c_conf_latent = type1_evidence_to_confidence(\n            z1_type1_evidence=z1_type1_evidence_latent, y_decval=y_decval,\n            x_stim=x_stim, type1_noise_signal_dependency=cfg.type1_noise_signal_dependency,\n            **({**params, **dict(type1_noise=type1_noise_estimated)} if cfg.type2_noise_type == 'noisy_temperature' else params)\n        )\n\n        c_conf_base = type1_evidence_to_confidence(\n            z1_type1_evidence=z1_type1_evidence, y_decval=y_decval,\n            x_stim=x_stim, type1_noise_signal_dependency=cfg.type1_noise_signal_dependency,\n            **({**params, **dict(type1_noise=type1_noise_estimated)} if cfg.type2_noise_type == 'noisy_temperature' else params)\n        )\n\n        if cfg.type2_noise_type == 'noisy_report':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=c_conf_base, type2_noise=params['type2_noise'],\n                                  type2_noise_type=cfg.type2_noise_type)\n            c_conf = np.maximum(0, np.minimum(1, dist.rvs((nsubjects, nsamples))))\n        else:\n            c_conf = c_conf_base\n\n        if cfg.enable_type2_param_criteria:\n            if 'type2_criteria' in params:\n                sum_criteria = np.sum(params['type2_criteria'])\n                if sum_criteria &gt; 1.001:\n                    old_criteria = params['type2_criteria']\n                    params['type2_criteria'] = check_criteria_sum(params['type2_criteria'])\n                    warnings.warn(\n                       '\\nThe first entry of the criterion list is a criterion, whereas the subsequent entries encode\\n'\n                       'the gap to the respective previous criterion. Hence, the sum of all entries in the criterion\\n'\n                       f\"list must be smaller than 1, but sum([{', '.join([f'{c:.3f}' for c in old_criteria])}]) = {sum_criteria:.3f}).\"\n                       f\"Changing criteria to [{', '.join([f'{c:.3f}' for c in params['type2_criteria']])}].\", UserWarning)\n                first_criterion_and_gaps = params['type2_criteria']\n                criteria = [v if i == 0 else np.sum(first_criterion_and_gaps[:i+1]) for i, v in enumerate(first_criterion_and_gaps)]\n            else:\n                first_criterion_and_gaps = np.ones(cfg.n_discrete_confidence_levels - 1) / cfg.n_discrete_confidence_levels\n                criteria = [v if i == 0 else np.sum(first_criterion_and_gaps[:i+1]) for i, v in enumerate(first_criterion_and_gaps)]\n                warnings.warn(\n                    '\\nType 2 criteria enabled, but type2_criteria have not been specified. Using default values\\n'\n                    f\"of a Bayesian confidence observer for {cfg.n_discrete_confidence_levels} discrete ratings: [{', '.join([f'{v:.3g}' for v in first_criterion_and_gaps])}].\\n\"\n                    'Note that the first entry of the criterion list is a criterion, whereas the subsequent\\n'\n                    f'entries encode the gap to the respective previous criterion.\\n'\n                    f\"The final criteria are: [{', '.join([f'{v:.3g}' for v in criteria])}]\", UserWarning)\n\n            c_conf = (np.digitize(c_conf, criteria) + 0.5) / cfg.n_discrete_confidence_levels\n            c_conf_base = (np.digitize(c_conf_base, criteria) + 0.5) / cfg.n_discrete_confidence_levels\n\n    if squeeze:\n        x_stim_category = x_stim_category.squeeze()\n        x_stim = x_stim.squeeze()\n        d_dec = d_dec.squeeze()\n        y_decval_latent = y_decval_latent.squeeze()\n        y_decval = y_decval.squeeze()\n        if not cfg.skip_type2:\n            z1_type1_evidence_base = z1_type1_evidence_base.squeeze()  # noqa\n            z1_type1_evidence = z1_type1_evidence.squeeze()  # noqa\n            c_conf_base = c_conf_base.squeeze()  # noqa\n            c_conf = c_conf.squeeze()  # noqa\n\n    simargs = dict(\n        nsubjects=nsubjects, nsamples=nsamples, params=params, cfg=cfg,\n        x_stim_category=x_stim_category, x_stim=x_stim, d_dec=d_dec,\n        y_decval=y_decval, y_decval_latent=y_decval_latent\n    )\n    if not cfg.skip_type2:\n        simargs.update(\n            z1_type1_evidence_latent=z1_type1_evidence_latent, z1_type1_evidence_base=z1_type1_evidence_base, z1_type1_evidence=z1_type1_evidence,\n            c_conf_latent=c_conf_latent, c_conf_base=c_conf_base, c_conf=c_conf\n        )\n\n    if compute_stats:\n        accuracy = (x_stim_category == d_dec).astype(int)\n        type1_stats = dict(\n            accuracy=np.mean(accuracy),\n            d1 = norm.ppf(min(1 - 1e-3, max(1e-3, d_dec[x_stim_category == 1].mean()))) - \\\n                 norm.ppf(min(1 - 1e-3, max(1e-3, d_dec[x_stim_category == 0].mean().mean()))),\n            choice_bias=d_dec.mean(),\n        )\n        simargs.update(type1_stats=type1_stats)\n        if not cfg.skip_type2:\n            bounds = np.arange(0, 0.81, 0.2)\n            fit = type2_SDT_MLE(x_stim_category.flatten(), d_dec.flatten(), discretize_confidence_with_bounds(c_conf.flatten(), bounds), len(bounds))\n            type2_stats = dict(\n                confidence=c_conf.mean(),\n                auroc2=type2roc(accuracy.flatten(), c_conf.flatten()),\n                mratio=fit.M_ratio\n            )\n            simargs.update(type2_stats=type2_stats)\n            if 'type2_criteria' in params:\n                params_extra = dict(\n                    type2_criteria_absolute=[np.sum(params['type2_criteria'][:i + 1]) for i in range(len(params['type2_criteria']))],\n                    type2_criteria_bias=np.mean(params['type2_criteria']) * (len(params['type2_criteria']) + 1) - 1\n                )\n                simargs.update(params_extra=params_extra)\n\n\n    simulation = Simulation(**simargs)\n    if verbosity:\n        print_dataset_characteristics(simulation)\n\n    return simulation\n</code></pre>"},{"location":"plot/#remeta.plot.tanh","title":"tanh","text":"<pre><code>tanh(x, beta, thresh, offset)\n</code></pre> Source code in <code>remeta/plot.py</code> <pre><code>def tanh(x, beta, thresh, offset):\n    return \\\n        (np.abs(x) &gt; thresh) * (\n                (1 - offset) * np.tanh(beta * (x - np.sign(x) * thresh)) + np.sign(x) * offset) + \\\n        (np.abs(x) &lt;= thresh) * np.sign(x) * offset\n</code></pre>"},{"location":"plot/#remeta.plot.type1_evidence_to_confidence","title":"type1_evidence_to_confidence","text":"<pre><code>type1_evidence_to_confidence(z1_type1_evidence, type2_evidence_bias_mult=1, type1_noise=None, type1_thresh=None, type1_noise_heteroscedastic=None, type1_noise_signal_dependency='none', y_decval=None, x_stim=None, **kwargs)\n</code></pre> <p>Transformation from type 1 evidence (z1) to confidence (c).</p>"},{"location":"plot/#remeta.plot.type1_evidence_to_confidence--parameters","title":"Parameters","text":"<p>z1_type1_evidence : array-like     Evidence at the type 1 level (= absolute decision value). type2_evidence_bias_mult : float or array-like     Multiplicative metacognitive bias parameter loading on evidence. type1_noise : float or array-like     Type 1 noise parameter. Can be array-like in case of signal-dependent type 1 noise. type1_noise_heteroscedastic : float or array-like     Signal-dependent type 1 noise parameter. type1_noise_signal_dependency : str     Signal-dependent type 1 noise type. One of 'linear', 'power', 'exponential', 'logarithm'. y_decval : array-like     Decision values. x_stim : array-like     Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value     codes the intensity. kwargs : dict     Convenience parameter to avoid an error if irrelevant parameters are passed.</p>"},{"location":"plot/#remeta.plot.type1_evidence_to_confidence--returns","title":"Returns","text":"<p>c_conf : array-like     Model-predicted confidence.</p> Source code in <code>remeta/transform.py</code> <pre><code>def type1_evidence_to_confidence(z1_type1_evidence,\n                                 type2_evidence_bias_mult=1,\n                                 type1_noise=None, type1_thresh=None,\n                                 type1_noise_heteroscedastic=None, type1_noise_signal_dependency='none',\n                                 y_decval=None, x_stim=None,\n                                 **kwargs):  # noqa\n    \"\"\"\n    Transformation from type 1 evidence (z1) to confidence (c).\n\n    Parameters\n    ----------\n    z1_type1_evidence : array-like\n        Evidence at the type 1 level (= absolute decision value).\n    type2_evidence_bias_mult : float or array-like\n        Multiplicative metacognitive bias parameter loading on evidence.\n    type1_noise : float or array-like\n        Type 1 noise parameter. Can be array-like in case of signal-dependent type 1 noise.\n    type1_noise_heteroscedastic : float or array-like\n        Signal-dependent type 1 noise parameter.\n    type1_noise_signal_dependency : str\n        Signal-dependent type 1 noise type. One of 'linear', 'power', 'exponential', 'logarithm'.\n    y_decval : array-like\n        Decision values.\n    x_stim : array-like\n        Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value\n        codes the intensity.\n    kwargs : dict\n        Convenience parameter to avoid an error if irrelevant parameters are passed.\n\n    Returns\n    ----------\n    c_conf : array-like\n        Model-predicted confidence.\n    \"\"\"\n    z1_type1_evidence = np.atleast_1d(z1_type1_evidence)\n\n    if ((type1_noise_signal_dependency != 'none') or (hasattr(type1_noise, '__len__') and len(type1_noise) == 2)):\n        if x_stim is None:\n            raise ValueError('Type 1 noise is signal-dependent, but stimuli (x_stim) have not been '\n                             'passed.')\n        type1_noise = compute_signal_dependent_type1_noise(\n            x_stim.reshape(-1, 1) if (x_stim.ndim == 1) and (z1_type1_evidence.ndim == 2) else x_stim,\n            type1_noise=type1_noise, type1_thresh=type1_thresh, type1_noise_heteroscedastic=type1_noise_heteroscedastic,\n            type1_noise_signal_dependency=type1_noise_signal_dependency)\n\n    z2_type2_evidence = type2_evidence_bias_mult * z1_type1_evidence\n    c_conf = np.tanh(np.pi * z2_type2_evidence / (2 * np.sqrt(3) * type1_noise))\n\n    return c_conf\n</code></pre>"},{"location":"simulation/","title":"remeta.gendata","text":""},{"location":"simulation/#remeta.gendata.TAB","title":"TAB  <code>module-attribute</code>","text":"<pre><code>TAB = '    '\n</code></pre>"},{"location":"simulation/#remeta.gendata.m","title":"m  <code>module-attribute</code>","text":"<pre><code>m = simu_data(params, nsubjects=1, nsamples=1000, **options)\n</code></pre>"},{"location":"simulation/#remeta.gendata.options","title":"options  <code>module-attribute</code>","text":"<pre><code>options = dict(meta_noise_type='noisy_report', enable_type1_param_thresh=1, enable_type1_param_bias=1, enable_type2_param_evidence_bias_mult=0, type2_noise_dist='beta_mode')\n</code></pre>"},{"location":"simulation/#remeta.gendata.params","title":"params  <code>module-attribute</code>","text":"<pre><code>params = dict(type1_noise=0.2, type1_thresh=0.2, type1_bias=0.2, type2_noise=0.2)\n</code></pre>"},{"location":"simulation/#remeta.gendata.Configuration","title":"Configuration  <code>dataclass</code>","text":"<p>Configuration for the ReMeta toolbox</p>"},{"location":"simulation/#remeta.gendata.Configuration--parameters","title":"Parameters","text":"<p>*** Basic definition of the model *** type2_fitting_type : str (default: 'criteria')     Whether confidence is fitted with discrete criteria or as a continuous variable.     Possible values: 'criteria', 'continuous' type2_noise_type : str (default: 'noisy-report)     Whether the model considers noise at readout or report.     Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature' type2_noise_dist : str         (default: noisy-report + criteria -&gt; 'truncated_norm_mode'                   noisy-report + continuous -&gt; 'truncated_norm_mode'                   noisy-readout + criteria -&gt; 'truncated_norm_mode'                   noisy-readout + continuous -&gt; 'truncated_norm_mode'                   noisy-temperature + criteria -&gt; 'lognorm_mode'                   noisy-temperature + continuous -&gt; 'truncated_norm_mode'         )     Metacognitive noise distribution.     Possible values:         noisy_report: 'beta_mean_std', 'beta_mode_std', 'beta_mode',                       'truncated_norm_mode_std', 'truncated_norm_mode' (default),                       'truncated_gumbel_mode_std', 'truncated_gumbel_mode',                       'truncated_lognorm_mode_std', 'truncated_lognorm', 'truncated_lognorm_mode',                       'truncated_lognorm_mean'         noisy_readout: 'lognorm_median_std', 'lognorm_mean', 'lognorm_mode', 'lognorm_mode_std', 'lognorm_mean_std',                        'gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv',                        'betaprime_mean_std',                        'truncated_norm_mode_std', 'truncated_norm_mode',                        'truncated_gumbel_mode_std', 'truncated_gumbel_mode'         noisy_temperature: same as noisy_readout</p> <p>*** Enable or disable specific parameters *** * Each setting can take the values 0, 1 or 2: *    0: Disable parameter. *    1: Enable parameter. *    2: Enable parameter and fit separate values for the negative and positive stimulus category         (works only for type 1 parameters!) enable_type1_param_noise : int (default: 1)     Fit separate type 1 noise parameters for both stimulus categories. enable_type1_param_noise_heteroscedastic : int (default: 0)     Fit an additional type 1 noise parameter for signal-dependent type 1 noise (the type of dependency is     defined via <code>type1_noise_signal_dependency</code>). enable_type1_param_nonlinear_encoding_gain : int (default: 0) enable_type1_param_nonlinear_encoding_transition : int (default: 0) enable_type1_param_thresh : int (default: 0)     Fit a type 1 threshold. enable_type1_param_bias : int (default: 1)     Fit a type 1 bias towards one of the stimulus categories. enable_type2_param_noise : int (default: 1)     Fit a metacognitive noise parameter enable_type2_param_evidence_bias_mult : int (default: 0)     Fit a multiplicative metacognitive bias loading on evidence. enable_type2_param_criteria : int (default: 0)     Fit confidence criteria.</p> <p>*** Additional options to specify the nature of type 2 fitting *** n_discrete_confidence_levels : int (default: 5)     Number of confidence criteria. Only applies in case of type2_fitting_type='criteria'.</p> <p>*** Define fitting characteristics of the parameters *** * The fitting of each parameter is characzerized as follows: *     1) An initial guess. *     2) Lower and upper bound. *     3) Grid range, i.e. list of values that are tested during the initial gridsearch search. * Sensible default values are provided for all parameters. To tweak those, one can either define an entire * ParameterSet, which is a container for a set of parameters, or each parameter individually. Note that the * parameters must be either defined as a Parameter instance or as List[Parameter] in case when separate values are * fitted for the positive and negative stimulus category/decision value). paramset_type1 : ParameterSet     Parameter set for the type 1 stage. paramset_type2 : ParameterSet     Parameter set for the type 2 stage. paramset : ParameterSet     Parameter set for both stages.</p> Union[Parameter, List[Parameter]]  (default: 1) <p>Parameter for type 1 noise.</p> <p>_type1_param_noise_heteroscedastic : Union[Parameter, List[Parameter]]  (default: 0)     Parameter for signal-dependent type 1 noise. _type1_param_nonlinear_encoding_gain : Union[Parameter, List[Parameter]]  (default: 0)     Gain parameter for nonlinear encoding (higher values -&gt; stronger nonlinearity). _type1_param_nonlinear_encoding_transition : Union[Parameter, List[Parameter]]  (default: 0)     Transition Parameter for nonlinear encoding (). type1_noise_signal_dependency: str (default: 'none')     Can be one of 'none', 'multiplicative', 'power', 'exponential' or 'logarithm'. _type1_param_thresh : Union[Parameter, List[Parameter]] (default: 0)     Parameter for the type 1 threshold. _type1_param_bias : Union[Parameter, List[Parameter]]  (default: 1)     Parameter for the type 1 bias. _type2_param_noise : Union[Parameter, List[Parameter]]  (default: 1)     Parameter for metacognitive noise. _type2_param_evidence_bias_mult : Union[Parameter, List[Parameter]]  (default: 0)     Parameter for a multiplicative metacognitive bias loading on evidence. type2_param_confidence_criteria : List[Parameter]  (default: 1)     List of parameter specifying the confidence criteria.</p> <p>*** Skip type 2 fitting *** skip_type2 : bool (default: False)     If True, ignore type 2 settings in the setup of the model configuration &amp; don't fit type 2 stage.</p> <p>*** Methodoligcal aspects of parameter fitting *** optim_type1_gridsearch : bool (default: False)     If True, perform initial (usually coarse) gridsearch search for type 1 fitting, based on the gridsearch defined     for a Parameter. optim_type1_fine_gridsearch : bool (default: False)     If True, perform an iteratively finer gridsearch search for each parameter (type 1). optim_type1_minimize_along_grid : bool (default: False)     If True, do sqlqp minimization for at each grid point (type 1). optim_type1_global_minimization : str (default: None)     Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with     a global minimizer (type 1). optim_type1_scipy_solvers : str or Tuple/List (default: 'trust-constr')     Set scipy.optimize.minimize gradient method (type 1)     If provided as Tuple/List, test different gradient methods and take the best optim_type2_gridsearch : bool (default: True)     If True, perform initial (usually coarse) gridsearch search for type 2 fitting, based on the gridsearch defined     for a Parameter. optim_type2_fine_gridsearch : bool (default: False)     If True, perform an iteratively finer gridsearch search for each parameter (type 2). optim_type2_minimize_along_grid : bool (default: False)     If True, do sqlqp minimization for at each grid point (type 2). optim_type2_global_minimization : str (default: None)     Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with     a global minimizer (type 2). optim_type2_scipy_solvers : str or Tuple/List (default: ('slsqp', 'Nelder-Mead'))     Set scipy.optimize.minimize gradient method (type 2)     If provided as Tuple/List, test different gradient methods and take the best optim_type2_slsqp_epsilon : float or Tuple/List (default: None)     Set parameter epsilon parameter for the SLSQP optimization method (type 2).     If provided as Tuple/List, test different eps parameters and take the best optim_multiproc : bool (default: False)     If True, use all optim_multiproc_cores cores for parameter estimation. If False, use a single core. optim_multiproc_cores : int (default: -1)     If multicore processing es enabled via optim_multiproc, use optim_multiproc_cores cores for parameter estimation     (-1 for all cores minus 1).</p> <p>*** Preprocessing *** normalize_stimuli_by_max : bool (default: True)     If True, normalize provided stimuli by their maximum value.</p> <p>*** Parameters for the type 2 likelihood computation *** min_type1_likelihood : float     Minimum probability used during the type 1 likelihood computation min_type2_likelihood : float     Minimum probability used during the type 2 likelihood computation type2_binsize : float     Integration bin size for the computation of the likelihood around empirical confidence values y_decval_range_nsds : int     Number of standard deviations around the mean considered for type 1 uncertainty. y_decval_range_nbins : int     Number of discrete decision values bins that are considered to represent type 1 uncertainty. resolution_noisy_temperature : float     Quintile resolution for the marginalization of type 1 noise in case of type2_noise_type 'noisy_temperature'. experimental_min_uniform_type2_likelihood : bool     Instead of using a minimum probability during the likelihood computation, use a maximum cumulative     likelihood based on a 'guessing' model experimental_wrap_type2_integration_window : bool (default: False)     Ensure constant window size for likelihood integration at the bounds.     Only applies in case of type2_fitting_type='continuous' and experimental_disable_type2_binsize=False experimental_include_incongruent_y_decval : bool (default: False)     Include incongruent decision values (i.e., sign(actual choice) != sign(decision value)) for the likelihood     computation experimental_disable_type2_binsize : bool (default: None)     Do not use an integegration window for likelihood computation.     Only applies in case of type2_fitting_type='continuous'</p> <p>*** Other *** true_params : Dict     Pass true (known) parameter values. This can be useful for testing to compare the likelihood of true and     fitted parameters. The likelihood of true parameters is returned (and printed). initilialize_fitting_at_true_params : bool (default: False)     Option to initialize the parameter fitting procedure at the true parameters; this can be helpful for testing. silence_configuration_warnings : bool (default: False)     If True, ignore warnings about user-specified settings. print_configuration : bool (default: True)     If True, print the configuration at instatiation of the ReMeta class.</p> Source code in <code>remeta/configuration.py</code> <pre><code>@reset_dataclass_on_init\n@dataclass\nclass Configuration(ReprMixin):\n    \"\"\"\n    Configuration for the ReMeta toolbox\n\n    Parameters\n    ----------\n    *** Basic definition of the model ***\n    type2_fitting_type : str (default: 'criteria')\n        Whether confidence is fitted with discrete *criteria* or as a continuous variable.\n        Possible values: 'criteria', 'continuous'\n    type2_noise_type : str (default: 'noisy-report)\n        Whether the model considers noise at readout or report.\n        Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'\n    type2_noise_dist : str\n            (default: noisy-report + criteria -&gt; 'truncated_norm_mode'\n                      noisy-report + continuous -&gt; 'truncated_norm_mode'\n                      noisy-readout + criteria -&gt; 'truncated_norm_mode'\n                      noisy-readout + continuous -&gt; 'truncated_norm_mode'\n                      noisy-temperature + criteria -&gt; 'lognorm_mode'\n                      noisy-temperature + continuous -&gt; 'truncated_norm_mode'\n            )\n        Metacognitive noise distribution.\n        Possible values:\n            noisy_report: 'beta_mean_std', 'beta_mode_std', 'beta_mode',\n                          'truncated_norm_mode_std', 'truncated_norm_mode' (default),\n                          'truncated_gumbel_mode_std', 'truncated_gumbel_mode',\n                          'truncated_lognorm_mode_std', 'truncated_lognorm', 'truncated_lognorm_mode',\n                          'truncated_lognorm_mean'\n            noisy_readout: 'lognorm_median_std', 'lognorm_mean', 'lognorm_mode', 'lognorm_mode_std', 'lognorm_mean_std',\n                           'gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv',\n                           'betaprime_mean_std',\n                           'truncated_norm_mode_std', 'truncated_norm_mode',\n                           'truncated_gumbel_mode_std', 'truncated_gumbel_mode'\n            noisy_temperature: same as noisy_readout\n\n\n    *** Enable or disable specific parameters ***\n    * Each setting can take the values 0, 1 or 2:\n    *    0: Disable parameter.\n    *    1: Enable parameter.\n    *    2: Enable parameter and fit separate values for the negative and positive stimulus category\n            (works only for type 1 parameters!)\n    enable_type1_param_noise : int (default: 1)\n        Fit separate type 1 noise parameters for both stimulus categories.\n    enable_type1_param_noise_heteroscedastic : int (default: 0)\n        Fit an additional type 1 noise parameter for signal-dependent type 1 noise (the type of dependency is\n        defined via `type1_noise_signal_dependency`).\n    enable_type1_param_nonlinear_encoding_gain : int (default: 0)\n    enable_type1_param_nonlinear_encoding_transition : int (default: 0)\n    enable_type1_param_thresh : int (default: 0)\n        Fit a type 1 threshold.\n    enable_type1_param_bias : int (default: 1)\n        Fit a type 1 bias towards one of the stimulus categories.\n    enable_type2_param_noise : int (default: 1)\n        Fit a metacognitive noise parameter\n    enable_type2_param_evidence_bias_mult : int (default: 0)\n        Fit a multiplicative metacognitive bias loading on evidence.\n    enable_type2_param_criteria : int (default: 0)\n        Fit confidence criteria.\n\n    *** Additional options to specify the nature of type 2 fitting ***\n    n_discrete_confidence_levels : int (default: 5)\n        Number of confidence criteria. Only applies in case of type2_fitting_type='criteria'.\n\n    *** Define fitting characteristics of the parameters ***\n    * The fitting of each parameter is characzerized as follows:\n    *     1) An initial guess.\n    *     2) Lower and upper bound.\n    *     3) Grid range, i.e. list of values that are tested during the initial gridsearch search.\n    * Sensible default values are provided for all parameters. To tweak those, one can either define an entire\n    * ParameterSet, which is a container for a set of parameters, or each parameter individually. Note that the\n    * parameters must be either defined as a Parameter instance or as List[Parameter] in case when separate values are\n    * fitted for the positive and negative stimulus category/decision value).\n    paramset_type1 : ParameterSet\n        Parameter set for the type 1 stage.\n    paramset_type2 : ParameterSet\n        Parameter set for the type 2 stage.\n    paramset : ParameterSet\n        Parameter set for both stages.\n\n    _type1_param_noise : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for type 1 noise.\n    _type1_param_noise_heteroscedastic : Union[Parameter, List[Parameter]]  (default: 0)\n        Parameter for signal-dependent type 1 noise.\n    _type1_param_nonlinear_encoding_gain : Union[Parameter, List[Parameter]]  (default: 0)\n        Gain parameter for nonlinear encoding (higher values -&gt; stronger nonlinearity).\n    _type1_param_nonlinear_encoding_transition : Union[Parameter, List[Parameter]]  (default: 0)\n        Transition Parameter for nonlinear encoding ().\n    type1_noise_signal_dependency: str (default: 'none')\n        Can be one of 'none', 'multiplicative', 'power', 'exponential' or 'logarithm'.\n    _type1_param_thresh : Union[Parameter, List[Parameter]] (default: 0)\n        Parameter for the type 1 threshold.\n    _type1_param_bias : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for the type 1 bias.\n    _type2_param_noise : Union[Parameter, List[Parameter]]  (default: 1)\n        Parameter for metacognitive noise.\n    _type2_param_evidence_bias_mult : Union[Parameter, List[Parameter]]  (default: 0)\n        Parameter for a multiplicative metacognitive bias loading on evidence.\n    type2_param_confidence_criteria : List[Parameter]  (default: 1)\n        List of parameter specifying the confidence criteria.\n\n    *** Skip type 2 fitting ***\n    skip_type2 : bool (default: False)\n        If True, ignore type 2 settings in the setup of the model configuration &amp; don't fit type 2 stage.\n\n    *** Methodoligcal aspects of parameter fitting ***\n    optim_type1_gridsearch : bool (default: False)\n        If True, perform initial (usually coarse) gridsearch search for type 1 fitting, based on the gridsearch defined\n        for a Parameter.\n    optim_type1_fine_gridsearch : bool (default: False)\n        If True, perform an iteratively finer gridsearch search for each parameter (type 1).\n    optim_type1_minimize_along_grid : bool (default: False)\n        If True, do sqlqp minimization for at each grid point (type 1).\n    optim_type1_global_minimization : str (default: None)\n        Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with\n        a global minimizer (type 1).\n    optim_type1_scipy_solvers : str or Tuple/List (default: 'trust-constr')\n        Set scipy.optimize.minimize gradient method (type 1)\n        If provided as Tuple/List, test different gradient methods and take the best\n    optim_type2_gridsearch : bool (default: True)\n        If True, perform initial (usually coarse) gridsearch search for type 2 fitting, based on the gridsearch defined\n        for a Parameter.\n    optim_type2_fine_gridsearch : bool (default: False)\n        If True, perform an iteratively finer gridsearch search for each parameter (type 2).\n    optim_type2_minimize_along_grid : bool (default: False)\n        If True, do sqlqp minimization for at each grid point (type 2).\n    optim_type2_global_minimization : str (default: None)\n        Use one of 'shgo', 'dual_annealing' 'differential_evolution' to start likelihood minimization with\n        a global minimizer (type 2).\n    optim_type2_scipy_solvers : str or Tuple/List (default: ('slsqp', 'Nelder-Mead'))\n        Set scipy.optimize.minimize gradient method (type 2)\n        If provided as Tuple/List, test different gradient methods and take the best\n    optim_type2_slsqp_epsilon : float or Tuple/List (default: None)\n        Set parameter epsilon parameter for the SLSQP optimization method (type 2).\n        If provided as Tuple/List, test different eps parameters and take the best\n    optim_multiproc : bool (default: False)\n        If True, use all optim_multiproc_cores cores for parameter estimation. If False, use a single core.\n    optim_multiproc_cores : int (default: -1)\n        If multicore processing es enabled via optim_multiproc, use optim_multiproc_cores cores for parameter estimation\n        (-1 for all cores minus 1).\n\n    *** Preprocessing ***\n    normalize_stimuli_by_max : bool (default: True)\n        If True, normalize provided stimuli by their maximum value.\n\n    *** Parameters for the type 2 likelihood computation ***\n    min_type1_likelihood : float\n        Minimum probability used during the type 1 likelihood computation\n    min_type2_likelihood : float\n        Minimum probability used during the type 2 likelihood computation\n    type2_binsize : float\n        Integration bin size for the computation of the likelihood around empirical confidence values\n    y_decval_range_nsds : int\n        Number of standard deviations around the mean considered for type 1 uncertainty.\n    y_decval_range_nbins : int\n        Number of discrete decision values bins that are considered to represent type 1 uncertainty.\n    resolution_noisy_temperature : float\n        Quintile resolution for the marginalization of type 1 noise in case of type2_noise_type 'noisy_temperature'.\n    experimental_min_uniform_type2_likelihood : bool\n        Instead of using a minimum probability during the likelihood computation, use a maximum cumulative\n        likelihood based on a 'guessing' model\n    experimental_wrap_type2_integration_window : bool (default: False)\n        Ensure constant window size for likelihood integration at the bounds.\n        Only applies in case of type2_fitting_type='continuous' and experimental_disable_type2_binsize=False\n    experimental_include_incongruent_y_decval : bool (default: False)\n        Include incongruent decision values (i.e., sign(actual choice) != sign(decision value)) for the likelihood\n        computation\n    experimental_disable_type2_binsize : bool (default: None)\n        Do not use an integegration window for likelihood computation.\n        Only applies in case of type2_fitting_type='continuous'\n\n\n    *** Other ***\n    true_params : Dict\n        Pass true (known) parameter values. This can be useful for testing to compare the likelihood of true and\n        fitted parameters. The likelihood of true parameters is returned (and printed).\n    initilialize_fitting_at_true_params : bool (default: False)\n        Option to initialize the parameter fitting procedure at the true parameters; this can be helpful for testing.\n    silence_configuration_warnings : bool (default: False)\n        If True, ignore warnings about user-specified settings.\n    print_configuration : bool (default: True)\n        If True, print the configuration at instatiation of the ReMeta class.\n    \"\"\"\n\n    type2_fitting_type: str = 'criteria'\n    type2_noise_type: str = 'noisy_report'\n    type2_noise_dist: str = None\n        # noisy-report + criteria -&gt; 'truncated_norm_mode'\n        # noisy-report + continuous -&gt; 'truncated_norm_mode'\n        # noisy-readout + criteria -&gt; 'truncated_norm_mode'\n        # noisy-readout + continuous -&gt; 'truncated_norm_mode'\n        # noisy-temperature + criteria -&gt; 'lognorm_mode'\n        # noisy-temperature + continuous -&gt; 'truncated_norm_mode'\n\n    enable_type1_param_noise: int = 1\n    enable_type1_param_thresh: int = 0\n    enable_type1_param_bias: int = 1\n    enable_type2_param_noise: int = 1\n    enable_type2_param_evidence_bias_mult: int = 0\n    enable_type2_param_criteria: int = 1\n    # Experimental:\n    enable_type1_param_noise_heteroscedastic: int = 0\n    enable_type1_param_nonlinear_encoding_gain: int = 0\n    enable_type1_param_nonlinear_encoding_transition: int = 0\n\n    n_discrete_confidence_levels: int = 5\n\n    paramset_type1: ParameterSet = None\n    paramset_type2: ParameterSet = None\n    paramset_all: ParameterSet = None\n\n    type1_param_noise_heteroscedastic: Parameter = Parameter(guess=0, bounds=(0, 10), grid_range=np.linspace(0, 1, 5))\n    type1_param_nonlinear_encoding_gain: Parameter = Parameter(guess=0, bounds=(-8/9, 10), grid_range=np.linspace(-0.5, 1, 5))\n    type1_param_nonlinear_encoding_transition: Parameter = Parameter(guess=1, bounds=(0.01, 10), grid_range=np.linspace(0.01, 2, 5))\n    type1_param_noise: Parameter = Parameter(guess=0.5, bounds=(0.001, 100), grid_range=np.linspace(0.1, 1, 8))\n    type1_param_thresh: Parameter = Parameter(guess=0, bounds=(0, 1), grid_range=np.linspace(0, 0.2, 5))\n    type1_param_bias: Parameter = Parameter(guess=0, bounds=(-1, 1), grid_range=np.linspace(-0.2, 0.2, 8))\n    type2_param_noise: Parameter = Parameter(guess=0.1, bounds=(0.05, 2), grid_range=np.linspace(0.1, 1, 8))\n    type2_param_evidence_bias_mult: Parameter = Parameter(guess=1, bounds=(0.5, 2), grid_range=np.linspace(0.5, 2, 8))\n    type2_param_criteria: Parameter = Parameter(bounds=(1e-8, 1))\n    type2_param_criteria_guesses: str | List[float] = 'equidistant'\n    type2_param_criteria_grid_ranges: str | List[np.ndarray] = 'equidistant'\n\n    type1_noise_signal_dependency: str = 'none'\n\n    skip_type2 = False\n\n    optim_type1_gridsearch: bool = False\n    optim_type1_fine_gridsearch: bool = False\n    optim_type1_minimize_along_grid: bool = False\n    optim_type1_global_minimization: str = None\n    _optim_type1_scipy_solvers_default = 'trust-constr'\n    optim_type1_scipy_solvers: str | List[str] | Tuple[str, ...] = 'trust-constr'\n    optim_type2_gridsearch: bool = True\n    optim_type2_fine_gridsearch: bool = False\n    optim_type2_minimize_along_grid: bool = False\n    optim_type2_global_minimization: str = None\n    optim_type2_scipy_solvers: str | List[str] | Tuple[str, ...] = ('slsqp', 'Nelder-Mead')\n    optim_type2_slsqp_epsilon: float = None\n    optim_multiproc: bool = False\n    optim_multiproc_cores: int = -1\n    _optim_multiproc_cores_effective: int = None\n\n    normalize_stimuli_by_max: bool = True\n    confidence_bounds_error: float = 0\n\n    min_type2_likelihood: float = 1e-10\n    min_type1_likelihood: float = 1e-10\n    type2_binsize: float = 0.01\n    y_decval_range_nsds: int = 5\n    y_decval_range_nbins: int = 101\n    resolution_noisy_temperature: float = 0.001\n\n    experimental_min_uniform_type2_likelihood: bool = False\n    experimental_wrap_type2_integration_window: bool = False\n    experimental_include_incongruent_y_decval: bool = False\n    experimental_disable_type2_binsize: bool = False\n\n    true_params: Dict = None\n    initilialize_fitting_at_true_params: bool = False\n    silence_configuration_warnings: bool = False\n    print_configuration: bool = False\n\n    type2_param_noise_min: float = 0.001\n\n    # setup_called = False\n\n    _type1_param_noise: Parameter | List[Parameter] = None\n    _type1_param_noise_heteroscedastic: Parameter | List[Parameter] = None\n    _type1_param_nonlinear_encoding_transition: Parameter | List[Parameter] = None\n    _type1_param_nonlinear_encoding_gain: Parameter | List[Parameter] = None\n    _type1_param_thresh: Parameter | List[Parameter] = None\n    _type1_param_bias: Parameter | List[Parameter] = None\n    _type2_param_noise: Parameter = None\n    _type2_param_evidence_bias_mult: Parameter = None\n    _type2_param_criteria: List[Parameter] = None\n\n    def setup(self, generative_mode=False):\n\n        if find_spec('multiprocessing_on_dill') is None:\n            warnings.warn(f'Multiprocessing on dill is not installed. Setting grid_multiproc is changed to False.')\n            self.optim_multiproc = False\n\n        if self.optim_multiproc:\n            from multiprocessing import cpu_count\n            self._optim_multiproc_cores_effective = max(1, (cpu_count() or 1) - 1) if self.optim_multiproc_cores == -1 \\\n                else self.optim_multiproc_cores\n\n        self._prepare_params_type1()\n        if self.skip_type2:\n            if self.optim_type2_slsqp_epsilon is None:\n                self.optim_type2_slsqp_epsilon = 1e-5\n        else:\n\n            if self.enable_type1_param_thresh and \\\n                (self.optim_type1_scipy_solvers == self._optim_type1_scipy_solvers_default):\n                self.optim_type1_scipy_solvers = ('trust-constr', 'Powell')\n\n\n            if self.type2_noise_dist is None:\n                if generative_mode:\n                    raise ValueError('In generative mode, you need to explicitly specify a type 2 noise distribution.')\n                else:\n                    if self.type2_noise_type == 'noisy_report':\n                        if (self.type2_fitting_type == 'criteria'):\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                    elif (self.type2_noise_type == 'noisy_readout'):\n                        if self.type2_fitting_type == 'criteria':\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n                    elif self.type2_noise_type == 'noisy_temperature':\n                        if self.type2_fitting_type == 'criteria':\n                            self.type2_noise_dist = 'lognorm_mode'\n                        else:\n                            self.type2_noise_dist = 'truncated_norm_mode'\n\n            self._prepare_params_type2()\n            if self.optim_type2_slsqp_epsilon is None:\n                self.optim_type2_slsqp_epsilon = 1e-5\n\n            if self.type2_binsize is None:\n                self.type2_binsize = 0.01\n\n        self._prepare_params_all()\n\n        self._check_compatibility(generative_mode=generative_mode)\n\n        if self.print_configuration:\n            self.print()\n        # self.setup_called = True\n\n    def _check_compatibility(self, generative_mode=False):\n\n        if not self.silence_configuration_warnings:\n\n            if not self.skip_type2:\n                if not self.enable_type2_param_noise:\n                    warnings.warn(f'Setting enable_type2_param_noise=False was provided -&gt; type2_param_noise is set to its default value '\n                                  f'({self._type2_param_noise_default}). You may change this value via the configuration.')\n\n                if (self.type2_noise_type == 'noisy_temperature') and self.type2_param_noise.default_changed and \\\n                    (self.type2_param_noise.bounds[0] &lt; 1e-5):\n                    warnings.warn('You manually changed the lower bound of the type 2 noise parameter for a '\n                                  'noisy-temperature model to a very low value (&lt;1e-5). Be warned that this may result '\n                                  'in numerical instabilities that severely distort the likelihood computation.')\n\n                if not generative_mode:\n                    # If the configuration instance is used for generating data, we should not complain\n                    # about fitting issues.\n\n                    if self.enable_type2_param_criteria and self.enable_type2_param_evidence_bias_mult:\n                        warnings.warn(\n                            'enable_type2_param_criteria=True in combination with enable_type2_param_evidence_bias_mult=True\\n'\n                            'can lead to biased parameter inferences. Use with caution.')\n\n                    if (self.type2_fitting_type == 'continuous') and self.enable_type2_param_criteria:\n                        raise ValueError(\"Setting type2_fitting_type='continuous' conflicts with enable_type2_param_criteria=1.'\")\n\n                    if (self.type2_fitting_type == 'criteria') and not self.enable_type2_param_criteria:\n                        warnings.warn(\"You selected type2_fitting_type='criteria', but did not enable type 2 criteria\\n\"\n                                      \"(enable_type2_param_criteria=0). This works, but be mindful that the model\\n\"\n                                      \"will assume equispaced ideal Bayesian observer criteria (respecting \\n\"\n                                      \"the setting n_discrete_confidence_levels).\")\n\n    def _prepare_params_type1(self):\n        # if self.paramset_type1 is None:\n\n            param_names_type1 = []\n            params_type1 = ('noise', 'noise_heteroscedastic', 'nonlinear_encoding_gain', 'nonlinear_encoding_transition', 'thresh', 'bias')\n            for param in params_type1:\n                if getattr(self, f'enable_type1_param_{param}'):\n                    param_names_type1 += [f'type1_{param}']\n                    if getattr(self, f'_type1_param_{param}') is None:\n                        param_definition = getattr(self, f'type1_param_{param}')\n                        if getattr(self, f'enable_type1_param_{param}') == 2:\n                            setattr(self, f'_type1_param_{param}', [param_definition, param_definition])\n                        else:\n                            setattr(self, f'_type1_param_{param}', param_definition)\n                        if self.true_params is not None and self.initilialize_fitting_at_true_params and f'type1_{param}' in self.true_params:\n                            getattr(self, f'_type1_param_{param}').guess = self.true_params[f'type1_{param}']\n\n            parameters = {k: getattr(self, f\"_type1_param_{k.split('type1_')[1]}\") for k in param_names_type1}\n            self.paramset_type1 = ParameterSet(parameters, param_names_type1)\n\n    def _prepare_params_type2(self):\n\n        # if self.paramset_type2 is None:\n\n            if self.enable_type2_param_noise and self._type2_param_noise is None and not self.type2_param_noise.default_changed:\n\n                lb = 0.05\n                self.type2_param_noise.bounds = dict(\n                    noisy_report = dict(\n                        beta_mean_std=(lb, 0.5),\n                        beta_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_norm_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_gumbel_mode_std=(lb, 1 / np.sqrt(12)),\n                        truncated_lognorm_mode_std=(lb, 1 / np.sqrt(12)),\n                        beta_mode=(lb, 1),\n                        truncated_norm_mode=(lb, 1),\n                        truncated_gumbel_mode=(lb, 1),\n                        truncated_lognorm_mode=(lb, 4),\n                        truncated_lognorm_mean=(lb, 4),\n                        truncated_lognorm=(lb, 4)\n                    ),\n                    noisy_readout = dict(\n                        lognorm_mean=(lb, 1),\n                        lognorm_mode=(lb, 1),\n                        gamma_mean_std=(lb, 1),\n                        lognorm_mean_std=(lb, 2),\n                        lognorm_mode_std=(lb, 2),\n                        lognorm_median_std=(lb, 2),\n                        gamma_mean_cv=(lb, 2),\n                        gamma_mean=(lb, 2),\n                        gamma_mode_std=(lb, 2),\n                        gamma_mode=(lb, 2),\n                        betaprime_mean_std=(lb, 2),\n                        truncated_norm_mode_std=(lb, 2),\n                        truncated_norm_mode=(lb, 2),\n                        truncated_gumbel_mode_std=(lb, 2),\n                        truncated_gumbel_mode=(lb, 2)\n                    ),\n                    noisy_temperature = dict(\n                        lognorm_mean=(lb, 1),\n                        gamma_mean_std=(lb, 1),\n                        lognorm_mean_std=(lb, 2),\n                        lognorm_median_std=(lb, 2),\n                        gamma_mean_cv=(lb, 2),\n                        gamma_mean=(lb, 2),\n                        gamma_mode_std=(lb, 2),\n                        gamma_mode=(lb, 2),\n                        betaprime_mean_std=(lb, 2),\n                        truncated_norm_mode_std=(lb, 2),\n                        truncated_norm_mode=(lb, 2),\n                        truncated_gumbel_mode_std=(lb, 2),\n                        truncated_gumbel_mode=(lb, 2),\n                        lognorm_mode=(lb, 4),\n                        lognorm_mode_std=(lb, 10),\n                    )\n                )[self.type2_noise_type][self.type2_noise_dist]\n                self.type2_param_noise.grid_range = np.exp(np.linspace(np.log(self.type2_param_noise.bounds[0]),\n                                                                       np.log(self.type2_param_noise.bounds[1]), 10)[1:-1])\n\n            param_names_type2 = []\n            params_type2 = ('noise', 'evidence_bias_mult')\n            for param in params_type2:\n                if getattr(self, f'enable_type2_param_{param}'):\n                    param_names_type2 += [f'type2_{param}']\n                    if getattr(self, f'_type2_param_{param}') is None:\n                        param_definition = getattr(self, f'type2_param_{param}')\n                        setattr(self, f'_type2_param_{param}', param_definition.copy())\n                        if self.true_params is not None and self.initilialize_fitting_at_true_params and f'type2_{param}' in self.true_params:\n                            getattr(self, f'_type2_param_{param}').guess = self.true_params[f'type2_{param}']\n\n\n            if self.enable_type2_param_criteria:\n                param_names_type2 += [f'type2_criteria']\n                initialize_true = (self.initilialize_fitting_at_true_params and\n                                   self.true_params is not None and 'type2_criteria' in self.true_params)\n                setattr(self, f'_type2_param_criteria',\n                        [Parameter(\n                           guess=self.true_params['type2_criteria'][i] if initialize_true\n                                    else (1 / self.n_discrete_confidence_levels if self.type2_param_criteria_guesses == 'equidistant'\n                                          else self.type2_param_criteria_guesses[i]),\n                           bounds=self.type2_param_criteria.bounds,\n                           grid_range=np.linspace(0.05, 2 / self.n_discrete_confidence_levels, 4) if\n                                self.type2_param_criteria_grid_ranges == 'equidistant' else self.type2_param_criteria_grid_ranges[i]\n                        )\n                         for i in range(self.n_discrete_confidence_levels - 1)]\n                        )\n                if self.true_params is not None:\n                    if isinstance(self.true_params, dict):\n                        # if 'type2_criteria' not in self.true_params:\n                        #     raise ValueError('type2_criteria are missing from cfg.true_params')\n                        if 'type2_criteria' in self.true_params:\n                            self.true_params.update(\n                                type2_criteria_absolute=[np.sum(self.true_params['type2_criteria'][:i+1]) for i in range(len(self.true_params['type2_criteria']))],\n                                type2_criteria_bias=np.mean(self.true_params['type2_criteria'])*(len(self.true_params['type2_criteria'])+1)-1\n                            )\n                    elif isinstance(self.true_params, list):\n                        for s in range(len(self.true_params)):\n                            # if 'type2_criteria' not in self.true_params[s]:\n                            #     raise ValueError(f'type2_criteria are missing from cfg.true_params (subject {s})')\n                            if 'type2_criteria' in self.true_params[s]:\n                                self.true_params[s].update(\n                                    type2_criteria_absolute=[np.sum(self.true_params[s]['type2_criteria'][:i+1]) for i in range(len(self.true_params[s]['type2_criteria']))],\n                                    type2_criteria_bias=np.mean(self.true_params[s]['type2_criteria'])*(len(self.true_params[s]['type2_criteria'])+1)-1\n                                )\n\n            parameters = {k: getattr(self, f\"_type2_param_{k.split('type2_')[1]}\") for k in param_names_type2}\n            self.paramset_type2 = ParameterSet(parameters, param_names_type2)\n\n\n            self.check_type2_constraints()\n\n\n    def _prepare_params_all(self):\n\n        if self.skip_type2:\n            self.paramset = self.paramset_type1\n        else:\n            parameters_all = {**self.paramset_type1.parameters, **self.paramset_type2.parameters}\n            param_names_all = self.paramset_type1.param_names + self.paramset_type2.param_names\n            self.paramset = ParameterSet(parameters_all, param_names_all)\n            # for k, attr in self.paramset_type2.__dict__.items():\n            #     attr_old = getattr(self.paramset, k)\n            #     if isinstance(attr, list):\n            #         attr_new = attr_old + attr\n            #     elif isinstance(attr, dict):\n            #         attr_new = {**attr_old, **attr}\n            #     elif isinstance(attr, np.ndarray):\n            #         if attr.ndim == 1:\n            #             attr_new = np.hstack((attr_old, attr))\n            #         else:\n            #             attr_new = np.vstack((attr_old, attr))\n            #     elif isinstance(attr, int):\n            #         attr_new = attr_old + attr\n            #     elif attr is None:\n            #         if attr_old is None:\n            #             attr_new = None\n            #         else:\n            #             raise ValueError(f'Type 2 attribute is None, but type 1 attribute is not.')\n            #     else:\n            #         raise ValueError(f'Unexpected type {type(attr)}')\n            #     setattr(self.paramset, k, attr_new)\n\n\n\n\n    def print(self):\n        # print('***********************')\n        print(f'{self.__class__.__name__}')\n        for k, v in self.__dict__.items():\n            # if not self.skip_type2 or ('type2' not in k):\n            print('\\n'.join([f'\\t{k}: {v}']))\n        # print('***********************')\n\n    def __repr__(self):\n        txt = f'{self.__class__.__name__}\\n'\n        txt += '\\n'.join([f'\\t{k}: {v}' for k, v in self.__dict__.items()])\n        return txt\n\n    def check_type2_constraints(self):\n        pass\n</code></pre>"},{"location":"simulation/#remeta.gendata.Simulation","title":"Simulation","text":"Source code in <code>remeta/gendata.py</code> <pre><code>class Simulation:\n    def __init__(self, nsubjects=None, nsamples=None, params=None, params_extra=None, cfg=None,\n                 x_stim=None, x_stim_category=None, d_dec=None, y_decval_latent=None, y_decval=None,\n                 z1_type1_evidence_latent=None, z1_type1_evidence_base=None, z1_type1_evidence=None,\n                 c_conf_latent=None, c_conf_base=None, c_conf=None,\n                 likelihood_dist=None, type1_stats=None, type2_stats=None):\n        self.nsubjects = nsubjects\n        self.nsamples = nsamples\n        self.params = params\n        self.params_type1 = {k: v for k, v in self.params.items() if k.startswith('type1_')}\n        self.params_type2 = {k: v for k, v in self.params.items() if k.startswith('type2_')}\n        self.params_extra = params_extra\n        self.cfg = cfg\n        self.stimuli = x_stim\n        self.stimuli_category = x_stim_category\n        self.choices = d_dec\n        self.accuracy = x_stim_category == d_dec\n        self.y_decval_latent = y_decval_latent\n        self.y_decval = y_decval\n        self.z1_type1_evidence_latent = z1_type1_evidence_latent\n        self.z1_type1_evidence_base = z1_type1_evidence_base\n        self.z1_type1_evidence = z1_type1_evidence\n        self.confidence_latent = c_conf_latent\n        self.confidence_base = c_conf_base\n        self.confidence = c_conf\n        self.likelihood_dist = likelihood_dist\n        self.type1_stats = type1_stats\n        self.type2_stats = type2_stats\n\n    def squeeze(self):\n        # for var in ('x_stim', 'x_stim_category', 'd_dec', 'accuracy'):\n        #     if getattr(self, var) is not None:\n        #         setattr(self, var, getattr(self, var).squeeze())\n        for name, value in self.__dict__.items():\n            if isinstance(value, np.ndarray):\n                setattr(self, name, value.squeeze())\n        return self\n</code></pre>"},{"location":"simulation/#remeta.gendata._check_param","title":"_check_param","text":"<pre><code>_check_param(x)\n</code></pre> Source code in <code>remeta/util.py</code> <pre><code>def _check_param(x):\n    if hasattr(x, '__len__'):\n        if len(x) == 2:\n            return x\n        elif len(x) == 1:\n            return [x[0], x[0]]\n        else:\n            print(f'Something went wrong, parameter array has length {len(x)}')\n    else:\n        return [x, x]\n</code></pre>"},{"location":"simulation/#remeta.gendata.check_criteria_sum","title":"check_criteria_sum","text":"<pre><code>check_criteria_sum(criteria)\n</code></pre> <p>Ensure that criteria sum up to at most 1</p>"},{"location":"simulation/#remeta.gendata.check_criteria_sum--parameters","title":"Parameters","text":"<p>criteria : array-like of dtype float     Confidence criteria (typically provided as a list)</p>"},{"location":"simulation/#remeta.gendata.check_criteria_sum--returns","title":"Returns","text":"<p>criteria : array-like of dtype float     Transformed confidence criteria</p> Source code in <code>remeta/transform.py</code> <pre><code>def check_criteria_sum(criteria):\n    \"\"\"\n    Ensure that criteria sum up to at most 1\n\n    Parameters\n    ----------\n    criteria : array-like of dtype float\n        Confidence criteria (typically provided as a list)\n\n    Returns\n    ----------\n    criteria : array-like of dtype float\n        Transformed confidence criteria\n    \"\"\"\n    ind1 = np.where(np.cumsum(criteria) &gt; 1)[0]\n    if len(ind1):\n        for i in ind1[1:]:\n            criteria[i] = 0\n        criteria[ind1[0]] = 1 - np.sum(criteria[:ind1[0]]) + 1e-8\n        # Note: we add 1e-8 to avoid edge cases due to floating point precision\n    return criteria\n</code></pre>"},{"location":"simulation/#remeta.gendata.compute_nonlinear_encoding","title":"compute_nonlinear_encoding","text":"<pre><code>compute_nonlinear_encoding(x_stim, type1_nonlinear_encoding_gain=None, type1_nonlinear_encoding_transition=None)\n</code></pre> Source code in <code>remeta/transform.py</code> <pre><code>def compute_nonlinear_encoding(x_stim, type1_nonlinear_encoding_gain=None, type1_nonlinear_encoding_transition=None):\n\n    gain = _check_param(0 if type1_nonlinear_encoding_gain is None else type1_nonlinear_encoding_gain)\n    transition = _check_param(1 if type1_nonlinear_encoding_transition is None else type1_nonlinear_encoding_transition)\n\n    neg, pos = x_stim &lt; 0, x_stim &gt;= 0\n    x_stim_nonlinear = np.empty_like(x_stim)\n    x_stim_nonlinear[neg] = x_stim[neg] * (1 + gain[0] * (x_stim[neg] / transition[0])**2 /\n                                           (1 + (x_stim[neg] / transition[0])**2))\n    x_stim_nonlinear[pos] = x_stim[pos] * (1 + gain[1] * (x_stim[pos] / transition[1])**2 /\n                                           (1 + (x_stim[pos] / transition[1])**2))\n\n    return x_stim_nonlinear\n</code></pre>"},{"location":"simulation/#remeta.gendata.compute_signal_dependent_type1_noise","title":"compute_signal_dependent_type1_noise","text":"<pre><code>compute_signal_dependent_type1_noise(x_stim, type1_noise=None, type1_thresh=None, type1_noise_heteroscedastic=None, type1_noise_signal_dependency='none', **kwargs)\n</code></pre> <p>Compute signal-dependent type 1 noise.</p>"},{"location":"simulation/#remeta.gendata.compute_signal_dependent_type1_noise--parameters","title":"Parameters","text":"<p>x_stim : array-like     Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value     codes the intensity. Must be normalized to [-1; 1]. type1_noise : float or array-like     Type 1 noise parameter. type1_thresh: float or array-like     Type 1 threshold. type1_noise_heteroscedastic : float or array-like     Signal-dependent type 1 noise parameter. type1_noise_signal_dependency : str     Define the signal dependency of type 1 noise. One of 'none', 'multiplicative', 'power', 'exponential', 'logarithm'. kwargs : dict     Convenience parameter to avoid an error if irrelevant parameters are passed.</p>"},{"location":"simulation/#remeta.gendata.compute_signal_dependent_type1_noise--returns","title":"Returns","text":"<p>type1_noise_heteroscedastic : array-like     Signal-dependent (heteroscedastic) type 1 noise of shape stimuli.shape.</p> Source code in <code>remeta/transform.py</code> <pre><code>def compute_signal_dependent_type1_noise(x_stim, type1_noise=None, type1_thresh=None, type1_noise_heteroscedastic=None,\n                                         type1_noise_signal_dependency='none', **kwargs):  # noqa\n    \"\"\"\n    Compute signal-dependent type 1 noise.\n\n    Parameters\n    ----------\n    x_stim : array-like\n        Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value\n        codes the intensity. Must be normalized to [-1; 1].\n    type1_noise : float or array-like\n        Type 1 noise parameter.\n    type1_thresh: float or array-like\n        Type 1 threshold.\n    type1_noise_heteroscedastic : float or array-like\n        Signal-dependent type 1 noise parameter.\n    type1_noise_signal_dependency : str\n        Define the signal dependency of type 1 noise. One of 'none', 'multiplicative', 'power', 'exponential', 'logarithm'.\n    kwargs : dict\n        Convenience parameter to avoid an error if irrelevant parameters are passed.\n\n    Returns\n    ----------\n    type1_noise_heteroscedastic : array-like\n        Signal-dependent (heteroscedastic) type 1 noise of shape stimuli.shape.\n    \"\"\"\n\n    type1_noise_ = _check_param(type1_noise)\n    type1_heteroscedastic_ = _check_param(type1_noise_heteroscedastic)\n    type1_thresh_ = (0, 0) if type1_thresh is None else _check_param(type1_thresh)\n    type1_noise_heteroscedastic = np.ones(x_stim.shape)\n    neg, pos = x_stim &lt; 0, x_stim &gt;= 0\n    if type1_noise_signal_dependency == 'none':\n        type1_noise_heteroscedastic[neg] *= type1_noise_[0]\n        type1_noise_heteroscedastic[pos] *= type1_noise_[1]\n    elif type1_noise_signal_dependency == 'multiplicative':\n        type1_noise_heteroscedastic[neg] = np.sqrt(type1_noise_[0] ** 2 +\n                                                         ((np.abs(x_stim[neg]) - type1_thresh_[0]) * type1_heteroscedastic_[0]) ** 2)  # noqa\n        type1_noise_heteroscedastic[pos] = np.sqrt(type1_noise_[1] ** 2 +\n                                                         ((np.abs(x_stim[pos]) - type1_thresh_[1]) * type1_heteroscedastic_[1]) ** 2)  # noqa\n    elif type1_noise_signal_dependency == 'power':\n        type1_noise_heteroscedastic[neg] = np.sqrt(type1_noise_[0] ** 2 +\n                                                         (np.abs(x_stim[neg]) - type1_thresh_[0]) ** (2 * type1_heteroscedastic_[0]))  # noqa\n        type1_noise_heteroscedastic[pos] = np.sqrt(type1_noise_[1] ** 2 +\n                                                         (np.abs(x_stim[pos]) - type1_thresh_[1]) ** (2 * type1_heteroscedastic_[1]))  # noqa\n    elif type1_noise_signal_dependency == 'exponential':\n        type1_noise_heteroscedastic[neg] = np.sqrt(type1_noise_[0] ** 2 +\n                                                         (np.exp(type1_heteroscedastic_[0] * (np.abs(x_stim[neg]) - type1_thresh_[0])) - 1) ** 2)  # noqa\n        type1_noise_heteroscedastic[pos] = np.sqrt(type1_noise_[1] ** 2 +\n                                                         (np.exp(type1_heteroscedastic_[1] * (np.abs(x_stim[pos]) - type1_thresh_[1])) - 1) ** 2)  # noqa\n    elif type1_noise_signal_dependency == 'logarithm':\n        type1_noise_heteroscedastic[neg] = np.sqrt(type1_noise_[0] ** 2 +\n                                                         np.log(type1_heteroscedastic_[0] * (np.abs(x_stim[neg]) - type1_thresh_[0]) + 1) ** 2)  # noqa\n        type1_noise_heteroscedastic[pos] = np.sqrt(type1_noise_[1] ** 2 +\n                                                         np.log(type1_heteroscedastic_[1] * (np.abs(x_stim[pos]) - type1_thresh_[1]) + 1) ** 2)  # noqa\n    else:\n        raise ValueError(f'{type1_noise_signal_dependency} is not a valid function for type1_noise_signal_dependency')\n\n    return type1_noise_heteroscedastic\n</code></pre>"},{"location":"simulation/#remeta.gendata.discretize_confidence_with_bounds","title":"discretize_confidence_with_bounds","text":"<pre><code>discretize_confidence_with_bounds(x, bounds)\n</code></pre> Source code in <code>remeta/util.py</code> <pre><code>def discretize_confidence_with_bounds(x, bounds):\n    confidence = np.full(x.shape, np.nan)\n    bounds = np.hstack((bounds, np.inf))\n    for i, b in enumerate(bounds[:-1]):\n        confidence[(bounds[i] &lt;= x) &amp; (x &lt; bounds[i + 1])] = i + 1\n    return confidence\n</code></pre>"},{"location":"simulation/#remeta.gendata.generate_stimuli","title":"generate_stimuli","text":"<pre><code>generate_stimuli(nsubjects, nsamples, stepsize=0.02, warn_in_case_of_nondivisible_stepsize=False)\n</code></pre> Source code in <code>remeta/gendata.py</code> <pre><code>def generate_stimuli(nsubjects, nsamples, stepsize=0.02, warn_in_case_of_nondivisible_stepsize=False):\n    levels = np.hstack((-np.arange(stepsize, 1.01, stepsize)[::-1], np.arange(stepsize, 1.01, stepsize)))\n    if warn_in_case_of_nondivisible_stepsize and ((nsamples % (2/stepsize)) != 0):\n        warnings.warn(f'At the chosen stepsize of {stepsize} there are {2/stepsize} stimulus levels,'\n                      f'which is not a divisor of the chosen sample size {nsamples}', UserWarning)\n    x_stim = np.array([np.random.permutation(np.tile(levels, int(np.ceil(nsamples / len(levels)))))[:nsamples] for _ in range(nsubjects)])\n    return x_stim\n</code></pre>"},{"location":"simulation/#remeta.gendata.get_type2_dist","title":"get_type2_dist","text":"<pre><code>get_type2_dist(type2_dist, type2_center, type2_noise, type2_noise_type='noisy_report')\n</code></pre> <p>Helper function to select appropriately parameterized type 2 noise distributions.</p>"},{"location":"simulation/#remeta.gendata.get_type2_dist--parameters","title":"Parameters:","text":"<p>type2_dist : str     Name of the type 2 noise distribution. Check TYPE2_NOISE_DISTS for possible values. type2_noise : float or array-like of dtype float     \"True\" value of a noisy type 2 variable. Corresponds to type 1 noise (noisy-temperature), metacognitve     evidence (noisy-readout) or confidence (noisy-report).     For mean- and mode preserving distributions, the center corresponds to the mean (suffix _mean) and mode (suffix     _mode), respectively. type2_noise : float or array-like of dtype float with mode.shape     Spread parameter that represents metacognitve noise. For standard-deviation-preserving distributions, this     parameter corresponds to the standard deviation when sampling data from the distribution (suffix _std). type2_noise_type : str (default='noisy_report')     Metacognitive noise type. Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'.</p>"},{"location":"simulation/#remeta.gendata.get_type2_dist--returns","title":"Returns:","text":"<p>scipy.stats continuous distribution instance</p> Source code in <code>remeta/type2_dist.py</code> <pre><code>def get_type2_dist(type2_dist, type2_center, type2_noise, type2_noise_type='noisy_report'):\n    \"\"\"\n    Helper function to select appropriately parameterized type 2 noise distributions.\n\n    Parameters:\n    -----------\n    type2_dist : str\n        Name of the type 2 noise distribution. Check TYPE2_NOISE_DISTS for possible values.\n    type2_noise : float or array-like of dtype float\n        \"True\" value of a noisy type 2 variable. Corresponds to type 1 noise (noisy-temperature), metacognitve\n        evidence (noisy-readout) or confidence (noisy-report).\n        For mean- and mode preserving distributions, the center corresponds to the mean (suffix _mean) and mode (suffix\n        _mode), respectively.\n    type2_noise : float or array-like of dtype float with mode.shape\n        Spread parameter that represents metacognitve noise. For standard-deviation-preserving distributions, this\n        parameter corresponds to the standard deviation when sampling data from the distribution (suffix _std).\n    type2_noise_type : str (default='noisy_report')\n        Metacognitive noise type. Possible values: 'noisy_report', 'noisy_readout', 'noisy_temperature'.\n\n    Returns:\n    --------\n    scipy.stats continuous distribution instance\n    \"\"\"\n\n    if type2_dist not in TYPE2_NOISE_DISTS:\n        raise ValueError(f\"Unkonwn distribution '{type2_dist}'.\")\n    elif (type2_noise_type == 'noisy_report') and type2_dist in TYPE2_NOISE_DISTS_READOUT_TEMPERATURE_ONLY:\n        raise ValueError(f\"Distribution '{type2_dist}' is only valid for noisy-readout or noisy-temperature models.\")\n    elif (type2_noise_type in ('noisy_readout', 'noisy_temperature')) and type2_dist in TYPE2_NOISE_DISTS_REPORT_ONLY:\n        raise ValueError(f\"Distribution '{type2_dist}' is only valid for noisy-report models.\")\n\n    if type2_noise &lt; 1e-10:\n        warnings.warn('Type 2 noise is smaller than 1e-10, which can lead to unstable numerical results. It is set'\n                      'to the hard minimum of 1e-10.')\n        type2_noise = 1e-10\n\n    if type2_dist == 'lognorm_median_std':\n        # type2_center = median, type2_noise = SD\n        s = np.maximum(1e-12, np.sqrt(np.log((1 + np.sqrt(1 + 4 * (type2_noise / type2_center) ** 2)) / 2)))\n        dist = lognorm(s=s, scale=type2_center)\n    elif type2_dist == 'lognorm_mean':\n        # type2_center = mean, type2_noise = SD in log space\n        dist = lognorm(s=type2_noise, scale=np.exp(-type2_noise ** 2 / 2) * type2_center)\n    elif type2_dist == 'lognorm_mode':\n        # type2_center = mode, type2_noise = SD in log space\n        if type2_noise &gt; 23:\n            # warnings.warn(f'Type 2 noise for a {type2_dist} distribution is too high. Capping at 23')\n            type2_noise = 23\n        dist = lognorm(s=type2_noise, scale=np.exp(type2_noise ** 2) * type2_center)\n    elif type2_dist == 'lognorm_mode_std':\n        # type2_center = mode, type2_noise = SD\n        shape, type2_noise = _params_lognorm_mode_std(np.maximum(1e-5, type2_center), type2_noise)\n        dist = lognorm(loc=0, scale=type2_noise, s=shape)\n    elif type2_dist == 'lognorm_mean_std':\n        # Corresponds to the CASSANDRE/LogN setup\n        # type2_center = mean, type2_noise = SD\n        type2_center = np.maximum(type2_center, 1e-12)\n        sigma2 = np.log1p((np.maximum(type2_noise, 0) / type2_center) ** 2)\n        scale = np.exp(np.log(type2_center) - sigma2 / 2)\n        dist = lognorm(loc=0, scale=scale, s=np.sqrt(sigma2))\n    elif type2_dist == 'beta_mean_std':\n        # Canonical \"precision\" parameterization of the beta distribution, where precision = 1 / noise**2, i.e.\n        # inverse variance.\n        # type2_center = mean, type2_noise = SD\n\n        type2_center = np.clip(type2_center, 1e-12, 1 - 1e-12)          # avoid a=0 or b=0\n        type2_noise = np.maximum(type2_noise, 1e-12)\n\n        phi = type2_center * (1 - type2_center) / (type2_noise ** 2) - 1\n        phi = np.maximum(phi, 1e-12)\n\n        a = type2_center * phi\n        b = (1 - type2_center) * phi\n\n        dist = beta(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'beta_mode_std':\n        # type2_center = mode, type2_noise = SD\n\n        type2_center = np.clip(type2_center, 1e-8, 1 - 1e-8)\n\n        var_target = np.maximum(type2_noise ** 2, 1e-8)\n        # Initial closed-form approximation\n        kappa = type2_center * (1 - type2_center) / var_target\n        # One-step variance correction\n        num = type2_center * (1 - type2_center) * kappa ** 2 + kappa + 1\n        den = (kappa + 2)**2 * (kappa + 3)\n        var_actual = num / den\n        kappa *= var_actual / var_target\n\n        a = type2_center * kappa + 1\n        b = (1 - type2_center) * kappa + 1\n        dist = beta(loc=0, a=a, b=b, scale=1)\n\n    elif type2_dist == 'beta_mode':\n        # type2_center = mode, type2_noise != SD\n        type2_center = np.maximum(1e-5, np.minimum(1 - 1e-5, type2_center))\n        a = 1 + (1 - type2_center) * type2_center ** 2 / type2_noise ** 2\n        b = (1 / type2_center - 1) * a - 1 / type2_center + 2\n        dist = beta(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'betaprime_mean_std':\n        # type2_center = mean, type2_noise = SD\n        b = 2 + (type2_center * (type2_center + 1)) / type2_noise ** 2\n        a = type2_center * (b - 1)\n        dist = betaprime(loc=0, a=a, b=b, scale=1)\n    elif type2_dist == 'gamma_mode_std':\n        # type2_center = mode, type2_noise = SD\n        u = (type2_center + np.sqrt(type2_center * type2_center + 4 * type2_noise ** 2)) / (2 * type2_noise)\n        dist = gamma(loc=0, a=u**2, scale=type2_noise / u)\n    elif type2_dist == 'gamma_mean_std':\n        # type2_center = mean, type2_noise = SD\n        dist = gamma(loc=0, a=(type2_center / type2_noise) ** 2, scale=(type2_noise ** 2) / type2_center)\n    elif type2_dist == 'gamma_mean':\n        # type2_center = mean, type2_noise != SD\n        dist = gamma(loc=0, a=type2_center / type2_noise, scale=type2_noise)\n    elif type2_dist == 'gamma_mean_cv':\n        # Corresponds to the CASSANDRE setup, i.e. standard multiplicative noise\n        # type2_center = center, type2_noise = CV (relative noise)\n        a = 1 / (type2_noise ** 2)\n        dist = gamma(loc=0, a=a, scale=type2_center / a)\n    elif type2_dist == 'gamma_mode':\n        # type2_center = mode; type2_noise != SD\n        dist = gamma(loc=0, a=(type2_center / type2_noise) + 1, scale=type2_noise)\n    elif type2_dist.startswith('truncated_'):\n        if type2_noise_type == 'noisy_report':\n            if type2_dist == 'truncated_norm_mode_std':\n                # type2_center = mode, type2_noise = SD of the truncated distribution\n                sigma = _params_truncnorm_mode_std(low=0, high=1, x=type2_center, noise=type2_noise)\n                dist = truncnorm(a=-type2_center / sigma, b=(1 - type2_center) / sigma, loc=type2_center, scale=sigma)\n            elif type2_dist == 'truncated_norm_mode':\n                # type2_center = mode, type2_noise = SD of the *un*truncated distribution\n                dist = truncnorm(a=-type2_center / type2_noise, b=(1 - type2_center) / type2_noise, loc=type2_center, scale=type2_noise)\n            elif type2_dist == 'truncated_gumbel_mode_std':\n                # type2_center = mode, type2_noise = SD of the truncated distribution\n                dist = TruncatedGumbelModeSD(mode=type2_center, noise=type2_noise, upper=1, n_newton=3)\n            elif type2_dist == 'truncated_gumbel_mode':\n                # type2_center = mode, type2_noise = SD of the *un*truncated distribution\n                dist = TruncatedGumbelMode(mode=type2_center, noise=type2_noise * np.sqrt(6) / np.pi, upper=1)\n            elif type2_dist == 'truncated_lognorm_mode_std':\n                # type2_center = mode, type2_noise = SD\n                dist = TruncatedLognormalModeSD(mode=type2_center, noise=type2_noise, upper=1, n_newton=5)\n            elif type2_dist == 'truncated_lognorm':\n                # type2_center = median of the untruncated lognormal, type2_noise = SD in log space\n                dist = TruncatedLognormal(median_untrunc=type2_center, noise=type2_noise, upper=1)\n            elif type2_dist == 'truncated_lognorm_mode':\n                # type2_center = mode, type2_noise = SD in log space\n                if type2_noise &gt; 25:\n                    # warnings.warn(f'Type 2 noise for a {type2_dist} distribution is too high. Capping at 25')\n                    type2_noise = 25\n                dist = TruncatedLognormalMode(mode=type2_center, noise=type2_noise, b=1)\n            elif type2_dist == 'truncated_lognorm_mean':\n                # type2_center = mean, type2_noise = SD in log space\n                dist = TruncatedLognormalMean(mean=type2_center, noise=type2_noise, upper=1)\n        elif type2_noise_type in ('noisy_readout', 'noisy_temperature'):\n            if type2_dist == 'truncated_norm_mode_std':\n                # type2_center = mode, type2_noise = SD\n                sigma = _params_truncnorm_mode_std(low=0, high=np.inf, x=type2_center, noise=type2_noise)\n                dist = truncnorm(a=-type2_center / sigma, b=np.inf, loc=type2_center, scale=sigma)\n            elif type2_dist == 'truncated_norm_mode':\n                # type2_center = mode, type2_noise != SD\n                dist = truncnorm(a=-type2_center / type2_noise, b=np.inf, loc=type2_center, scale=type2_noise)\n            elif type2_dist == 'truncated_gumbel_mode_std':\n                # type2_center = mode, type2_noise = SD\n                dist = TruncatedGumbelModeSD(mode=type2_center, noise=type2_noise, upper=np.inf, n_newton=3)\n            elif type2_dist == 'truncated_gumbel_mode':\n                # type2_center = mode, type2_noise != SD\n                dist = TruncatedGumbelMode(mode=type2_center, noise=type2_noise * np.sqrt(6) / np.pi, upper=np.inf)\n\n        else:\n            raise ValueError(f\"'{type2_noise_type}' is an unknown type 2 noise type\")\n\n    return dist  # noqa\n</code></pre>"},{"location":"simulation/#remeta.gendata.print_dataset_characteristics","title":"print_dataset_characteristics","text":"<pre><code>print_dataset_characteristics(sim)\n</code></pre> Source code in <code>remeta/util.py</code> <pre><code>def print_dataset_characteristics(sim):\n    print('----------------------------------')\n    if sim.cfg.skip_type2:\n        print('..Generative parameters:')\n        for p, v in sim.params_type1.items():\n            print(f'{TAB}{p}: {np.array2string(np.array(v), precision=3)}')\n    else:\n        print('..Generative model:')\n        print(f'{TAB}Type 2 noise type: {sim.cfg.type2_noise_type}')\n        print(f'{TAB}Type 2 noise distribution: {sim.cfg.type2_noise_dist}')\n        print('..Generative parameters:')\n        for p, v in sim.params.items():\n            print(f'{TAB}{p}: {np.array2string(np.array(v), precision=3)}')\n        if sim.params_extra is not None:\n            if 'type2_criteria_absolute' in sim.params_extra:\n                print(f\"{TAB}Type 2 criteria (absolute): [{', '.join([f'{c:.5g}' for c in sim.params_extra['type2_criteria_absolute']])}]\")\n            if 'type2_criteria_bias' in sim.params_extra:\n                print(f\"{TAB}Criterion bias: {sim.params_extra['type2_criteria_bias']:.5g}\")\n    print('..Descriptive statistics:')\n    print(f'{TAB}No. subjects: {sim.nsubjects}')\n    print(f'{TAB}No. samples: {sim.nsamples}')\n    if sim.type1_stats is not None:\n        print(f\"{TAB}Performance: {100 * sim.type1_stats['accuracy']:.1f}% correct\")\n        print(f\"{TAB}Choice bias: {('-', '+')[int(sim.type1_stats['choice_bias'] &gt; 0.5)]}{100*np.abs(sim.type1_stats['choice_bias'] - 0.5):.1f}%\")\n    if not sim.cfg.skip_type2 and sim.type2_stats is not None:\n        print(f\"{TAB}Confidence: {sim.type2_stats['confidence']:.2f}\")\n        print(f\"{TAB}M-Ratio: {sim.type2_stats['mratio']:.2f}\")\n        print(f\"{TAB}AUROC2: {sim.type2_stats['auroc2']:.2f}\")\n    print('----------------------------------')\n</code></pre>"},{"location":"simulation/#remeta.gendata.simu_data","title":"simu_data","text":"<pre><code>simu_data(params, nsubjects=1, nsamples=1000, cfg=None, stimuli_external=None, verbosity=True, stimuli_stepsize=0.02, squeeze=False, warn_in_case_of_nondivisible_stepsize=False, compute_stats=True, **kwargs)\n</code></pre> Source code in <code>remeta/gendata.py</code> <pre><code>def simu_data(params, nsubjects=1, nsamples=1000, cfg=None, stimuli_external=None, verbosity=True, stimuli_stepsize=0.02,\n              squeeze=False, warn_in_case_of_nondivisible_stepsize=False,\n              compute_stats=True, **kwargs):\n    params = params.copy()  # this variable can be modifed, thus better to make a copy\n    if cfg is None:\n        # Set configuration attributes that match keyword arguments\n        cfg_kwargs = {k: v for k, v in kwargs.items() if k in Configuration.__dict__}\n        cfg = Configuration(**cfg_kwargs)\n        for setting in cfg.__dict__:\n            if setting.startswith('enable_'):\n                if setting.split('enable_')[1].replace('_param_', '_') not in params:\n                    setattr(cfg, setting, 0)\n    # if not cfg.setup_called:\n    cfg.setup(generative_mode=True)\n\n    if cfg.type2_noise_dist is None:\n        cfg.type2_noise_dist = dict(noisy_report='truncated_norm_mode', noisy_readout='truncated_norm_mode', noisy_temperature='lognorm_mode')[cfg.type2_noise_type]\n\n    # Make sure no unwanted parameters have been passed\n    for p in ('thresh', 'bias', 'noise_heteroscedastic', 'nonlinear_encoding_gain', 'nonlinear_encoding_transition'):\n        if not getattr(cfg, f'enable_type1_param_{p}'):\n            params.pop(f'type1_{p}', None)\n    for p in ('evidence_bias_mult', 'criteria'):\n        if not getattr(cfg, f'enable_type2_param_{p}'):\n            params.pop(f'type2_{p}', None)\n\n    if stimuli_external is None:\n        x_stim = generate_stimuli(nsubjects, nsamples, stepsize=stimuli_stepsize,\n                                  warn_in_case_of_nondivisible_stepsize=warn_in_case_of_nondivisible_stepsize)\n    else:\n        x_stim = stimuli_external / np.max(np.abs(stimuli_external))\n        if stimuli_external.shape != (nsubjects, nsamples):\n            x_stim = np.tile(x_stim, (nsubjects, 1))\n    x_stim_category = (np.sign(x_stim) &gt; 0).astype(int)\n    y_decval_latent, y_decval, d_dec = simu_type1_responses(x_stim, params, cfg)\n\n    if not cfg.skip_type2:\n\n        z1_type1_evidence_latent = np.abs(y_decval_latent)\n        z1_type1_evidence_base = np.abs(y_decval)\n\n        if cfg.type2_noise_type == 'noisy_readout':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=z1_type1_evidence_base, type2_noise=params['type2_noise'],\n                                  type2_noise_type=cfg.type2_noise_type)\n\n            z1_type1_evidence = np.maximum(0, dist.rvs((nsubjects, nsamples)))\n        elif cfg.type2_noise_type == 'noisy_temperature':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=params['type1_noise'] * np.ones_like(z1_type1_evidence_base),\n                                  type2_noise=params['type2_noise'], type2_noise_type=cfg.type2_noise_type)\n            type1_noise_estimated = dist.rvs()\n            z1_type1_evidence = z1_type1_evidence_base\n        elif cfg.type2_noise_type == 'noisy_report':\n            z1_type1_evidence = z1_type1_evidence_base\n        else:\n            raise ValueError('Unknown type 2 noise type')\n\n        c_conf_latent = type1_evidence_to_confidence(\n            z1_type1_evidence=z1_type1_evidence_latent, y_decval=y_decval,\n            x_stim=x_stim, type1_noise_signal_dependency=cfg.type1_noise_signal_dependency,\n            **({**params, **dict(type1_noise=type1_noise_estimated)} if cfg.type2_noise_type == 'noisy_temperature' else params)\n        )\n\n        c_conf_base = type1_evidence_to_confidence(\n            z1_type1_evidence=z1_type1_evidence, y_decval=y_decval,\n            x_stim=x_stim, type1_noise_signal_dependency=cfg.type1_noise_signal_dependency,\n            **({**params, **dict(type1_noise=type1_noise_estimated)} if cfg.type2_noise_type == 'noisy_temperature' else params)\n        )\n\n        if cfg.type2_noise_type == 'noisy_report':\n            dist = get_type2_dist(cfg.type2_noise_dist, type2_center=c_conf_base, type2_noise=params['type2_noise'],\n                                  type2_noise_type=cfg.type2_noise_type)\n            c_conf = np.maximum(0, np.minimum(1, dist.rvs((nsubjects, nsamples))))\n        else:\n            c_conf = c_conf_base\n\n        if cfg.enable_type2_param_criteria:\n            if 'type2_criteria' in params:\n                sum_criteria = np.sum(params['type2_criteria'])\n                if sum_criteria &gt; 1.001:\n                    old_criteria = params['type2_criteria']\n                    params['type2_criteria'] = check_criteria_sum(params['type2_criteria'])\n                    warnings.warn(\n                       '\\nThe first entry of the criterion list is a criterion, whereas the subsequent entries encode\\n'\n                       'the gap to the respective previous criterion. Hence, the sum of all entries in the criterion\\n'\n                       f\"list must be smaller than 1, but sum([{', '.join([f'{c:.3f}' for c in old_criteria])}]) = {sum_criteria:.3f}).\"\n                       f\"Changing criteria to [{', '.join([f'{c:.3f}' for c in params['type2_criteria']])}].\", UserWarning)\n                first_criterion_and_gaps = params['type2_criteria']\n                criteria = [v if i == 0 else np.sum(first_criterion_and_gaps[:i+1]) for i, v in enumerate(first_criterion_and_gaps)]\n            else:\n                first_criterion_and_gaps = np.ones(cfg.n_discrete_confidence_levels - 1) / cfg.n_discrete_confidence_levels\n                criteria = [v if i == 0 else np.sum(first_criterion_and_gaps[:i+1]) for i, v in enumerate(first_criterion_and_gaps)]\n                warnings.warn(\n                    '\\nType 2 criteria enabled, but type2_criteria have not been specified. Using default values\\n'\n                    f\"of a Bayesian confidence observer for {cfg.n_discrete_confidence_levels} discrete ratings: [{', '.join([f'{v:.3g}' for v in first_criterion_and_gaps])}].\\n\"\n                    'Note that the first entry of the criterion list is a criterion, whereas the subsequent\\n'\n                    f'entries encode the gap to the respective previous criterion.\\n'\n                    f\"The final criteria are: [{', '.join([f'{v:.3g}' for v in criteria])}]\", UserWarning)\n\n            c_conf = (np.digitize(c_conf, criteria) + 0.5) / cfg.n_discrete_confidence_levels\n            c_conf_base = (np.digitize(c_conf_base, criteria) + 0.5) / cfg.n_discrete_confidence_levels\n\n    if squeeze:\n        x_stim_category = x_stim_category.squeeze()\n        x_stim = x_stim.squeeze()\n        d_dec = d_dec.squeeze()\n        y_decval_latent = y_decval_latent.squeeze()\n        y_decval = y_decval.squeeze()\n        if not cfg.skip_type2:\n            z1_type1_evidence_base = z1_type1_evidence_base.squeeze()  # noqa\n            z1_type1_evidence = z1_type1_evidence.squeeze()  # noqa\n            c_conf_base = c_conf_base.squeeze()  # noqa\n            c_conf = c_conf.squeeze()  # noqa\n\n    simargs = dict(\n        nsubjects=nsubjects, nsamples=nsamples, params=params, cfg=cfg,\n        x_stim_category=x_stim_category, x_stim=x_stim, d_dec=d_dec,\n        y_decval=y_decval, y_decval_latent=y_decval_latent\n    )\n    if not cfg.skip_type2:\n        simargs.update(\n            z1_type1_evidence_latent=z1_type1_evidence_latent, z1_type1_evidence_base=z1_type1_evidence_base, z1_type1_evidence=z1_type1_evidence,\n            c_conf_latent=c_conf_latent, c_conf_base=c_conf_base, c_conf=c_conf\n        )\n\n    if compute_stats:\n        accuracy = (x_stim_category == d_dec).astype(int)\n        type1_stats = dict(\n            accuracy=np.mean(accuracy),\n            d1 = norm.ppf(min(1 - 1e-3, max(1e-3, d_dec[x_stim_category == 1].mean()))) - \\\n                 norm.ppf(min(1 - 1e-3, max(1e-3, d_dec[x_stim_category == 0].mean().mean()))),\n            choice_bias=d_dec.mean(),\n        )\n        simargs.update(type1_stats=type1_stats)\n        if not cfg.skip_type2:\n            bounds = np.arange(0, 0.81, 0.2)\n            fit = type2_SDT_MLE(x_stim_category.flatten(), d_dec.flatten(), discretize_confidence_with_bounds(c_conf.flatten(), bounds), len(bounds))\n            type2_stats = dict(\n                confidence=c_conf.mean(),\n                auroc2=type2roc(accuracy.flatten(), c_conf.flatten()),\n                mratio=fit.M_ratio\n            )\n            simargs.update(type2_stats=type2_stats)\n            if 'type2_criteria' in params:\n                params_extra = dict(\n                    type2_criteria_absolute=[np.sum(params['type2_criteria'][:i + 1]) for i in range(len(params['type2_criteria']))],\n                    type2_criteria_bias=np.mean(params['type2_criteria']) * (len(params['type2_criteria']) + 1) - 1\n                )\n                simargs.update(params_extra=params_extra)\n\n\n    simulation = Simulation(**simargs)\n    if verbosity:\n        print_dataset_characteristics(simulation)\n\n    return simulation\n</code></pre>"},{"location":"simulation/#remeta.gendata.simu_type1_responses","title":"simu_type1_responses","text":"<pre><code>simu_type1_responses(x_stim, params, cfg)\n</code></pre> Source code in <code>remeta/gendata.py</code> <pre><code>def simu_type1_responses(x_stim, params, cfg):\n\n    if (cfg.type1_noise_signal_dependency != 'none') or (cfg.enable_type1_param_noise == 2):\n        type1_noise = compute_signal_dependent_type1_noise(\n            x_stim=x_stim, type1_noise_signal_dependency=cfg.type1_noise_signal_dependency, **params)\n    else:\n        type1_noise = params['type1_noise']\n\n    type1_param_thresh = _check_param(params['type1_thresh']) if cfg.enable_type1_param_thresh else (0, 0)\n    type1_param_bias = _check_param(params['type1_bias']) if cfg.enable_type1_param_bias else (0, 0)\n\n    if cfg.enable_type1_param_nonlinear_encoding_gain:\n        x_stim_transform = compute_nonlinear_encoding(\n            x_stim, params['type1_nonlinear_encoding_gain'],\n            params['type1_nonlinear_encoding_transition'] if cfg.enable_type1_param_nonlinear_encoding_transition else None)\n    else:\n        x_stim_transform = x_stim\n\n    y_decval_latent = np.full(x_stim_transform.shape, np.nan)\n    y_decval_latent[x_stim_transform &lt; 0] = (np.abs(x_stim_transform[x_stim_transform &lt; 0]) &gt; type1_param_thresh[0]) * \\\n                                   x_stim_transform[x_stim_transform &lt; 0] + type1_param_bias[0]\n    y_decval_latent[x_stim_transform &gt;= 0] = (np.abs(x_stim_transform[x_stim_transform &gt;= 0]) &gt; type1_param_thresh[1]) * \\\n                                    x_stim_transform[x_stim_transform &gt;= 0] + type1_param_bias[1]\n\n    y_decval = y_decval_latent + logistic_dist(scale=type1_noise * np.sqrt(3) / np.pi).rvs(size=x_stim_transform.shape)\n    d_dec = (y_decval &gt;= 0).astype(int)\n\n    return y_decval_latent, y_decval, d_dec\n</code></pre>"},{"location":"simulation/#remeta.gendata.type1_evidence_to_confidence","title":"type1_evidence_to_confidence","text":"<pre><code>type1_evidence_to_confidence(z1_type1_evidence, type2_evidence_bias_mult=1, type1_noise=None, type1_thresh=None, type1_noise_heteroscedastic=None, type1_noise_signal_dependency='none', y_decval=None, x_stim=None, **kwargs)\n</code></pre> <p>Transformation from type 1 evidence (z1) to confidence (c).</p>"},{"location":"simulation/#remeta.gendata.type1_evidence_to_confidence--parameters","title":"Parameters","text":"<p>z1_type1_evidence : array-like     Evidence at the type 1 level (= absolute decision value). type2_evidence_bias_mult : float or array-like     Multiplicative metacognitive bias parameter loading on evidence. type1_noise : float or array-like     Type 1 noise parameter. Can be array-like in case of signal-dependent type 1 noise. type1_noise_heteroscedastic : float or array-like     Signal-dependent type 1 noise parameter. type1_noise_signal_dependency : str     Signal-dependent type 1 noise type. One of 'linear', 'power', 'exponential', 'logarithm'. y_decval : array-like     Decision values. x_stim : array-like     Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value     codes the intensity. kwargs : dict     Convenience parameter to avoid an error if irrelevant parameters are passed.</p>"},{"location":"simulation/#remeta.gendata.type1_evidence_to_confidence--returns","title":"Returns","text":"<p>c_conf : array-like     Model-predicted confidence.</p> Source code in <code>remeta/transform.py</code> <pre><code>def type1_evidence_to_confidence(z1_type1_evidence,\n                                 type2_evidence_bias_mult=1,\n                                 type1_noise=None, type1_thresh=None,\n                                 type1_noise_heteroscedastic=None, type1_noise_signal_dependency='none',\n                                 y_decval=None, x_stim=None,\n                                 **kwargs):  # noqa\n    \"\"\"\n    Transformation from type 1 evidence (z1) to confidence (c).\n\n    Parameters\n    ----------\n    z1_type1_evidence : array-like\n        Evidence at the type 1 level (= absolute decision value).\n    type2_evidence_bias_mult : float or array-like\n        Multiplicative metacognitive bias parameter loading on evidence.\n    type1_noise : float or array-like\n        Type 1 noise parameter. Can be array-like in case of signal-dependent type 1 noise.\n    type1_noise_heteroscedastic : float or array-like\n        Signal-dependent type 1 noise parameter.\n    type1_noise_signal_dependency : str\n        Signal-dependent type 1 noise type. One of 'linear', 'power', 'exponential', 'logarithm'.\n    y_decval : array-like\n        Decision values.\n    x_stim : array-like\n        Array of signed stimulus intensity values, where the sign codes the stimulus category and the absolut value\n        codes the intensity.\n    kwargs : dict\n        Convenience parameter to avoid an error if irrelevant parameters are passed.\n\n    Returns\n    ----------\n    c_conf : array-like\n        Model-predicted confidence.\n    \"\"\"\n    z1_type1_evidence = np.atleast_1d(z1_type1_evidence)\n\n    if ((type1_noise_signal_dependency != 'none') or (hasattr(type1_noise, '__len__') and len(type1_noise) == 2)):\n        if x_stim is None:\n            raise ValueError('Type 1 noise is signal-dependent, but stimuli (x_stim) have not been '\n                             'passed.')\n        type1_noise = compute_signal_dependent_type1_noise(\n            x_stim.reshape(-1, 1) if (x_stim.ndim == 1) and (z1_type1_evidence.ndim == 2) else x_stim,\n            type1_noise=type1_noise, type1_thresh=type1_thresh, type1_noise_heteroscedastic=type1_noise_heteroscedastic,\n            type1_noise_signal_dependency=type1_noise_signal_dependency)\n\n    z2_type2_evidence = type2_evidence_bias_mult * z1_type1_evidence\n    c_conf = np.tanh(np.pi * z2_type2_evidence / (2 * np.sqrt(3) * type1_noise))\n\n    return c_conf\n</code></pre>"},{"location":"simulation/#remeta.gendata.type2_SDT_MLE","title":"type2_SDT_MLE","text":"<pre><code>type2_SDT_MLE(stimID, response, rating, nRatings, cellpadding=None, equalVariance=1)\n</code></pre> Source code in <code>remeta/type2_SDT.py</code> <pre><code>def type2_SDT_MLE(stimID, response, rating, nRatings, cellpadding=None, equalVariance=1):\n    # out = type2_SDT(input)\n    #\n    # Given data from an experiment where an observer discriminates between two\n    # stimulus alternatives on every trial and provides confidence ratings,\n    # provides a type 2 SDT analysis of the data.\n    #\n    # The function estimates the parameters of the unequal variance SDT model,\n    # and uses those estimates to find a maximum likelihood estimate of\n    # meta-da.\n    #\n    # INPUTS\n    #\n    # format of the input may be either:\n    #\n    # 1) stimID, response, rating, nRatings, (cellpadding), (equalVariance)\n    #    where each of the first 3 inputs is a 1xN vector describing the outcome\n    #    of N trials. Contents of input should be as follows.\n    #\n    #    stimID   : 0=S1 stimulus presented, 1=S2 stimulus presented\n    #    response : 0=subject responded S1, 1=subject responded S2\n    #    rating   : values ranges from 1 to m where 1 is the lowest rating\n    #               and m is the highest.\n    #\n    #               All trials where any of these prescribed ranges of values\n    #               are violated are omitted from analysis.\n    #\n    #    nRatings : the number of ratings available to the subject (e.g. for a\n    #               confidence scale of 1-4, nRatings=4).\n    #    cellpadding : if any data cells (e.g. high confidence \"S2\" responses)\n    #               are empty, then the value of cellpadding will be added\n    #               to every data cell. If not specified, default = 1/(2*nRatings)\n    #    equalVariance : if 1, force analysis to use the equal variance SDT\n    #               model. If 0, use an estimate of s = sd(S1) / sd(S2) where\n    #               s is the slope of the zROC data (estimated using MLE).\n    #               If not specified, default = 0.\n    #\n    # 2) nR_S1, nR_S2, (cellpadding), (equalVariance)\n    #    where these are vectors containing the total number of responses in\n    #    each response modality, conditional on presentation of S1 and S2.\n    #    size of each array is 2*nRatings, where each element corresponds to a\n    #    count of responses in each response modality. Response categories are\n    #    ordered as follows:\n    #    highest conf \"S1\" ... lowest conf \"S1\", lowest conf \"S2\", ... highest conf \"S2\"\n    #\n    #    e.g. if nR_S1 = [100 50 20 10 5 1], then when stimulus S1 was\n    #    presented, the subject had the following response counts:\n    #    responded S1, rating=3 : 100 times\n    #    responded S1, rating=2 : 50 times\n    #    responded S1, rating=1 : 20 times\n    #    responded S2, rating=1 : 10 times\n    #    responded S2, rating=2 : 5 times\n    #    responded S2, rating=3 : 1 time\n    #\n    #    cellpadding and equalVariance are defined as above.\n    #\n    #\n    #\n    #\n    # OUTPUTS\n    #\n    # out.d_a       : d_a for input data. If s=1, d_a = d'\n    # out.meta_d_a  : meta_d_a for input data\n    # out.M_ratio   : meta_d_a / d_a; measure of metacognitive efficiency\n    # out.M_diff    : meta_d_a - d_a; measure of metacognitive efficiency\n    # out.c_a       : criterion c_a for input data. If s=1, c_a = c.\n    # out.cprime    : relative criterion used for type 2 estimates. c' = c_a / d_a\n    # out.s         : ratio of evidence distribution standard deviations assumed for the analysis.\n    # out.type2_fit : output of fit_meta_d_MLE for the type 2 SDT fit.\n\n    # 9/24/10 - bm - fixed program-crashing bug for (nR_S1, nR_S2) input\n    # 9/7/10 - bm - wrote it\n\n    ## parse inputs\n    if len(stimID) == 0:\n        raise ValueError(\"Empty data\")\n\n    if cellpadding is None:\n        cellpadding = 1 / (2 * nRatings)\n\n    # filters bad trials\n    f = ((stimID == 0) | (stimID == 1)) &amp; ((response == 0) | (response == 1)) &amp; ((rating &gt;= 1) &amp; (rating &lt;= nRatings))\n    stimID = stimID[f]\n    response = response[f]\n    rating = rating[f]\n\n    # convert to trial count format...\n    nR_S1, nR_S2 = np.full(nRatings*2, np.nan), np.full(nRatings*2, np.nan)\n\n    # get tallies of \"S1\" rating responses for S1 and S2 stim\n    for i in range(nRatings):\n        nR_S1[i] = np.sum((stimID == 0) &amp; (response == 0) &amp; (rating == nRatings - i))\n        nR_S2[i] = np.sum((stimID == 1) &amp; (response == 0) &amp; (rating == nRatings - i))\n\n    # get tallies of \"S2\" rating responses for S1 and S2 stim\n    for i in range(nRatings):\n        nR_S1[i + nRatings] = np.sum((stimID == 0) &amp; (response == 1) &amp; (rating == i + 1))\n        nR_S2[i + nRatings] = np.sum((stimID == 1) &amp; (response == 1) &amp; (rating == i + 1))\n\n    if np.any(nR_S1 == 0) | np.any(nR_S2 == 0):\n        nR_S1 = nR_S1 + cellpadding\n        nR_S2 = nR_S2 + cellpadding\n\n    ## standard SDT analysis\n\n    if equalVariance:\n        s = 1\n\n    ## type 2 SDT analysis\n\n    fit = fit_meta_d_MLE(nR_S1, nR_S2, s)\n\n    return fit\n</code></pre>"},{"location":"simulation/#remeta.gendata.type2roc","title":"type2roc","text":"<pre><code>type2roc(correct, conf, nbins=5)\n</code></pre> Source code in <code>remeta/type2_SDT.py</code> <pre><code>def type2roc(correct, conf, nbins=5):\n    # Calculate area under type 2 ROC\n    #\n    # correct - vector of 1 x ntrials, 0 for error, 1 for correct\n    # conf - vector of continuous confidence ratings between 0 and 1\n    # nbins - how many bins to use for discretization\n\n    bs = 1 / nbins\n    H2, FA2 = np.full(nbins, np.nan), np.full(nbins, np.nan)\n    for c in range(nbins):\n        if c:\n            H2[nbins - c - 1] = np.sum((conf &gt; c*bs) &amp; (conf &lt;= (c+1)*bs) &amp; (correct).astype(bool)) + 0.5\n            FA2[nbins - c - 1] = np.sum((conf &gt; c*bs) &amp; (conf &lt;= (c+1)*bs) &amp; ~(correct).astype(bool)) + 0.5\n        else:\n            H2[nbins - c - 1] = np.sum((conf &gt;= c * bs) &amp; (conf &lt;= (c + 1) * bs) &amp; (correct).astype(bool)) + 0.5\n            FA2[nbins - c - 1] = np.sum((conf &gt;= c * bs) &amp; (conf &lt;= (c + 1) * bs) &amp; ~(correct).astype(bool)) + 0.5\n\n    H2 /= np.sum(H2)\n    FA2 /= np.sum(FA2)\n    cum_H2 = np.hstack((0, np.cumsum(H2)))\n    cum_FA2 = np.hstack((0, np.cumsum(FA2)))\n\n    k = np.full(nbins, np.nan)\n    for c in range(nbins):\n        k[c] = (cum_H2[c+1] - cum_FA2[c])**2 - (cum_H2[c] - cum_FA2[c+1])**2\n\n    auroc2 = 0.5 + 0.25*np.sum(k)\n\n    return auroc2\n</code></pre>"}]}