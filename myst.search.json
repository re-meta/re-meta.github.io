{"version":"1","records":[{"hierarchy":{"lvl1":"Configuration"},"type":"lvl1","url":"/configuration","position":0},{"hierarchy":{"lvl1":"Configuration"},"content":"import remeta\n%load_ext autoreload\n%autoreload 2\n\n\n\nThe default settings of ReMeta are chosen in a way that the toolbox can be used pretty ‘out of the box’.\n\nTo change these settings, create an instance of the configuration object, change the desired settings and pass it to the ReMeta instantiation:import remeta\n\n# create configuration object\ncfg = remeta.Configuration()\n\n# change any setting\ncfg.some_setting = some_value\n\n# pass configuration to ReMeta\nremeta = ReMeta(cfg)\n\n","type":"content","url":"/configuration","position":1},{"hierarchy":{"lvl1":"Configuration","lvl2":"Important settings"},"type":"lvl2","url":"/configuration#important-settings","position":2},{"hierarchy":{"lvl1":"Configuration","lvl2":"Important settings"},"content":"\n\n","type":"content","url":"/configuration#important-settings","position":3},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.param_type2_criteria.enable","lvl2":"Important settings"},"type":"lvl3","url":"/configuration#cfg-param-type2-criteria-enable","position":4},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.param_type2_criteria.enable","lvl2":"Important settings"},"content":"Setting\n\nDefault\n\nPossible values\n\ncfg.param_type2_criteria.enable\n\n3\n\n0, <ncriteria>\n\nIf confidence ratings were provided on a discrete scale with e.g. 4 or 5 available options, it is strongly recommended to set up the number of confidence criteria parameter accordingly. If there are n confidence response options, there are n-1 confidence criteria.\n\n0 disables confidence criteria.\n\n","type":"content","url":"/configuration#cfg-param-type2-criteria-enable","position":5},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.normalize_stimuli_by_max","lvl2":"Important settings"},"type":"lvl3","url":"/configuration#cfg-normalize-stimuli-by-max","position":6},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.normalize_stimuli_by_max","lvl2":"Important settings"},"content":"Setting\n\nDefault\n\nPossible values\n\ncfg.normalize_stimuli_by_max\n\nFalse\n\nFalse, True\n\nIf stimuli are well outside the range [-1; 1], parameter estimation may work less reliably, since many internal settings (like initial parameter guesses) are optimized for this stimulus range. In this case case, either normalize the stimuli beforehand, or use:cfg.normalize_stimuli_by_max = True\n\n","type":"content","url":"/configuration#cfg-normalize-stimuli-by-max","position":7},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.type2_noise_type","lvl2":"Important settings"},"type":"lvl3","url":"/configuration#cfg-type2-noise-type","position":8},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.type2_noise_type","lvl2":"Important settings"},"content":"Setting\n\nDefault\n\nPossible values\n\ncfg.type2_noise_type\n\nreport\n\n'report', 'readout', 'temperature'\n\nBy default, ReMeta assumes that the dominant source of metacognitive noise is at the level of confidence report, i.e.:cfg.type2_noise_type = 'report'\n\nThis is perhaps the settings with the greatest impact on the type 2 stage and it is important to be aware of it. As outlined in \n\nType 2: the metacognitive stage, you may alternatively specify metacognitive noise as an unreliable estimate of one’s own type 1 noise (cfg.type2_noise_type = 'report') or as unreliable readout of type 1 evidence (cfg.type2_noise_type = 'readout').\n\nAt this point it is pretty much an open empirical question which source of metacognitive noise is most relevant.\n\n","type":"content","url":"/configuration#cfg-type2-noise-type","position":9},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.optim_num_cores","lvl2":"Important settings"},"type":"lvl3","url":"/configuration#cfg-optim-num-cores","position":10},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.optim_num_cores","lvl2":"Important settings"},"content":"\n\nSetting\n\nDefault\n\nPossible values\n\ncfg.optim_num_cores\n\n1\n\n-1, 1, ... n\n\nNumber of cores used for parameter estimation. -1 means all system cores minus 1.\n\n","type":"content","url":"/configuration#cfg-optim-num-cores","position":11},{"hierarchy":{"lvl1":"Configuration","lvl2":"Other useful settings"},"type":"lvl2","url":"/configuration#other-useful-settings","position":12},{"hierarchy":{"lvl1":"Configuration","lvl2":"Other useful settings"},"content":"\n\n","type":"content","url":"/configuration#other-useful-settings","position":13},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.skip_type2","lvl2":"Other useful settings"},"type":"lvl3","url":"/configuration#cfg-skip-type2","position":14},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.skip_type2","lvl2":"Other useful settings"},"content":"Setting\n\nDefault\n\nPossible values\n\ncfg.skip_type2\n\nFalse\n\nFalse, True\n\nOnly fit a type 1 model (no confidence data required)\n\nExample:\n\nds = remeta.load_dataset('type1_only')\n\n\n\ncfg = remeta.Configuration()\ncfg.skip_type2 = True\nrem = remeta.ReMeta(cfg=cfg)\nrem.fit(ds.stimuli, ds.choices)\n\n\n\nThough the following works as well (fit_type1() instead of fit()):\n\nrem = remeta.ReMeta(cfg=cfg)\nrem.fit_type1(ds.stimuli, ds.choices)\n\n\n\n","type":"content","url":"/configuration#cfg-skip-type2","position":15},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.optim_type1_gridsearch & cfg.optim_type2_gridsearch","lvl2":"Other useful settings"},"type":"lvl3","url":"/configuration#cfg-optim-type1-gridsearch-cfg-optim-type2-gridsearch","position":16},{"hierarchy":{"lvl1":"Configuration","lvl3":"cfg.optim_type1_gridsearch & cfg.optim_type2_gridsearch","lvl2":"Other useful settings"},"content":"Setting\n\nDefault\n\nPossible values\n\ncfg.optim_type1_gridsearch\n\nFalse\n\nFalse, True\n\ncfg.optim_type2_gridsearch\n\nTrue\n\nFalse, True\n\nBy default, type 2 (but not type 1) fitting uses an initial grid search procedure to find the most promising region for gradient-based parameter optimization durch maximum likelihood optimization.\n\nThe gridsearch range for a parameter can be defined viacfg.param_<param_name>.grid_range = <range> \n\nFor most cases, the default grid ranges are suitable.","type":"content","url":"/configuration#cfg-optim-type1-gridsearch-cfg-optim-type2-gridsearch","position":17},{"hierarchy":{"lvl1":"Group estimation and priors"},"type":"lvl1","url":"/group-estimation-priors","position":0},{"hierarchy":{"lvl1":"Group estimation and priors"},"content":"import remeta\nimport numpy as np\n%load_ext autoreload\n%autoreload 2\n\n\n\nOften, data from single participants are not sufficient for precise parameter estimation. In this section, we introduce to methods to address this: group estimation (random effects or group-level fixed effects) and priors.\n\n","type":"content","url":"/group-estimation-priors","position":1},{"hierarchy":{"lvl1":"Group estimation and priors","lvl2":"Group estimation"},"type":"lvl2","url":"/group-estimation-priors#group-estimation","position":2},{"hierarchy":{"lvl1":"Group estimation and priors","lvl2":"Group estimation"},"content":"To use group-level information we need to pass 2d data to ReMeta (n_subjects x n_samples) and specify the group attribute for parameters.\n\nThe fit method of ReMeta accepts data as either 1d arrays (single participant) or as 2d arrays (group data). To this aim, we simulate type 1 data for 4 participants:\n\nnp.random.seed(42)\ncfg = remeta.Configuration()\ncfg.skip_type2 = True\nparams_true = dict(\n    type1_noise=0.5,\n    type1_bias=0.1,\n)\ncfg.true_params = params_true\ndata = remeta.simulate(nsubjects=5, nsamples=200, params=params_true, cfg=cfg)\n\n\n\nNote that\n\ndata.stimuli.shape\n\n\n\nWe fit a default ReMeta model:\n\nrem = remeta.ReMeta(cfg=cfg)\nrem.fit(data.stimuli, data.choices, data.confidence)\nresult = rem.summary()\n\n\n\nIn case of a group-level fit, the result returned by the summary() method is a list of length nsubjects. We can print the final parameter estimates more cleanly as follows:\n\nfor s in range(result.nsubjects):\n    print(f'Subject {s}')\n    for k, v in result.type1.params[s].items():\n        print(f'\\t{k}: {v:.3f} ± {result.type1.params_se[s][k]:.3f}')\n\n\n\nIn this section, we exemplarily focus on the type1_bias parameter which was set to 0.1 in the simulated data. The fitted parameters for type1_bias vary strongly around 0.1. In line with this variability, the standard errors of the parameter estimates are big.\n\nYet, this is no fitting error, since the empirical log-likelihood is always hight than one for the true parameters. We can verify this as follows:\n\nfor s in range(result.nsubjects):\n    print(f'Subject {s}')\n    print(f'\\tLog likelihood of true parameters: {result.type1.loglik_true[s]:.2f}')\n    print(f'\\tLog likelihood of estimated parameters: {result.type1.loglik[s]:.2f}')\n\n\n\nThe issue is rather that few samples are often not representative of the ground truth. By contrast, the more samples, the less likely that the data — in aggregate — behave substantially different than the ground truth.\n\n","type":"content","url":"/group-estimation-priors#group-estimation","position":3},{"hierarchy":{"lvl1":"Group estimation and priors","lvl3":"Random effects","lvl2":"Group estimation"},"type":"lvl3","url":"/group-estimation-priors#random-effects","position":4},{"hierarchy":{"lvl1":"Group estimation and priors","lvl3":"Random effects","lvl2":"Group estimation"},"content":"An elegant way to regularize unreliable parameter estimates at the individual level are hierarchical random effect models. For random effects parameters, the likelihood computation comprises not only the likelihood given the individual data of a participant, but also the likelihood under a population distribution.\n\nWhat is elegant about random effects models is that this population distribution is itself learned from the data, with the only “prio”\" that the distribution is Gaussian (at least in the frequentist domain).\n\nIn this way, extreme estimates for individual subjects are “tamed” (regularized), since they tend to be unlikely under the population distribution. Nevertheless, random effects models leave enough room for interindividual variability between participants.\n\nIn our example above, we specify the type 1 bias parameter as a random effects group parameter:\n\ncfg.param_type1_bias.group = 'random'\n\n\n\nWe restart the fitting procedure with this new setting:\n\nrem = remeta.ReMeta(cfg=cfg)\nrem.fit(data.stimuli, data.choices, data.confidence)\nresult = rem.summary()\n\n\n\nIn the current example, all participant estimates for type1_bias are pretty similar, reflecting the fact that the data were in fact generated by the same ground truth model:\n\nfor s in range(result.nsubjects):\n    print(f'Subject {s}')\n    for k, v in result.type1.params[s].items():\n        print(f'\\t{k}: {v:.6f}')\n\n\n\nThis observation is matched by an inspection of the population estimate for type1_nonlinear_gain:\n\nprint(f'Estimated population mean: {result.params_random_effect.mean['type1_bias']:.3f}')\nprint(f'Estimated population SD: {result.params_random_effect.std['type1_bias']:.3f}')\n\n\n\n","type":"content","url":"/group-estimation-priors#random-effects","position":5},{"hierarchy":{"lvl1":"Group estimation and priors","lvl3":"Fixed effects","lvl2":"Group estimation"},"type":"lvl3","url":"/group-estimation-priors#fixed-effects","position":6},{"hierarchy":{"lvl1":"Group estimation and priors","lvl3":"Fixed effects","lvl2":"Group estimation"},"content":"Another option to tackle unreliable individual parameter estimates is to fit a single parameter to the entire group. In ReMeta, this is possible by setting the group attribute of the parameter to 'fixed':\n\ncfg.param_type1_bias.group = 'fixed'\n\n\n\nWe fit the model as usual:\n\nrem = remeta.ReMeta(cfg=cfg)\nrem.fit(data.stimuli, data.choices, data.confidence)\nresult = rem.summary()\n\n\n\nNow, the parameter type1_bias is fitted to the entire group dataset and thus the parameter is identical for each participant. The final estimate of 0.109 much closer to the ground truth value of 0.1. We once again print the final parameter more cleanly:\n\nfor s in range(result.nsubjects):\n    print(f'Subject {s}')\n    for k, v in result.type1.params[s].items():\n        print(f'\\t{k}: {v:.3f}')\n\n\n\nNote that even though parameter recovery improved, the log-likelihood of this group fit is worse (i.e. lower) than the single-subject fit:\n\nfor s in range(result.nsubjects):\n    print(f'Subject {s}')\n    print(f'\\tnegll(subject fit): {result.type1.subject.loglik[s]:.3f}')\n    print(f'\\tnegll(group fit): {result.type1.group.loglik[s]:.3f}')\n\n\n\nYet, in this case this effectively means that the model is not overfit to random peculiarities of each subject’s data and better fits the broad trends in the group data.\n\n","type":"content","url":"/group-estimation-priors#fixed-effects","position":7},{"hierarchy":{"lvl1":"Group estimation and priors","lvl2":"Priors"},"type":"lvl2","url":"/group-estimation-priors#priors","position":8},{"hierarchy":{"lvl1":"Group estimation and priors","lvl2":"Priors"},"content":"Priors present another way to inform and regularize point estimates of participants. If there is good reason from prior literature or a prior study to assume a prior distribution for a parameter, one can perform Maximum A Posteriori estimation (MAP) instead of Maximum Likelihood estimation (MLE). In Remeta this is possible by specifying the prior attribute of a parameter. In the following example, we delete the previous random effect for type1_bias and specify a prior instead - a tuple of the form (prior_mean, prior_std).\n\nSpecifically, we assume that the bias will be on average 0 with a standard deviation 0.05 (under a normal model). This is a unrealistically strong prior, but it serves for demonstration.\n\ncfg.param_type1_bias.group = None\ncfg.param_type1_bias.prior = (0, 0.05)\nrem = remeta.ReMeta(cfg=cfg)\nrem.fit(data.stimuli, data.choices, data.confidence)\nresult = rem.summary()\n\n\n\nAccording to our prior, a null effect for the type1_bias should be most likely. Indeed, as seen in the following output, the type1_bias is biased towards 0 compared to the original estimates without a prior:\n\nfor s in range(result.nsubjects):\n    print(f'Subject {s}')\n    for k, v in result.type1.params[s].items():\n        print(f'\\t{k}: {v:.3f}')\n\n","type":"content","url":"/group-estimation-priors#priors","position":9},{"hierarchy":{"lvl1":"ReMeta Guide"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"ReMeta Guide"},"content":"The ReMeta (“Reverse engineering of Metacognition”) toolbox allows researchers to estimate latent type 1 and type 2 parameters based on data of cognitive or perceptual decision-making tasks with two response categories.\n\nGitHub Repository: \n\nhttps://​github​.com​/coconeuro​/remeta\n\nOriginal paper: Guggenmos, M. (2022). Reverse engineering of metacognition. eLife, 11. \n\nDOI: 10.7554/elife.75420.\n\nCitation for Toolbox: Guggenmos, M. (Year of release). ReMeta toolbox (Version X.Y.Z) [Computer software]. GitHub. \n\nhttps://​github​.com​/coconeuro​/remeta","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Installation"},"type":"lvl1","url":"/install","position":0},{"hierarchy":{"lvl1":"Installation"},"content":"Remeta requires a working Python installation. It should run with Python >=3.10.\n\nThe ReMeta itself can be installed with pip:pip install remeta\n\nOr directly from GitHub:pip install git+https://github.com/m-guggenmos/remeta.git\n\n(this command requires an installed Git, e.g. \n\ngitforwindows)\n\nRequired packages (should be automatically installed with pip):\n\nnumpy (>=1.20)\n\nscipy (>=1.3)\n\nmultiprocessing_on_dill (>=3.5.0a4) (only necessary for when the toolbox should be used with multiple cores)\n\nmatplotlib (>=3.6.0)\n\nnumdifftools (>=0.9.38)","type":"content","url":"/install","position":1},{"hierarchy":{"lvl1":"General idea and terminology"},"type":"lvl1","url":"/intro","position":0},{"hierarchy":{"lvl1":"General idea and terminology"},"content":"The ReMeta toolbox allows researchers to estimate latent parameters of observers that perform a decision-making task involving confidence ratings. Type 1 parameters refer to parameters that describe the decision-making process, while Type 2 parameters refer to parameters that describe the confidence rating process.\n\nThe focus of Remeta is an estimate of metacognitive noise, i.e. how accurately observers can monitor their own performance, and metacognitive bias, i.e. whether observers tend to be over- or under-confident.\n\nThe basic architecture of the toolbox is shown in the figure below.\n\n\n\nFrom stimulus x to decisions D...\n\nBased on a stimulus input x, the observer computes a decision value y, the sign of which determines a type 1 decision D. This process can be characterized by a variety of type 1 parameters, including type 1 noise \\sigma_{1} (sensory or decisional noise) and a type 1 decision bias \\delta_1.\n\n... from type 1 decision values y to confidence ratings C\n\nThe absolute value of the noisy decision value — labeled type 1 evidence z_1 — is the relevant input to the type 2 level. It may or may not be subject to a multiplicative bias \\varphi_2, resulting in type 2 evidence z_2.\n\nBy design, the model assumes that observers represent confidence as probability correct, thus computing confidence as c=p(\\text{correct}\\mid z_2,\\hat{\\sigma}_1). This is sometimes referred to as Bayesian confidence\n\nMeyniel et al., 2015.\n\nFinally, the mapping from an internal estimate of confidence c to reported confidence C is governed by a set of confidence criteria \\gamma^1, \\dots, \\gamma^n.\n\nNoise at the type 2 stage\n\nReMeta considers three possible sources of metacognitive noise. First, the “readout” of type 1 evidence may not be perfect, which is referred to as a noisy-readout model\n\nGuggenmos, 2022.\n\nSecond, for the computation of p(\\text{correct}\\mid z_2,\\hat{\\sigma}_1), the observer invariably needs an estimate \\hat{\\sigma}_1 of their own type 1 noise \\sigma_1. This is far from trivial and may thus plausibly be subject to noise itself\n\nBoundy-Singer et al., 2022. This is referred to as noisy-temperature model.\n\nFinally, internal representations of confidence need to be communicated to the experimenter. This too is plausibly a lossy process and may inject noise into the process. This last form of metacognitive noise is referred to as a noisy-report model\n\nShekhar & Rahnev, 2021.\n\nIn the figure above, all three possible sources of noise are indicated with the same parameter symbol \\sigma_2. Not because, noise is thought to be identical, but because in ReMeta, the researcher must decide for one metacognitive noise source and hence only a single metacognitive noise parameter \\sigma_2 will be estimated.","type":"content","url":"/intro","position":1},{"hierarchy":{"lvl1":"Plotting"},"type":"lvl1","url":"/plotting","position":0},{"hierarchy":{"lvl1":"Plotting"},"content":"import remeta\n%load_ext autoreload\n%autoreload 2\n\n\n\nReMeta includes three methods that offer some basic plotting functionality of type 1 and 2 data.\n\nTo illustrate, we simulate a dataset:\n\ncfg = remeta.Configuration()\nparams = dict(\n    type1_noise=0.5,\n    type1_bias=0.15,\n    type2_noise=0.2,\n    type2_criteria=[0.25, 0.5, 0.75]\n)\nds = remeta.simulate(nsamples=1000, params=params, squeeze=True, stim_levels=4, cfg=cfg)\n\n\n\n... and fit a ReMeta model:\n\ncfg.optim_type2_gridsearch = False\nrem = remeta.ReMeta(cfg)\nrem.fit(ds.stimuli, ds.choices, ds.confidence)\nresult = rem.summary()\n\n\n\n","type":"content","url":"/plotting","position":1},{"hierarchy":{"lvl1":"Plotting","lvl2":"plot_psychometric"},"type":"lvl2","url":"/plotting#plot-psychometric","position":2},{"hierarchy":{"lvl1":"Plotting","lvl2":"plot_psychometric"},"content":"\n\nData-driven psychometric curve:\n\nremeta.plot_psychometric(ds.stimuli, ds.choices)\n\n\n\nPlot data alongside fitted parameters. In this case, plot_psychometric can be called from the ReMeta object:\n\nrem.plot_psychometric()\n\n\n\nPlot model only:\n\nrem.plot_psychometric(model_only=True)\n\n\n\nPlot arbitray (theoretical) psychometric curve:\n\nremeta.plot_psychometric(type1_noise_dist='normal', type1_noise=0.5, type1_bias=-0.2)\n\n\n\n","type":"content","url":"/plotting#plot-psychometric","position":3},{"hierarchy":{"lvl1":"Plotting","lvl2":"plot_stimulus_versus_confidence"},"type":"lvl2","url":"/plotting#plot-stimulus-versus-confidence","position":4},{"hierarchy":{"lvl1":"Plotting","lvl2":"plot_stimulus_versus_confidence"},"content":"\n\nData-driven plot:\n\nremeta.plot_stimulus_versus_confidence(ds.stimuli, ds.confidence, ds.choices)\n\n\n\nPlot data alongside fitted parameters. In this case, plot_stimulus_versus_confidence can be called from the ReMeta object:\n\nrem.plot_stimulus_versus_confidence()\n\n\n\nSeparate by accuracy:\n\nrem.plot_stimulus_versus_confidence(separate_by_accuracy=True)\n\n\n\nPlot model only:\n\nrem.plot_stimulus_versus_confidence(model_only=True)\n\n\n\nPlot arbitray (theoretical) stimulus-confidence relationship (reqiores at least type1_noise and type2_noise):\n\nremeta.plot_stimulus_versus_confidence(type1_noise=0.5, type2_noise=0.2)\n\n\n\n","type":"content","url":"/plotting#plot-stimulus-versus-confidence","position":5},{"hierarchy":{"lvl1":"Plotting","lvl2":"plot_confidence_histogram"},"type":"lvl2","url":"/plotting#plot-confidence-histogram","position":6},{"hierarchy":{"lvl1":"Plotting","lvl2":"plot_confidence_histogram"},"content":"\n\nData-driven plot:\n\nremeta.plot_confidence_histogram(ds.confidence)\n\n\n\nPlot data alongside fitted parameters. In this case, plot_confidence_histogram can be called from the ReMeta object:\n\nrem.plot_confidence_histogram()\n\n\n\nSeparate by accuracy:\n\nrem.plot_confidence_histogram(separate_by_accuracy=True)\n\n\n\n\n\nSeparate by stimulus category:\n\nrem.plot_confidence_histogram(separate_by_category=True)\n\n\n\nPlot model only:\n\nrem.plot_confidence_histogram(model_only=True)\n\n\n\nPlot arbitray (theoretical) confidence distribution (reqiores at least type1_noise and type2_noise):\n\nremeta.plot_confidence_histogram(type1_noise=0.5, type2_noise=0.2)\n\n\n\nChange the type 2 noise distribution by passing a Configuration object:\n\ncfg.param_type2_noise.distribution = 'truncated_normal_mode'\n\n\n\nremeta.plot_confidence_histogram(\n    type1_noise=0.5, type2_noise=0.2, type2_criteria=[0.25, 0.5, 0.75], cfg=cfg\n)\n\n","type":"content","url":"/plotting#plot-confidence-histogram","position":7},{"hierarchy":{"lvl1":"Parameter estimates and model evidence"},"type":"lvl1","url":"/results","position":0},{"hierarchy":{"lvl1":"Parameter estimates and model evidence"},"content":"import remeta\nimport numpy as np\n%load_ext autoreload\n%autoreload 2\n\n\n\nThe two critical results of any computational model are 1) how well did the model fit (model evidence) and 2) what are the estimated parameters of the model. In this section, we cover both.\n\n","type":"content","url":"/results","position":1},{"hierarchy":{"lvl1":"Parameter estimates and model evidence","lvl2":"Parameter estimates"},"type":"lvl2","url":"/results#parameter-estimates","position":2},{"hierarchy":{"lvl1":"Parameter estimates and model evidence","lvl2":"Parameter estimates"},"content":"\n\nWe load a builtin dataset and fit the default ReMeta model.\n\nds = remeta.load_dataset('default')\nrem = remeta.ReMeta(optim_type2_gridsearch=False)\nrem.fit(ds.stimuli, ds.choices, ds.confidence)\n\n\n\nTo access the results, it is recommended to first invoke the summary() method on the ReMeta instance:\n\nresult = rem.summary()\n\n\n\nThe final parameter estimates are accessible via\n\nresult.params\n\n\n\nIn addition, each fitting stage (type 1 and type 2) can be accessed separately.\n\nresult.type1.params\n\n\n\nresult.type2.params\n\n\n\n","type":"content","url":"/results#parameter-estimates","position":3},{"hierarchy":{"lvl1":"Parameter estimates and model evidence","lvl3":"Uncertainty of parameter estimates","lvl2":"Parameter estimates"},"type":"lvl3","url":"/results#uncertainty-of-parameter-estimates","position":4},{"hierarchy":{"lvl1":"Parameter estimates and model evidence","lvl3":"Uncertainty of parameter estimates","lvl2":"Parameter estimates"},"content":"\n\nFor each parameter estimate, ReMeta computes a standard error, reflecting the uncertainty in the parameter estimate:\n\nresult.params_se\n\n\n\nThe standard error is computed based on the Hessian matrix, which is the second derivative (i.e. curvature) of the log likelihood \\ell(\\boldsymbol{\\theta}) evaluated at the estimated parameters \\boldsymbol{\\hat{\\theta}}:H(\\boldsymbol{\\hat{\\theta}})\n=\n\\frac{\\partial^2 \\ell(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta} \\, \\partial \\boldsymbol{\\theta}^\\top}\n\\Bigg|_{\\boldsymbol{\\theta} = \\boldsymbol{\\hat{\\theta}}}\n\nThe Hessian is a k x k matrix with k being the number of prameters. By inverting the Hessian, we get the covariance matrix:\\hat{\\mathrm{Cov}}(\\boldsymbol{\\hat{\\theta}})\n=\n\\left(\n- H(\\boldsymbol{\\hat{\\theta}})\n\\right)^{-1}\n\nThe estimated standard error \\hat{\\mathrm{SE}} of the j-th parameter \\hat{\\theta}_j is the square root of the j-th diagonal element of the covariance matrix:\\hat{\\mathrm{SE}}(\\hat{\\theta}_j)\n=\n\\sqrt{\n\\left[\n\\hat{\\mathrm{Cov}}(\\boldsymbol{\\hat{\\theta}})\n\\right]_{jj}\n}\n\nThe Hessian of the log-likelihood evaluated at \\boldsymbol{\\hat{\\theta}} measures the local curvature of the likelihood surface at that point. A steeper curvature indicates that even small deviations from \\boldsymbol{\\hat{\\theta}} are inconsistent with the observed data and thus higher certainty about our particular parameter estimate. In contrast, a flatter curvature a flatter curvature indicates that the log-likelihood changes only gradually near \\boldsymbol{\\hat{\\theta}}, meaning that a wider range of parameter values is consistent with the data and we are less certain about our particular parameter estimate.\n\n","type":"content","url":"/results#uncertainty-of-parameter-estimates","position":5},{"hierarchy":{"lvl1":"Parameter estimates and model evidence","lvl3":"Subject versus group level","lvl2":"Parameter estimates"},"type":"lvl3","url":"/results#subject-versus-group-level","position":6},{"hierarchy":{"lvl1":"Parameter estimates and model evidence","lvl3":"Subject versus group level","lvl2":"Parameter estimates"},"content":"\n\nWhen the model fit includes group-level data (with random or fixed effects), a third level in the result object becomes relevant, which separates subject-level and group-level information. For this purpose, we load a built-in dataset with three participants:\n\nds2 = remeta.load_dataset('group')\n\n\n\nFor this dataset, we fit the type1_bias as a random effect:\n\ncfg = remeta.Configuration()\ncfg.param_type1_bias.group = 'random'\ncfg.optim_type2_gridsearch = False\n\n\n\nrem = remeta.ReMeta(cfg)\nrem.fit(ds2.stimuli, ds2.choices, ds2.confidence)\nresult2 = rem.summary()\n\n\n\nGroup-level parameters are first fitted at an individual level, to provide suitable initial values for the group-level estimate. The result object always contains information for both levels, accessible as follows.\n\nresult2.subject.params\n\n\n\nresult2.group.params\n\n\n\nIn the case, the type 2 level was not fitted at the group level and thus the group-level parameters only include type 1 parameters.\n\nThe following works as well:\n\nresult2.type1.subject.params\n\n\n\nresult2.type1.group.params\n\n\n\nresult2.type2.subject.params\n\n\n\nSince the type 2 level did not include group-level parameters, result2.type2.group is empty (i.e., None):\n\nresult2.type2.group is None\n\n\n\n","type":"content","url":"/results#subject-versus-group-level","position":7},{"hierarchy":{"lvl1":"Parameter estimates and model evidence","lvl2":"Model evidence"},"type":"lvl2","url":"/results#model-evidence","position":8},{"hierarchy":{"lvl1":"Parameter estimates and model evidence","lvl2":"Model evidence"},"content":"\n\nReMeta is fundamentally based on a frequentist framework and uses maximum likelihood estimation to estimate parameters.\n\nLikelihood at the type 1 stage is the probability of decisions D given the stimuli x and the type 1 parameters \\boldsymbol{\\theta_1}. Maximum likelihood estimation tries to find the set of paratemers \\boldsymbol{\\hat{\\theta}_1} that maximize the likelihood under the sampling distribution f(D\\mid \\boldsymbol{\\theta_1}, x):\\boldsymbol{\\hat{\\theta}_1} = arg\\max_{\\boldsymbol{\\theta_1}} f(D\\mid x,\\boldsymbol{\\theta_1})\n\nLikelihood at the type 2 stage is the probability of reported confidence ratings C given type 1 decision values y and the type 2 parameters \\boldsymbol{\\theta_2}.\\boldsymbol{\\hat{\\theta}_2} = arg\\max_{\\boldsymbol{\\theta_2}} f(C\\mid y,\\boldsymbol{\\theta_2})f(y)\n\nLikelihoods are computed for the type 1 and type 2 stage separately and are represented as such in the result object:\n\n\nprint(f'Type 1 log likelihood (overall): {result.type1.loglik:.3f}')\nprint(f'Type 1 log likelihood (per sample): {result.type1.loglik_per_sample:.3f}')\nprint(f'Type 2 log likelihood (overall): {result.type2.loglik:.3f}')\nprint(f'Type 2 log likelihood (per sample): {result.type2.loglik_per_sample:.3f}')\n\n\n\n\nHigher likelihoods indicate better fits. If different studies are compared, it is best to assess the likelihood per sample.\n\nIn Addition, ReMeta reports the Akaike information criterion (AIC) and the Bayesian information criterion (BIC):\\mathrm{AIC} = 2k-2 \\log L(\\boldsymbol{\\hat{\\theta}})\n\nwhere k is the number of parameters and L(\\boldsymbol{\\hat{\\theta}}) is the likelihood for the estimated parameters \\boldsymbol{\\hat{\\theta}}.\\mathrm{BIC} = k \\log n-2 \\log L(\\boldsymbol{\\hat{\\theta}})\n\nwhere n is the number of samples.\n\n\nprint(f'Type 1 AIC (overall): {result.type1.aic:.2f}')\nprint(f'Type 1 AIC (per sample): {result.type1.aic_per_sample:.4f}')\nprint(f'Type 2 AIC (overall): {result.type2.aic:.2f}')\nprint(f'Type 2 AIC (per sample): {result.type2.aic_per_sample:.4f}\\n')\n\nprint(f'Type 1 BIC (overall): {result.type1.bic:.2f}')\nprint(f'Type 1 BIC (per sample): {result.type1.bic_per_sample:.4f}')\nprint(f'Type 2 BIC (overall): {result.type2.bic:.2f}')\nprint(f'Type 2 BIC (per sample): {result.type2.bic_per_sample:.4f}')\n\n\n\nHere, lower AIC and BIC values indicate better fits.\n\nIf a group-level model was fitted to a stage, the result objects contains both the model evidence of the subject-level fit and the group-level fit for each:\n\nfor s in range(result2.nsubjects):\n    print(f'[subject {s}] Subject-level log likelihood: {result2.type1.subject.loglik[s]:.2f}')\n\n\n\nfor s in range(result2.nsubjects):\n    print(f'[subject {s}] Group-level log likelihood: {result2.type1.group.loglik[s]:.2f}')\n\n\n\nNote that the group level log likelihood is expected to be higher than the subject-level log likelihood. It is precisely the goal of group-level fits to avoid overfitting to individual subjects.","type":"content","url":"/results#model-evidence","position":9},{"hierarchy":{"lvl1":"Getting started"},"type":"lvl1","url":"/start","position":0},{"hierarchy":{"lvl1":"Getting started"},"content":"%load_ext autoreload\n%autoreload 2\n\n\n\nThree types of data are required to fit a ReMeta model:-  Table -\n\nVariable\n\nDescription\n\nstimuli\n\nlist/array of signed stimulus intensity values, where the sign codes the stimulus category and the absolute value codes the intensity.\n\nchoices\n\nlist/array of choices coded as 0 (or alternatively -1) for the negative stimuli category and 1 for the positive stimulus category.\n\nconfidence\n\nlist/array of confidence ratings. Confidence ratings must be normalized to [0; 1]. Discrete confidence ratings must be normalized accordingly (e.g., if confidence ratings are 1-4, subtract 1 and divide by 3).\n\nWhen fitting individual participants, these are 1d lists / arrays with length n_trials. If 2d lists / arrays are passed, ReMeta treats this as group data with shape n_subjects x n_trials (-> Group data).\n\n","type":"content","url":"/start","position":1},{"hierarchy":{"lvl1":"Getting started","lvl2":"A simple example"},"type":"lvl2","url":"/start#a-simple-example","position":2},{"hierarchy":{"lvl1":"Getting started","lvl2":"A simple example"},"content":"To quickly demonstrate ReMeta, we load a simple build-in dataset as follows.\n\nimport remeta\nds = remeta.load_dataset('default')\n\n\n\nThe output provides information how the dataset was generated and some descriptive statistics.\n\nThe dataset was generated with four stimulus intensities. The psychometric function below shows that the observer performed at around 60% correct for the lowest (and most difficult) stimulus intensity and close to perfect for the highest intensity.\n\nremeta.plot_psychometric(ds)\n\n\n\nThe relationship between stimulus intensity and confidence likewise reflects the bias, but also shows that the observer is less sensitive than predicted by a model without metacognitive noise. In the above descriptive statistics is visible from the low M-Ratio of 0.58.\n\nremeta.plot_stimulus_versus_confidence(ds, model_prediction=True)\n\n\n\nIn a next step, we fit the model. Since the data were generated with the default ReMeta model, we do not need to define any other settings. We pass the Configuration option optim_type2_gridsearch=False to the ReMeta instance though, to speed up this demo.\n\nrem = remeta.ReMeta(optim_type2_gridsearch=False)\nrem.fit(ds.stimuli, ds.choices, ds.confidence)\n\n\n\nSince the dataset is based on simulation, we know the true parameters of the underlying generative model (see first output), which are quite close to the fitted parameters.\n\nWe can access the fitted parameters by invoking the summary() method on the ReMeta instance:\n\n# Access fitted parameters\nimport numpy as np\nresult = rem.summary()\nfor k, v in result.params.items():\n    print(f'{k}: {np.array2string(np.array(v), precision=3)}')\n\n\n\nBy default, the model fits parameters for type 1 noise and a type 1 bias, as well as metacognitive ‘type 2’ noise and three confidence criteria.\n\n","type":"content","url":"/start#a-simple-example","position":3},{"hierarchy":{"lvl1":"Getting started","lvl2":"Enabling and disabling parameters"},"type":"lvl2","url":"/start#enabling-and-disabling-parameters","position":4},{"hierarchy":{"lvl1":"Getting started","lvl2":"Enabling and disabling parameters"},"content":"\n\nReMeta is not “a” model, but rather a framework for several possible models. To define a model, we need make architectural decisions, but also specify the set of parameters to fit. Model specification works via the Configuration object. While there is a dedicated section (->Link!) for the Configuration, we will introduce it briefly here.\n\ncfg = remeta.Configuration()\n\n\n\nFor instance, we could fit the above data without a type 1 bias. To do so, we disable the corresponding parameter in the configuration.\n\ncfg.param_type1_bias.enable = 0\n\n\n\n0 disables a parameter and 1 enables a parameter. In addition, type 1 parameters can be separated by stimulus category by setting it to 2. In the case of confidence criteria (type2_criteria), the number indicates the number of criteria (number of confidence ratings minus 1). Type 1 and type 2 noise parameters cannot be disabled since the model needs to be probabilistic in nature to compute likelihoods.\n\nParameter\n\nDescription\n\ndefault\n\nsupported\n\ntype1_noise\n\nType 1 noise\n\n1\n\n1,2\n\ntype1_thresh\n\nSensory threshold\n\n0\n\n0,1,2\n\ntype1_bias\n\nChoice bias\n\n1\n\n0,1\n\ntype1_nonlinear_encoding_gain\n\nNonlinear encoding\n\n0\n\n0,1,2\n\ntype1_nonlinear_encoding_scale\n\nNonlinear encoding\n\n0\n\n0,1,2\n\ntype2_noise\n\nMetacognitive noise\n\n1\n\n1\n\ntype2_evidence_bias\n\nMetacognitive evidence bias\n\n0\n\n0,1\n\ntype2_confidence_bias\n\nMetacognitive confidence bias\n\n0\n\n0,1\n\ntype2_criteria\n\nConfidence criteria\n\n3\n\n0,1...10\n\nEach parameter in the above table can be enabled or disabled by changing the value of cfg.param_<parameterName>.enable = X.\n\nLet’s fit the model to the same data, but this time without a type 1 bias. To let ReMeta know about our above setting cfg.param_type1_bias.enable = 0, we pass the configuration instance to the ReMeta constructor:\n\nrem = remeta.ReMeta(cfg)\nrem.fit(ds.stimuli, ds.choices, ds.confidence)\n\n\n\nLet’s check out the parameters:\n\nresult = rem.summary()\nfor k, v in result.params.items():\n    print(f'{k}: {np.array2string(np.array(v), precision=3)}')\n\n\n\n\n\nThe fit is still good, although we note that the estimate of type 1 noise is slightly worse. In the output, we also see that the negative log-likelihood for the type 1 stage is significantly worse (692.61) than in the model that included a bias (683.64).","type":"content","url":"/start#enabling-and-disabling-parameters","position":5},{"hierarchy":{"lvl1":"Type 1: the decision making stage"},"type":"lvl1","url":"/type1","position":0},{"hierarchy":{"lvl1":"Type 1: the decision making stage"},"content":"import remeta\n%load_ext autoreload\n%autoreload 2\n\n\n\nConfidence is regarded as a metacognitive variable and thus identifies as a mental representation of another mental representation. In the case of confidence, the other mental representation is the decision variable that guided our original decision about which we are about to express a sense of confidence. As a consequence, a good model of confidence needs to model the construction of the decision variable — the type 1 stage — as precisely as possible.\n\n\n\nCentral to the type 1 stage is the psychometric function which describes the relationship between the stimulus variable x and choice probability p^+(x) for the positive stimulus category S^+. By default, ReMeta uses a Gaussian type 1 noise model:p^+(x) =  \\Phi\\left(\\frac{x+\\delta_1}{\\sigma_1}\\right)\n\nThe default psychometric curve in ReMeta is characterized by two parameters, \\sigma_1, which reflects type 1 noise, and \\delta_1, which reflects the type 1 bias.\n\nIn addition, ReMeta includes a threshold parameter, representing a minimal stimulus intensity that is required to drive any response in the observer:p^+(x) =  \\begin{cases} \\Phi\\left(\\frac{x+\\delta_1}{\\sigma_1}\\right) & |x| \\ge \\vartheta_1 \\\\ \\Phi\\left(\\frac{\\delta_1}{\\sigma_1}\\right) & |x| < \\vartheta_1 \\end{cases}\n\nThe following sections describe the type 1 parameters available in ReMeta.\n\n","type":"content","url":"/type1","position":1},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl2":"Default parameters"},"type":"lvl2","url":"/type1#default-parameters","position":2},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl2":"Default parameters"},"content":"\n\n","type":"content","url":"/type1#default-parameters","position":3},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl3":"Type 1 noise","lvl2":"Default parameters"},"type":"lvl3","url":"/type1#type-1-noise","position":4},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl3":"Type 1 noise","lvl2":"Default parameters"},"content":"\n\nParameter\n\nSymbol\n\nHow to enable\n\nDefault\n\nPossible values\n\ntype1_noise\n\n\\sigma_1\n\ncfg.param_type1_noise.enable = 1\n\n1\n\n0, 1, 2\n\nIn ReMeta, type 1 noise reflects noise that is present either in the stimulus or in the decision process. The associated parameter \\sigma_1 reflects the standard deviation of the underlying type 1 noise process. Lower \\sigma_1 means lower stimulus/decision noise, hence a more sensitive observer and a steeper psychometric curve. Higher \\sigma_1 means higher stimulus/decision noise, hence a less sensitive observer and a flatter psychometric curve.\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='img/type1_parameters_noise.png'))\n\n\n\nEvery ReMeta model must fit type1 noise, hence it cannot be disabled. However, ReMeta allows separate type 1 noise estimates for the two stimulus categories by setting cfg.enable_param_type1_noise = 2. See below for an example of a psychometric curve with stimulus-category-dependent type 1 noise.\n\nremeta.plot_psychometric(type1_noise=[0.5, 0.2])\n\n\n\nBy default, the type 1 model assumes a normal distribution for type 1 noise (cfg.param_type1_noise.model = 'normal'). However, for some decisional processes a logistic noise model may be more approprtiate cfg.param_type1_noise.model = 'logistic'.\n\nTo appreciate the difference between normal and logistic noise models, consider a random dot kinematogram in which observers have to judge whether the dominant direction of motion is left or right. Is observers choose the direction of motion based on the difference between the average response of right-tuned versus the average response of left-tuned neurons, then a normal noise model would typically be appropriate.\n\nIf they choose the direction based on the maximum response of right-tuned versus the maximum response of left-tuned neurons, then a logistic noise model may be more appropriate.\n\nThe logistic distribution (kurtosis 4.2) is is tail-heavier than the normal distribution (kurtosis 3):\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='img/normal_versus_logistic.png'))\n\n\n\n","type":"content","url":"/type1#type-1-noise","position":5},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl3":"Type 1 bias","lvl2":"Default parameters"},"type":"lvl3","url":"/type1#type-1-bias","position":6},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl3":"Type 1 bias","lvl2":"Default parameters"},"content":"\n\nParameter\n\nSymbol\n\nHow to enable\n\nDefault\n\nPossible values\n\ntype1_bias\n\n\\delta_1\n\ncfg.param_type1_bias.enable = 1\n\n1\n\n0, 1\n\nThe type 1 bias may either be perceptual or decisional in nature and leads to a horizontal shift of the psychometric function. A negative bias indicates a preference for S^- and thus a right-ward shift of the curve. A positive bias indicates a preference for S^+ and thus a left-ward shift of the curve.\n\ndisplay(Image(filename='img/type1_parameters_bias.png'))\n\n\n\nNote that while setting cfg.param_type1_bias.enable = 2, i.e. different biases per stimulus category, is theoretically possible, it is discouraged, as it defeats the interpretation of a bias parameter and leads to a discontinuity at the stimulus boundary.\n\n","type":"content","url":"/type1#type-1-bias","position":7},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl2":"Additional parameters"},"type":"lvl2","url":"/type1#additional-parameters","position":8},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl2":"Additional parameters"},"content":"\n\n","type":"content","url":"/type1#additional-parameters","position":9},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl3":"Type 1 threshold","lvl2":"Additional parameters"},"type":"lvl3","url":"/type1#type-1-threshold","position":10},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl3":"Type 1 threshold","lvl2":"Additional parameters"},"content":"\n\nParameter\n\nSymbol\n\nHow to enable\n\nDefault\n\nPossible values\n\ntype1_bias\n\n\\vartheta_1\n\ncfg.param_type1_bias.enable = 1\n\n0\n\n0, 1, 2\n\nA threshold parameter describes the minimum amount of evidence to elicit any change in choice probability. It is not enabled by default and is mostly useful if an experiment includes many stimuli around the threshold of conscious awareness.\n\ndisplay(Image(filename='img/type1_parameters_thresh.png'))\n\n\n\nType 1 thresholds can be fitted per stimulus category. See below for an example of an associated psychometric curve.\n\nremeta.plot_psychometric(type1_noise=0.5, type1_thresh=[0.1, 0.2])\n\n\n\n","type":"content","url":"/type1#type-1-threshold","position":11},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl3":"Type 1 nonlinear encoding","lvl2":"Additional parameters"},"type":"lvl3","url":"/type1#type-1-nonlinear-encoding","position":12},{"hierarchy":{"lvl1":"Type 1: the decision making stage","lvl3":"Type 1 nonlinear encoding","lvl2":"Additional parameters"},"content":"\n\nParameter\n\nSymbol\n\nHow to enable\n\nDefault\n\nPossible values\n\ntype1_nonlinear_gain\n\n\\kappa_1\n\ncfg.param_type1_nonlinear_gain.enable = 1\n\n0\n\n0, 1, 2\n\ntype1_nonlinear_scale\n\n\\tau_1\n\ncfg.param_type1_nonlinear_scale.enable = 1\n\n0\n\n0, 1, 2\n\nMost models of binary choices assume a linear relationship between stimulus intensity as defined by the modeler and stimulus intensity as encoded in the nervous system. While this assumption is certainly  not warranted, from a pragmatic standpoint it is understandable. Not because of a lack of formalism, but due to the fact that the loss of information through binary choices makes an estimate of underlying nonlinearities a daunting task.\n\nNevertheless, in ReMeta, nonlinearities can be accouted for with two parameters, type1_nonlinear_gain (\\kappa_1) and type1_nonlinear_scale (\\tau_1):\n\nx'=x\\left(1+\\kappa_1\\,\\frac{(x/\\tau_1)^2}{1+(x/\\tau_1)^2}\\right)\n\nThe chosen nonlinearity has a few favorable properties, including the fact that it’s first derivative equals 1 at x = 0 (i.e. it does not change the sensitivity of the psychometric function) and it is approximately linear for small x. See below for the effects of both nonlinearity parameters.\n\ndisplay(Image(filename='img/type1_parameters_nonlinear_stimtrans.png'))\n\n\n\nA few practical recommendations:\n\nFor most scenarios, leave these parameters disabled. Both parameters require very large numbers of samples to yield sufficiently precise estimates.\n\nIf you consider modeling nonlinearities, we recommend to only fit the gain parameter (i.e. cfg.param_type1_nonlinear_gain.enable = 1). In this case, the scale parameter set to the maximum stimulus intensity by default. To change the default scale parameter, use cfg.param_type1_nonlinear_scale.default.\n\nIf you consider modeling nonlinearities, we strongly recommend to fit these parameters as \n\ngroup parameters to utilize as many samples as possible. Even with 2000 samples per observer, both parameter estimates will be all over the place.","type":"content","url":"/type1#type-1-nonlinear-encoding","position":13},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage"},"type":"lvl1","url":"/type2","position":0},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage"},"content":"import remeta\nimport numpy as np\n%load_ext autoreload\n%autoreload 2\n\n\n\nThe main goal of ReMeta is making inferences about metacognitive noise and biases. Metacognitive noise is any noise that occurs after the computation of the type 1 decision variable (y). Metacognitive biases describe under- and overconfidence.\n\nAs shown in the below diagram, this fundamentally requires a decision by the modeler at which point in the hierarchy such sources of noise and bias may occur.\n\n\n\nReMeta considers three sources of metacognitive noise:\n\n\n\nHow to enable\n\nWhat it is\n\nReference\n\nNoisy report (default)\n\ncfg.type2_noise_type = 'report'\n\nNoisy confidence report\n\nShekhar & Rahnev (2021)\n\nNoisy readout\n\ncfg.type2_noise_type = 'readout'\n\nNoisy readout of type 1 evidence\n\nGuggenmos (2022)\n\nNoisy temperature\n\ncfg.type2_noise_type = 'temperature'\n\nNoisy estimate of one’s own type 1 noise\n\nBoundy-Singer et al. (2022)\n\nAt this point, these noise sources are mutually excluside, i.e. the modeler has to make an assumption about the dominant source of metacognitive noise (or do systematic testing).\n\nReMeta is fundamentally built on the assumption that observers computed as the probability p(\\text{correct}) that the type 1 decision was correct, i.e.p(\\text{correct}) = \\Phi\\left(\\frac{z_2}{\\sigma_1}\\right)\n\nin case of normal type 1 noise andp(\\text{correct}) = \\frac{1}{2}\\tanh\\left(\\frac{\\pi}{2\\sqrt{3}\\sigma_1}z_2\\right)\n\nin case of logistic type 1 noise.\n\nNote that this assumption is required to compute metacognitive biases, as otherwise one would have to ask: under/overconfident relative to what?\n\nIn ReMeta, confidence c is represented on a scale from 0, corresponding to p(\\text{correct})=0.5 (i.e. guessing), to 1, corresponding to p(\\text{correct})=1. Mathematically this is a simple transformation:c = 2\\,p(\\text{correct}) - 1\n\nFor this reason, confidence ratings should be passed as normalized values between 0 and 1 to ReMeta.\n\n","type":"content","url":"/type2","position":1},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl2":"Default parameters"},"type":"lvl2","url":"/type2#default-parameters","position":2},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl2":"Default parameters"},"content":"\n\n","type":"content","url":"/type2#default-parameters","position":3},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl3":"Type 2 noise","lvl2":"Default parameters"},"type":"lvl3","url":"/type2#type-2-noise","position":4},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl3":"Type 2 noise","lvl2":"Default parameters"},"content":"\n\nParameter\n\nSymbol\n\nHow to enable\n\nDefault\n\nPossible values\n\ntype2_noise\n\n\\sigma_2\n\ncfg.param_type2_noise.enable = 1\n\n1\n\n1\n\nWhile the the nature of type 2 metacognitive noise is up to the researcher ('report', 'readout' or 'temperature'), the type2 noise parameter cannot be disabled, as a probabilistic aspect is required for maximum likelihood estimation.\n\nThe difference between the three sources of metacognitive noise is not just conceptual. Each noise source has a clear computational signature, as seen below:\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='img/type2_parameters_noise.png'))\n\n\n\nIn all three cases, higher metacognitive noise (\\sigma_2) leads to a shallower relationship between type 1 evidence z_1 and confidence c, i.e. to reduced metacognitive sensitivity. Howvever, for very high metacognitive noise, the noisy-report model predicts average confidence for all type 1 evidence, the noise-readout model maximum confidence, and the noisy-temperature model minimal confidence.\n\nWhich noise type is dominant for human observers is currently unknown. For an individual study, we recommend checking out the type 2 likelihod for each of the architectures to make an informed decision.\n\n","type":"content","url":"/type2#type-2-noise","position":5},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl4":"Type 2 noise distribution","lvl3":"Type 2 noise","lvl2":"Default parameters"},"type":"lvl4","url":"/type2#type-2-noise-distribution","position":6},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl4":"Type 2 noise distribution","lvl3":"Type 2 noise","lvl2":"Default parameters"},"content":"\n\nLikewise unknown is the precise distributional nature of metacognitive noise. In ReMeta, the distribution can be set viacfg.param_type2_noise.distribution = '...'\n\nAvailable options:\n\ncfg.type2_noise_type\n\nDefault\n\nAvailable choices for cfg.type2_noise_dist\n\n'report'\n\n'beta_mode'\n\n'beta_mean_std', 'beta_mode_std', 'beta_mode', 'truncated_normal_mode_std', 'truncated_normal_mode', 'truncated_gumbel_mode_std', 'truncated_gumbel_mode', 'truncated_lognormal_mode_std', 'truncated_lognorm', 'truncated_lognormal_mode', 'truncated_lognormal_mean'\n\n'readout' / 'temperature'\n\n'lognormal_mode_std'\n\n'lognormal_median_std', 'lognormal_mean', 'lognormal_mode', 'lognormal_mode_std', 'lognormal_mean_std','gamma_mode_std', 'gamma_mean_std', 'gamma_mean', 'gamma_mode', 'gamma_mean_cv','betaprime_mean_std','truncated_normal_mode_std', 'truncated_normal_mode','truncated_gumbel_mode_std', 'truncated_gumbel_mode'\n\nThe suffices _mode and _mean indicate a parameterization of the distribution which preserves the mode and mean, respectively. The suffices _std and _cv indicate a parameterization such that type2_noise (i.e. \\sigma_2) corresponds to the standard deviation and the coefficient of variation, respectively.\n\nA paper is still to be written about the evidence and validity of different metacognitive noise distributions. For now, the default choices (beta_mode / lognormal_mode_std) in ReMeta are motivated by the following criteria:\n\nGood performance on test datasets (not yet published)\n\nExternal validity: type 2 noise parameter correlates well with IQ (not yet published)\n\nAnalytical parameterization possible (=computationally efficient)\n\nParameter type2_noise corresponds roughly to the standard deviation of the noise distribution, for ease of interpretability across models.\n\nPrevious evidence in favor of the lognormal distribution as opposed to e.g. a Gamma distribution \n\nShekhar & Rahnev, 2021\n\nBoundy-Singer et al., 2022.\n\nimport remeta\nimport numpy as np\n%load_ext autoreload\n%autoreload 2\n\n\n\nThe default distribution for noisy-report models 'beta_mode' follows the standard mode-concentration parameterization of the Beta distribution:x=\\text{confidence }c;\\quad\\sigma_2=\\text{metacognitive noise}\\text{Concentration:}\\quad\\kappa\n=\n2 + \\frac{x(1-x)}{\\sigma_2^2}X \\sim \\mathrm{Beta}(1 + x(\\kappa - 2),1 + (1-x)(\\kappa - 2))\n\nIn this parameterization, \\sigma_2 is not exactly the standard deviation of the Beta distribution, but a reasonable approximation.\n\nThe default distribution for noise-readout/temperature models 'lognormal_mode_std' is an exact parameterization in terms of standard deviation and mode:\n\n\\text{Noisy-readout:}\\;\\; x=\\text{type 1 evidence }z_1\\text{Noisy-temperature:}\\;\\; x=\\text{type 1 noise }\\sigma_1\\sigma_2=\\text{metacognitive noise}a \\;=\\; \\frac{\\sigma_2^{2}}{x^{2}}\\Delta \\;=\\; \\sqrt{3}\\,\\sqrt{256a^{3}+27a^{2}} \\;-\\; 9aR \\;=\\;\n-\\frac{16\\left(\\frac{2}{3}\\right)^{1/3}a}{\\Delta^{1/3}}\n+\n2\\left(\\frac{2}{3}\\right)^{2/3}\\Delta^{1/3}\n+\n1S \\;=\\;\n\\frac{4\\left(\\frac{2}{3}\\right)^{1/3}a}{\\Delta^{1/3}}\n-\n\\frac{\\Delta^{1/3}}{2^{1/3}3^{2/3}}\n+\n\\frac{1}{2\\sqrt{R}}\n+\n\\frac{1}{2}T \\;=\\; \\frac{1}{4}\\sqrt{R} \\;+\\; \\frac{1}{2}\\sqrt{S} \\;+\\; \\frac{1}{4}X \\sim \\mathrm{LogNormal}\\big(\\ln(x\\,T),\\,\\ln T\\big)\\text{(i.e. }\\ln(x\\,T)\\text{ and }\\ln T\\text{ are the mean and variance}\\\\\\text{of the underlying normal distribution)}\n\n\n\nType 2 noise distributions with \\sigma_2=0.1.\n\n","type":"content","url":"/type2#type-2-noise-distribution","position":7},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl3":"Type 2 criteria","lvl2":"Default parameters"},"type":"lvl3","url":"/type2#type-2-criteria","position":8},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl3":"Type 2 criteria","lvl2":"Default parameters"},"content":"\n\nParameter\n\nSymbol\n\nHow to enable\n\nDefault\n\nPossible values\n\ntype2_criteria\n\n\\gamma^i\n\ncfg.param_type2_criteria.enable = <ncriteria>\n\n3\n\n0, <ncriteria>\n\nThe majority of confidence scales used in the field are based on a discrete scales, e.g. 1=very unsure, 2=unsure, 3=sure, 4=very sure. Thus, participants in such studies need to apply criteria to map a (possibly) continuous internal estimate to a finite number of response options.\n\nIn ReMeta, such criteria are fitted at the last stage of the process. Specifically, it is assumed that observers have a continuous internal estimate of confidence (range 0-1) and apply a set of criteria \\gamma^1, \\dots, \\gamma^n to this internal estimate.\n\ndisplay(Image(filename='img/type2_parameters_criteria.png'))\n\n\n\nThe number of fitted confidence criteria is set via cfg.param_type2_criteria.enable = <ncriteria>, where <ncriteria> should be the number of available confidence ratings minus 1. The default is 3, i.e. corresponding to a 4-point confidence scale.\n\nBy default, it is recommended to fit confidence criteria even for continuous confidence scales. Even for continuous scales, observers tend to have idiosyncratic ways of using different sections of the scale and this is often best accounted for through confidence criteria. For continuous scales, we recommend setting cfg.param_type2_criteria.enable to 3 or 4.\n\n","type":"content","url":"/type2#type-2-criteria","position":9},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl4":"Criterion and confidence bias","lvl3":"Type 2 criteria","lvl2":"Default parameters"},"type":"lvl4","url":"/type2#criterion-and-confidence-bias","position":10},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl4":"Criterion and confidence bias","lvl3":"Type 2 criteria","lvl2":"Default parameters"},"content":"\n\nIf confidence data are fitted with type2_criteria enabled, ReMeta will compute an average criterion bias which could be considered an estimate of the overall metacognitive bias.\n\nAs an example, let’s generate data in which each of the observers three confidence criteria [0.35, 0.5, 0.85] is shifted by 0.1 from Bayes-optimal confidence criteria [0.25, 0.5, 0.75].\n\nparams = dict(type1_noise=0.5, type1_bias=0, type2_noise=0.25, type2_criteria=[0.35, 0.6, 0.85])\nnp.random.seed(0)\nsim = remeta.simulate(params=params)\n\n\n\nThe red line below indicates the new mapping from internal (continuous) confidence to reported (discrete) confidence, relative to the Bayes-optimal mapping (blue line):\n\ndisplay(Image(filename='img/type2_parameters_criteria_underconfident.png'))\n\n\n\nPositive criterion shifts are reflected in underconfident confidence reports, since higher internal confidence is required for each discrete confidence rating.\n\nWe now fit ReMeta to this dataset (using default settings, i.e. no remeta.Configuration is required, though we pass the true parameters for reference).\n\nrem = remeta.ReMeta(true_params=params)\nrem.fit(sim.stimuli, sim.choices, sim.confidence)\nresult = rem.summary()\n\n\n\nIf type 2 criteria are fitted, the ReMeta summary contains a key 'type2_criteria_bias' in result.params_extra which reflects just that:\n\nprint(f\"Criterion bias: {result.params_extra['type2_criteria_bias']:.3f}\")\n\n\n\nThus the criterion bias was estimated to be +0.089 (ground truth was +0.1). The confidence bias is just the negative of the criterion bias and is also part of params_extra:\n\nprint(f\"Confidence bias: {result.params_extra['type2_criteria_confidence_bias']:.3f}\")\n\n\n\nNegative and positive confidence biases correspond to under- and overconfident observers, respectively.\n\nIn ReMeta, the type 2 criterion bias is computed as a weighted average deviation of estimated criteria \\hat{\\boldsymbol{\\gamma}} from Bayes-optimal criteria \\boldsymbol{\\gamma}^\\text{bayes}. The weights are the uncertainty estimates of each criterion, such that more reliabily estimated criteria contribute stronger to the bias computation. In practise, we use the full covariance matrix for this weighted bias to also consider the correlation between criterion uncertainties:\\text{criterion bias}\n=\n\\frac{\n\\mathbf{1}^\\top \\Sigma_{\\gamma}^{-1} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}^\\text{bayes} \\right)}{\\mathbf{1}^\\top \\Sigma_{\\gamma}^{-1} \\mathbf{1}}\n\nwhere \\Sigma_{\\gamma}^{-1} is the inverse sampling covariance matrix of the criteria.\n\nHowever, there’s a catch...\n\nWhile computing a confidence bias in this way serves for a first approximation, these bias estimates are post-hoc, i.e. they are not part of the process model and thus are likely (sic) biased.\n\nThis is a real dilemma in confidence modeling: human participants clearly apply idiosyncratic confidence criteria in their responses and those should be modeled; on the other hand, modeling those criteria makes an estimation of metacognitive biases very challenging.\n\nThe two parameters described below (type2_evidence_bias and type2_confidence_bias) make an attempt at incorporating bias parameters within the process model.\n\n","type":"content","url":"/type2#criterion-and-confidence-bias","position":11},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl2":"Additional parameters"},"type":"lvl2","url":"/type2#additional-parameters","position":12},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl2":"Additional parameters"},"content":"\n\n","type":"content","url":"/type2#additional-parameters","position":13},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl3":"Metacognitive evidence bias","lvl2":"Additional parameters"},"type":"lvl3","url":"/type2#metacognitive-evidence-bias","position":14},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl3":"Metacognitive evidence bias","lvl2":"Additional parameters"},"content":"\n\nParameter\n\nSymbol\n\nHow to enable\n\nDefault\n\nPossible values\n\ntype2_evidence_bias\n\n\\varphi_2\n\ncfg.type2_evidence_bias.enable = 1\n\n0\n\n0, 1\n\nType 2 criteria account for metacognitive biases that occur at the level of the confidence report, i.e. when observers map an internal sense of confidence to an external confidence scale. The idea of a metacognitive evidene bias \\varphi_2 is that biases may also occur at an earlier stage - specifically for the internal representation of metacognitive evidence z_2, which is algorithmically reflected as follows (assuming normal type 1 noise):p(\\text{correct}) = \\Phi\\left(\\frac{\\varphi_2 z_2}{\\sigma_1}\\right)\n\nThe effect of \\phi_2 on confidence is shown in the figure below.\n\ndisplay(Image(filename='img/type2_parameters_evidence_bias.png'))\n\n\n\nIn theory, the metacognitive evidence parameter \\varphi_2 is separable from the effects of type 2 criteria \\gamma_i; in practice — with limited sample size — they might trade of each other.\n\n","type":"content","url":"/type2#metacognitive-evidence-bias","position":15},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl3":"Metacognitive confidence bias","lvl2":"Additional parameters"},"type":"lvl3","url":"/type2#metacognitive-confidence-bias","position":16},{"hierarchy":{"lvl1":"Type 2: the metacognitive stage","lvl3":"Metacognitive confidence bias","lvl2":"Additional parameters"},"content":"\n\nParameter\n\nSymbol\n\nHow to enable\n\nDefault\n\nPossible values\n\ntype2_confidence_bias\n\n\\alpha_2\n\ncfg.type2_confidence_bias.enable = 1\n\n0\n\n0, 1\n\nIt seems tempting to model a metacognitive bias as a parameter that systematically affects all confidence criteria. For instance, consistent withe notion of an additive and multiplicative criteriorn bias above, one could think of describing metacognitive biases as a parametric deviation from Bayes-optimal criteria (e.g. a\\gamma_i^\\text{bayes} + b). Indeed, the evidence bias \\varphi_2 above plays a role similar to a.\n\nAdditive shifts, in turn, are always tricky, since they need to be accompanied by clipping (e.g., neither evidence nor confidence can be below 0). ReMeta offers one additional metacognitive bias parameter that circumvents clipping issues: the confidence bias \\alpha_2:p(\\text{correct}) = \\left(\\Phi\\Big(\\frac{z_2}{\\sigma_1}\\Big)\\right)^{1/\\alpha_2}\n\n\\alpha_2 is a power-law parameter which ensures that confidence remains between 0 and 1. Such a parameter has little biological plausibility, but it is mathematically convenient. To some very rough approximation it resembles an additive shift, with appropriate asymptotic behavior at the confidence bounds 0 and 1.\n\ndisplay(Image(filename='img/type2_parameters_confidence_bias_trans2.png'))\n\n\n\nThe computational signature of \\alpha_2 is clearly distinguishable from the evidence bias \\varphi_2:\n\ndisplay(Image(filename='img/type2_parameters_biases.png'))\n\n\n\nThe parameters \\varphi_2 and \\alpha_2 are thus likely distinguishable in a parameter recovery analysis. Nevertheless, they have little empirical validation and may not work well with simultaneous confidence criteria. Fot this reason, \\varphi_2 and \\alpha_2 are disabled by default.","type":"content","url":"/type2#metacognitive-confidence-bias","position":17}]}